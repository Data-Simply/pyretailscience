{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyRetailScience","text":"<p>Welcome to PyRetailScience.</p>"},{"location":"analysis_modules/","title":"Analysis Modules","text":""},{"location":"analysis_modules/#plots","title":"Plots","text":""},{"location":"analysis_modules/#line-plot","title":"Line Plot","text":"<p>Line plots are particularly good for visualizing sequences that are ordered or sequential, but not necessarily categorical, such as:</p> <ul> <li>Days since an event (e.g., -2, -1, 0, 1, 2)</li> <li>Months since a competitor opened</li> <li>Tracking how metrics change across key events</li> </ul> <p>They are often used to compare trends across categories, show the impact of events on performance, and visualize changes over time-like sequences.</p> <p>Note: While this module can handle datetime values on the x-axis, the plots.time_line plot module has additional features that make working with datetimes easier, such as easily resampling the data to alternate time frames.</p> <p>Example:</p> <pre><code>import pandas as pd\nfrom pyretailscience.plots import line\n\ndf = pd.DataFrame({\n    \"months_since_event\": range(-5, 6),\n    \"category A\": [10000, 12000, 13000, 15000, 16000, 17000, 18000, 20000, 21000, 20030, 25000],\n    \"category B\": [9000, 10000, 11000, 13000, 14000, 15000, 10000, 7000, 3500, 3000, 2800],\n})\n\nline.plot(\n    df=df,\n    value_col=[\"category A\", \"category B\"],\n    x_label=\"Months Since Event\",\n    y_label=\"Revenue (\u00a3)\",\n    title=\"Revenue Trends across Categories\",\n    x_col=\"months_since_event\",\n    group_col=None,\n    source_text=\"Source: PyRetailScience - 2024\",\n    move_legend_outside=True,\n)\n</code></pre>"},{"location":"analysis_modules/#period-on-period-plot","title":"Period-on-Period Plot","text":"<p>Period-on-period plots help compare the same metric across two or more time intervals, all aligned to a common starting point. This is useful when you want to:</p> <ul> <li>Compare different promotional weeks</li> <li>Analyze performance across multiple holiday seasons</li> <li>Benchmark key metrics across repeated events (e.g., monthly product launches)</li> </ul> <p>Each period is overlaid on the same plot, allowing for easy visual comparison of trends across intervals.</p> <p>Note: Dates are automatically realigned to a reference start year, so all lines start at the same x=0 point, regardless of calendar time.</p>"},{"location":"analysis_modules/#example","title":"Example","text":"<pre><code>import pandas as pd\nfrom pyretailscience.plots.period_on_period import plot\n\nperiods = [\n    (\"2022-01-01\", \"2022-04-01\"),\n    (\"2023-01-01\", \"2023-04-01\"),\n]\ndata = {\n    'date': [\n        '2022-01-02', '2022-01-09', '2022-01-16', '2022-01-23', '2022-01-30',\n        '2022-02-06', '2022-02-13', '2022-02-20', '2022-02-27', '2022-03-06',\n        '2022-03-13', '2022-03-20', '2022-03-27',\n        '2023-01-03', '2023-01-08', '2023-01-15', '2023-01-22', '2023-01-29',\n        '2023-02-05', '2023-02-12', '2023-02-19', '2023-02-26', '2023-03-05',\n        '2023-03-12', '2023-03-19', '2023-03-26',\n    ],\n    'sales': [\n        1024, 1199, 1214, 1295, 1249, 1194, 988, 973, 1029, 910, 952, 976, 1099,\n        1195, 1316, 1317, 1361, 1403, 1240, 1164, 1053, 984, 1051, 1079, 1141, 1169,\n    ]\n}\n\ndf = pd.DataFrame(data)\n\nplot(\n    df=df,\n    x_col=\"date\",\n    value_col=\"sales\",\n    periods=periods,\n    x_label=\" \",\n    y_label=\"Sales\",\n    title=\"Period on Period Comparison\",\n    legend_title=\"Periods\",\n    source_text=\"Source: PyRetailScience - Sales FY2024\",\n    move_legend_outside=True,\n)\n</code></pre>"},{"location":"analysis_modules/#area-plot","title":"Area Plot","text":"<p>Area plots are useful for visualizing cumulative trends, showing relative contributions, and comparing multiple data series over time. They are often used for:</p> <ul> <li>Visualizing stacked contributions (e.g., market share over time)</li> <li>Comparing cumulative sales or revenue</li> <li>Showing growth trends across multiple categories</li> </ul> <p>Similar to line plots, area plots can display time-series data, but they emphasize the area under the curve, making them ideal for tracking proportions and cumulative metrics.</p> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pyretailscience.plots import area\n\nperiods = 6\nrng = np.random.default_rng(42)\ndata = {\n    \"transaction_date\": np.repeat(pd.date_range(\"2023-01-01\", periods=periods, freq=\"ME\"), 3),\n    \"unit_spend\": rng.integers(1, 6, size=3 * periods),\n    \"category\": [\"Jeans\", \"Shoes\", \"Dresses\"] * periods,\n}\ndf = pd.DataFrame(data)\ndf_pivoted = df.pivot(index=\"transaction_date\", columns=\"category\", values=\"unit_spend\").reset_index()\n\narea.plot(\n    df=df_pivoted,\n    value_col=[\"Jeans\", \"Dresses\", \"Shoes\"],\n    x_label=\"\",\n    y_label=\"Sales\",\n    title=\"Sales Trends by Product Category\",\n    x_col=\"transaction_date\",\n    source_text=\"Source: PyRetailScience - 2024\",\n    move_legend_outside=True,\n    alpha=0.5,\n)\n</code></pre>"},{"location":"analysis_modules/#scatter-plot","title":"Scatter Plot","text":"<p>Scatter plots are useful for visualizing relationships between two numerical variables, detecting patterns, and identifying outliers. They are often used for:</p> <ul> <li>Exploring correlations between variables</li> <li>Identifying clusters in data</li> <li>Spotting trends and outliers</li> </ul> <p>Scatter plots are particularly useful when analyzing distributions and understanding how one variable influences another. They can also be enhanced with colors and sizes to represent additional dimensions in the data.</p> <p>Example:</p> <pre><code>import random\nimport pandas as pd\nfrom pyretailscience.plots import scatter\n\nmonths = [\n    \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n    \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n]\ncategories = [\"Electronics\", \"Clothing\", \"Home Decor\", \"Sports\", \"Books\"]\n\ndata = {\n    \"month\": months * len(categories),\n    \"sales\": [random.randint(500, 5000) for _ in range(12 * len(categories))],\n    \"profit\": [random.randint(100, 2000) for _ in range(12 * len(categories))],\n    \"expenses\": [random.randint(300, 4000) for _ in range(12 * len(categories))],\n    \"category\": categories * 12,\n}\n\ndf = pd.DataFrame(data)\n\nscatter.plot(\n    df=df,\n    value_col=[\"sales\", \"profit\", \"expenses\"],\n    x_col=\"month\",\n    x_label=\"\",\n    y_label=\"Sales\",\n    title=\"Sales, Profit &amp; Expenses Scatter Plot\",\n    source_text=\"Source: PyRetailScience - 2024\",\n    move_legend_outside=True,\n    alpha=0.8,\n)\n</code></pre>"},{"location":"analysis_modules/#venn-diagram","title":"Venn Diagram","text":"<p>Venn diagrams are useful for visualizing overlaps and relationships between multiple categorical sets. They help in:</p> <ul> <li>Identifying commonalities and differences between groups</li> <li>Understanding intersections between two or three sets</li> <li>Highlighting exclusive and shared elements</li> </ul> <p>Venn diagrams provide a clear way to analyze how different groups relate to each other. They are often used in market segmentation, user behavior analysis, and set comparisons.</p> <p>Example:</p> <pre><code>import pandas as pd\nfrom pyretailscience.plots import venn\n\ndf =  pd.DataFrame({\n    \"groups\": [(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)],\n    \"percent\": [0.119403, 0.089552, 0.238806, 0.208955, 0.134328, 0.208955, 0.104111]\n})\nlabels = [\"Frequent Buyers\", \"High-Spenders\", \"Loyal Members\"]\n\nvenn.plot(\n    df,\n    labels=labels,\n    title=\"E-commerce Customer Segmentation\",\n    source_text=\"Source: PyRetailScience - 2024\",\n    vary_size=False,\n    subset_label_formatter=lambda v: f\"{v:.1%}\"\n)\n</code></pre>"},{"location":"analysis_modules/#histogram-plot","title":"Histogram Plot","text":"<p>Histograms are particularly useful for visualizing the distribution of data, allowing you to see how values in one or more metrics are spread across different ranges. This module also supports grouping by categories, enabling you to compare the distributions across different groups. When grouping by a category, multiple histograms are generated on the  same plot, allowing for easy comparison across categories.</p> <p>Histograms are commonly used to analyze:</p> <ul> <li>Sales, revenue or other metric distributions</li> <li>Distribution of customer segments (e.g., by age, income)</li> <li>Comparing metric distributions across product categories</li> </ul> <p>This module allows you to customize legends, axes, and other visual elements, as well as apply clipping or filtering on the data values to focus on specific ranges.</p> <p>Example:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom pyretailscience.plots import histogram\n\ndf = pd.DataFrame({\n    'first_purchase_revenue': np.concatenate([\n        np.random.normal(70, 10, 50000),\n        np.random.normal(90, 15, 50000)\n    ]),\n    'product': ['Product A'] * 50000 + ['Product B'] * 50000\n})\n\nhistogram.plot(\n    df=df,\n    value_col='first_purchase_revenue',\n    group_col='product',\n    title=\"First Purchase Revenue by Product (\u00a3)\",\n    x_label=\"Revenue (\u00a3)\",\n    y_label=\"Number of Customers\",\n    source_text=\"Source: PyRetailScience - 2024\",\n    move_legend_outside=True,\n    use_hatch=True\n)\n</code></pre>"},{"location":"analysis_modules/#bar-plot","title":"Bar Plot","text":"<p>Bar plots are ideal for visualizing comparisons between categories or groups, showing how metrics such as revenue, sales, or other values vary across different categories. This module allows you to easily group bars by different categories and stack them when comparing multiple metrics. You can also add data labels to display absolute or percentage values for each bar.</p> <p>Bar plots are frequently used to compare:</p> <ul> <li>Product sales across regions or quarters</li> <li>Revenue across product categories or customer segments</li> <li>Performance metrics side by side</li> </ul> <p>This module provides flexibility in customizing legends, axes, and other visual elements, making it easy to represent data across different dimensions, either as grouped or single bar plots.</p> <p>Example:</p> <pre><code>import pandas as pd\nfrom pyretailscience.plots import bar\n\n# Example DataFrame with sales data for different product categories\ndf = pd.DataFrame({\n    \"product\": [\"A\", \"B\", \"C\", \"D\"],\n    \"sales_q1\": [25000, 18000, 22000, 15000],\n    \"sales_q2\": [35000, 50000, 2000, 5000]\n})\n\n# Plot grouped bar chart to show sales across different products and quarters\nbar.plot(\n    df=df,\n    value_col=[\"sales_q1\", \"sales_q2\"],\n    x_col=\"product\",\n    title=\"Sales by Product (Q1 vs Q2)\",\n    x_label=\"Product\",\n    y_label=\"Sales (\u00a3)\",\n    data_label_format=\"percentage_by_bar_group\",\n    source_text=\"Source: PyRetailScience - 2024\",\n    move_legend_outside=True,\n    num_digits=3\n)\n</code></pre>"},{"location":"analysis_modules/#waterfall-plot","title":"Waterfall Plot","text":"<p>Waterfall plots are particularly good for showing how different things add or subtract from a starting number. For instance,</p> <ul> <li>Changes in sales figures from one period to another</li> <li>Breakdown of profit margins</li> <li>Impact of different product categories on overall revenue</li> </ul> <p>They are often used to identify key drivers of financial performance, highlight areas for improvement, and communicate complex data stories to stakeholders in an intuitive manner.</p> <p>Example:</p> <pre><code>from pyretailscience.plots import waterfall\n\nlabels = [\"New\", \"Continuning\", \"Churned\"]\namounts = [660000, 420000, -382000]\n\nwaterfall.plot(\n    labels=labels,\n    amounts=amounts,\n    title=\"New customer growth hiding churn issue\",\n    source_text=\"Source: PyRetailScience - Sales FY2024 vs FY2023\",\n    display_net_bar=True,\n    rot=0,\n)\n</code></pre>"},{"location":"analysis_modules/#index-plots","title":"Index Plots","text":"<p>Index plots are visual tools used in retail analytics to compare different categories or segments against a baseline or average value, typically set at 100. Index plots allow analysts to:</p> <p>Quickly identify which categories over- or underperform relative to the average Compare performance across diverse categories on a standardized scale Highlight areas of opportunity or concern in retail operations Easily communicate relative performance to stakeholders without revealing sensitive absolute numbers</p> <p>In retail contexts, index plots are valuable for:</p> <p>Comparing sales performance across product categories Analyzing customer segment behavior against the overall average Evaluating store or regional performance relative to company-wide metrics Identifying high-potential areas for growth or investment</p> <p>By normalizing data to an index, these plots facilitate meaningful comparisons and help focus attention on significant deviations from expected performance, supporting more informed decision-making in retail strategy and operations.</p> <p>Example:</p> <pre><code>from pyretailscience.plots import index\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\n\ncategories = [\"Music\", \"Electronics\", \"Books\", \"Clothing\", \"Food\", \"Home\", \"Sports\", \"Beauty\"]\nsegments = [\"Light\", \"Medium\", \"Heavy\"]\n\ndata = []\nfor segment in segments:\n    for category in categories:\n        base_price = np.random.uniform(10, 100)\n        for quarter in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n            data.append({\n                \"segment_name\": segment,\n                \"category_0_name\": category,\n                \"unit_price\": base_price * (1 + np.random.uniform(-0.2, 0.3)),\n                \"quarter\": quarter\n            })\n\ndf = pd.DataFrame(data)\n\nindex.plot(\n    df,\n    value_col=\"unit_price\",\n    group_col=\"category_0_name\",\n    index_col= \"segment_name\",\n    value_to_index=\"Light\",\n    agg_func=\"mean\",\n    title=\"Music an opportunity category for Light?\",\n    y_label=\"Categories\",\n    x_label=\"Indexed Spend\",\n    source_text=\"Source: Transaction data financial year 2023\",\n    sort_by=\"value\",\n    sort_order=\"descending\",\n    legend_title=\"Quarter\",\n    highlight_range=None\n)\n</code></pre>"},{"location":"analysis_modules/#cohort-plot","title":"Cohort Plot","text":"<p>Cohort plots are essential for understanding customer retention and behavior over time. These visualizations help identify trends in customer engagement, repeat purchases, and churn rates by grouping customers based on their initial interaction or purchase period. They are particularly useful for:</p> <ul> <li>Analyzing customer retention patterns over time</li> <li>Understanding the effectiveness of marketing campaigns in retaining customers</li> <li>Identifying the impact of seasonality on repeat purchases</li> <li>Evaluating long-term customer engagement with products or services</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pyretailscience.plots import cohort\n\ncohort_start_dates = [\n    \"2022-12\", \"2023-01\", \"2023-02\", \"2023-03\", \"2023-04\",\n    \"2023-05\", \"2023-06\", \"2023-07\", \"2023-08\", \"2023-09\",\n    \"2023-10\", \"2023-11\", \"2023-12\"\n]\n\ndef generate_retention():\n    values = [1.0]\n    for _ in range(11):\n        values.append(max(values[-1] - np.random.uniform(0.05, 0.12), np.random.uniform(0.10, 0.25)))\n    return values\n\ncohort_data = {\"min_period_shopped\": cohort_start_dates}\nfor i in range(12):\n    cohort_data[i] = [generate_retention()[i] for _ in cohort_start_dates]\n\ndf = pd.DataFrame(cohort_data)\ndf = df.set_index(\"min_period_shopped\").reset_index()\ndf = df.melt(id_vars=[\"min_period_shopped\"], var_name=\"period_since\", value_name=\"retention\")\ndf_pivot = df.pivot(index=\"min_period_shopped\", columns=\"period_since\", values=\"retention\")\n\ncohort.plot(\n    df=df_pivot,\n    x_label=\"Months Since Initial Purchase\",\n    y_label=\"Cohort Start Date\",\n    title=\"Customer Retention Cohort Analysis\",\n    source_text=\"Source: PyRetailScience - 2024\",\n    cbar_label=\"Number of Retained Customers\",\n    percentage=True,\n    figsize=(8,8),\n)\n</code></pre>"},{"location":"analysis_modules/#timeline-plot","title":"Timeline Plot","text":"<p>Timeline plots are a fundamental tool for interpreting transactional data within a temporal context. By presenting data in a chronological sequence, these visualizations reveal patterns and trends that might otherwise remain hidden in raw numbers, making them essential for both historical analysis and forward-looking insights. They are particularly useful for:</p> <ul> <li>Tracking sales performance across different periods (e.g., daily, weekly, monthly)</li> <li>Identifying seasonal patterns or promotional impacts on sales</li> <li>Comparing the performance of different product categories or store locations over time</li> <li>Visualizing customer behavior trends, such as purchase frequency or average transaction value</li> </ul> <p>Example:</p> <pre><code>import numpy as np\nimport pandas as pd\n\nfrom pyretailscience.plots import time\n\n# Create a sample DataFrame with 3 groups\nrng = np.random.default_rng(42)\ndf = pd.DataFrame(\n    {\n        \"transaction_date\": pd.concat(\n            [pd.Series(pd.date_range(start=\"2022-01-01\", periods=200, freq=\"D\"))] * 3\n        ),\n        \"total_price\": np.concatenate(\n            [rng.integers(1, 1000, size=200) * multiplier for multiplier in range(1, 4)]\n        ),\n        \"group\": [\"Group A\"] * 200 + [\"Group B\"] * 200 + [\"Group C\"] * 200,\n    },\n)\n\ntime.plot(\n    df,\n    period=\"M\",\n    group_col=\"group\",\n    value_col=\"total_price\",\n    agg_func=\"sum\",\n    title=\"Monthly Sales by Customer Group\",\n    y_label=\"Sales\",\n    legend_title=\"Customer Group\",\n    source_text=\"Source: PyRetailScience - Sales FY2024\",\n    move_legend_outside=True,\n)\n</code></pre>"},{"location":"analysis_modules/#analysis-modules","title":"Analysis Modules","text":""},{"location":"analysis_modules/#cohort-analysis","title":"Cohort Analysis","text":"<p>The cohort analysis module provides functionality for analyzing customer retention patterns over time. It helps businesses understand customer behavior by tracking groups of users (cohorts) based on their first interaction and observing their activity over subsequent periods.</p> <p>Cohort analysis is useful in multiple business applications:</p> <ol> <li>Customer Retention Analysis: Identifies how long users stay engaged with a product or service.</li> <li>Churn Rate Measurement: Helps determine at which stage customers tend to drop off.</li> <li>Marketing Performance Evaluation: Measures the long-term impact of marketing campaigns.</li> <li>Revenue Analysis: Tracks spending behavior over time to optimize pricing strategies.</li> <li>User Engagement Trends: Understands how different user segments behave based on their joining time.</li> </ol> <p>This module calculates cohort tables using various aggregation functions such as <code>nunique</code>, <code>sum</code>, and <code>mean</code>, allowing flexible analysis of customer data.</p> <p>The following key metrics are used in the analysis:</p> <ul> <li>Aggregation Column: Defines the metric to track (e.g., unique customers, total spend).</li> <li>Aggregation Function: Determines how values are aggregated (e.g., sum, mean, count).</li> <li>Cohort Period: Defines the period granularity (year, quarter, month, week, or day).</li> <li>Retention Percentage: Calculates retention rates as a percentage of the first-period cohort.</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nimport datetime\nfrom pyretailscience.analysis.cohort import CohortAnalysis\n\ndata = {\n    \"transaction_id\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n    \"customer_id\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 5, 4],\n    \"transaction_date\": [\n        datetime.date(2023, 1, 15),\n        datetime.date(2023, 1, 20),\n        datetime.date(2023, 2, 5),\n        datetime.date(2023, 2, 10),\n        datetime.date(2023, 3, 1),\n        datetime.date(2023, 3, 15),\n        datetime.date(2023, 3, 20),\n        datetime.date(2023, 4, 10),\n        datetime.date(2023, 4, 25),\n        datetime.date(2023, 5, 5),\n        datetime.date(2023, 5, 20),\n        datetime.date(2023, 6, 10),\n    ],\n    \"unit_spend\": [100, 150, 200, 120, 160, 210, 130, 170, 220, 140, 180, 230]\n}\ndf = pd.DataFrame(data)\n\ncohort = CohortAnalysis(\n    df=df,\n    aggregation_column=\"unit_spend\",\n    agg_func=\"sum\",\n    period=\"month\",\n    percentage=True,\n)\ncohort.table.head()\n</code></pre> min_period_shopped 0 1 2 3 2023-01-01 1.00 1.00 1.00 1.00 2023-02-01 0.80 1.75 0.76 0.00 2023-03-01 0.00 0.00 0.00 0.00 2023-04-01 0.00 0.00 0.00 0.00 2023-05-01 1.28 1.92 0.00 0.00"},{"location":"analysis_modules/#product-association-rules","title":"Product Association Rules","text":"<p>The product association module implements functionality for generating product association rules, a powerful technique in retail analytics and market basket analysis.</p> <p>Product association rules are used to uncover relationships between different products that customers tend to purchase together. These rules provide valuable insights into consumer behavior and purchasing patterns, which can be leveraged by retail businesses in various ways:</p> <ol> <li> <p>Cross-selling and upselling: By identifying products frequently bought together, retailers can make targeted product    recommendations to increase sales and average order value.</p> </li> <li> <p>Store layout optimization: Understanding product associations helps in strategic product placement within stores,    potentially increasing impulse purchases and overall sales.</p> </li> <li> <p>Inventory management: Knowing which products are often bought together aids in maintaining appropriate stock levels    and predicting demand.</p> </li> <li> <p>Marketing and promotions: Association rules can guide the creation ofeffective bundle offers and promotional    campaigns.</p> </li> <li> <p>Customer segmentation: Patterns in product associations can reveal distinct customer segments with specific    preferences.</p> </li> <li> <p>New product development: Insights from association rules can inform decisions about new product lines or features.</p> </li> </ol> <p>The module uses metrics such as support, confidence, and uplift to quantifythe strength and significance of product associations:</p> <ul> <li>Support: The frequency of items appearing together in transactions.</li> <li>Confidence: The likelihood of buying one product given the purchase of another.</li> <li>Uplift: The increase in purchase probability of one product when another is bought.</li> </ul> <p>Example:</p> <pre><code>from pyretailscience.analysis.product_association import ProductAssociation\n\npa = ProductAssociation(\n    df,\n    value_col=\"product_name\",\n    group_col=\"transaction_id\",\n)\npa.df.head()\n</code></pre> product_name_1 product_name_2 occurrences_1 occurrences_2 cooccurrences support confidence uplift 100 Animals Book 100% Organic Cold-Pressed... 78 78 1 0.000039 0.0128205 4.18 100 Animals Book 20K Sousaphone 78 81 3 0.000117 0.0384615 12.10 100 Animals Book 360 Sport 2.0 Boxer Briefs 78 79 1 0.000039 0.0128205 4.13 100 Animals Book 4-Series 4K UHD 78 82 1 0.000039 0.0128205 3.98 100 Animals Book 700S Eterna Trumpet 78 71 1 0.000039 0.0128205 4.60"},{"location":"analysis_modules/#cross-shop","title":"Cross Shop","text":"<p>Cross Shop analysis visualizes the overlap between different customer groups or product categories, helping retailers understand cross-purchasing behaviors. This powerful visualization technique employs Venn or Euler diagrams to show how customers interact across different product categories or segments.</p> <p>Key applications include:</p> <ul> <li>Identifying opportunities for cross-selling and bundling</li> <li>Evaluating product category relationships</li> <li>Analyzing promotion cannibalization</li> <li>Understanding customer shopping patterns across departments</li> <li>Planning targeted marketing campaigns based on complementary purchasing behavior</li> </ul> <p>The module provides options to visualize both the proportional size of each group and the percentage of overlap, making it easy to identify significant patterns in customer shopping behavior.</p> <p>Example:</p> <pre><code>import pandas as pd\nfrom pyretailscience.analysis import cross_shop\n\ndata = {\n    \"customer_id\": [1, 2, 3, 4, 5, 5, 6, 9, 7, 7, 8, 9, 5, 8],\n    \"category_name\" = [\n        \"Electronics\", \"Clothing\", \"Home\", \"Sports\", \"Clothing\", \"Electronics\", \"Electronics\"\n        \"Clothing\", \"Home\", \"Electronics\", \"Clothing\", \"Electronics\", \"Home\", \"Home\"\n        ]\n    \"unit_spend\": [100, 200, 300, 400, 200, 500, 100, 200, 300, 350, 400, 500, 250, 360]\n}\n\ndf = pd.DataFrame(data)\n\ncs_customers = cross_shop.CrossShop(\n    df,\n    group_1_col=\"category_name\",\n    group_1_val=\"Electronics\",\n    group_2_col=\"category_name\",\n    group_2_val=\"Clothing\",\n    group_3_col=\"category_name\",\n    group_3_val=\"Home\",\n    labels=[\"Electronics\", \"Clothing\", \"Home\"],\n)\n\ncs_customers.plot(\n    title=\"Customer Spend Overlap Across Categories\",\n    source_text=\"Source: PyRetailScience\",\n)\n</code></pre>"},{"location":"analysis_modules/#gain-loss","title":"Gain Loss","text":"<p>The Gain Loss module (also known as switching analysis) helps analyze changes in customer behavior between two time periods. It breaks down revenue or customer movement between a focus group and a comparison group by:</p> <ul> <li>New customers: Customers who didn't purchase in period 1 but did in period 2</li> <li>Lost customers: Customers who purchased in period 1 but not in period 2</li> <li>Increased/decreased spending: Existing customers who changed their spending level</li> <li>Switching: Customers who moved between the focus and comparison groups</li> </ul> <p>This module is particularly valuable for:</p> <ul> <li>Analyzing promotion cannibalization</li> <li>Understanding customer migration between brands or categories</li> <li>Evaluating the effectiveness of marketing campaigns</li> <li>Quantifying the sources of revenue changes</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pyretailscience.analysis.gain_loss import GainLoss\n\nnp.random.seed(42)\nn_customers = 30\n\ndf = pd.DataFrame({\n    \"customer_id\": [f\"C{i:03d}\" for i in range(n_customers)] * 2,\n    \"unit_spend\": np.random.randint(10, 100, size=n_customers * 2),\n    \"brand\": np.random.choice([\"Brand A\", \"Brand B\"], size=n_customers * 2),\n    \"period\": [\"p1\"] * n_customers + [\"p2\"] * n_customers,\n})\n\ngain_loss = GainLoss(\n    df=df,\n    p1_index= df[\"period\"] == \"p1\",\n    p2_index= df[\"period\"] == \"p2\",\n    focus_group_index=df[\"brand\"] == \"Brand A\",\n    focus_group_name=\"Brand A\",\n    comparison_group_index=df[\"brand\"] == \"Brand B\",\n    comparison_group_name=\"Brand B\",\n)\n\ngain_loss.plot(\n    title=\"Brand A vs Brand B: Customer Movement Analysis\",\n    x_label=\"Revenue Change\",\n    source_text=\"Source: PyRetailScience\",\n    move_legend_outside=True,\n)\n</code></pre>"},{"location":"analysis_modules/#customer-decision-hierarchy","title":"Customer Decision Hierarchy","text":"<p>A Customer Decision Hierarchy (CDH), also known as a Customer Decision Tree, is a powerful tool in retail analytics that  visually represents the sequential steps and criteria customers use when making purchase decisions within a specific  product category. Here's a brief summary of its purpose and utility:</p> <p>CDHs allow analysts to:</p> <ul> <li>Map out the hierarchical structure of customer decision-making processes</li> <li>Identify key product attributes that drive purchase decisions</li> <li>Understand product substitutions and alternatives customers consider</li> <li>Prioritize product attributes based on their importance to customers</li> </ul> <p>In retail contexts, CDHs are valuable for:</p> <ul> <li>Optimizing product assortments and shelf layouts</li> <li>Developing targeted marketing strategies</li> <li>Identifying opportunities for new product development</li> <li>Understanding competitive dynamics within a category</li> </ul> <p>By visualizing the decision-making process, CDHs help retailers align their offerings and strategies with customer preferences, potentially increasing sales and customer satisfaction. They provide insights into how customers navigate choices, enabling more effective category management and merchandising decisions.</p> <p>Example:</p> <pre><code>from pyretailscience.analysis.customer_decision_hierarchy import CustomerDecisionHierarchy\n\ncdh = CustomerDecisionHierarchy(df)\nax = cdh.plot(\n    orientation=\"right\",\n    source_text=\"Source: Transactions 2024\",\n    title=\"Snack Food Substitutions\",\n)\n</code></pre>"},{"location":"analysis_modules/#revenue-tree","title":"Revenue Tree","text":"<p>The Revenue Tree is a hierarchical breakdown of factors contributing to overall revenue, allowing for detailed analysis of sales performance and identification of areas for improvement.</p> <p>Key Components of the Revenue Tree:</p> <ol> <li> <p>Revenue: The top-level metric, calculated as Customers * Revenue per Customer.</p> </li> <li> <p>Revenue per Customer: Average revenue generated per customer, calculated as:    Orders per Customer * Average Order Value.</p> </li> <li> <p>Orders per Customer: Average number of orders placed by each customer.</p> </li> <li> <p>Average Order Value: Average monetary value of each order, calculated as:    Items per Order * Price per Item.</p> </li> <li> <p>Items per Order: Average number of items in each order.</p> </li> <li> <p>Price per Item: Average price of each item sold.</p> </li> </ol> <p>Example:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pyretailscience.analysis import revenue_tree\n\nnp.random.seed(42)\n\n# Generate 100 records\nnum_records = 100\ndf = pd.DataFrame({\n    \"group_id\": np.random.choice([1, 2], size=num_records),\n    \"customer_id\": np.random.randint(1, 31, size=num_records),\n    \"transaction_id\": np.arange(1, num_records + 1),\n    \"unit_spend\": np.random.uniform(50, 500, size=num_records).round(2),\n    \"unit_quantity\": np.random.randint(1, 6, size=num_records),\n    \"transaction_date\": pd.to_datetime(\n        np.random.choice(pd.date_range(\"2023-01-01\", \"2023-01-10\"), size=num_records)\n    )\n})\n\ndf[\"period\"] = df[\"transaction_date\"].apply(lambda x: \"P1\" if x &lt; pd.Timestamp(\"2023-01-04\") else \"P2\")\n\nrev_tree = revenue_tree.RevenueTree(\n    df,\n    period_col=\"period\",\n    p1_value = \"P1\",\n    p2_value = \"P2\",\n)\n</code></pre>"},{"location":"analysis_modules/#hml-segmentation","title":"HML Segmentation","text":"<p>Heavy, Medium, Light (HML) is a segmentation that places customers into groups based on their percentile of spend or the number of products they bought. Heavy customers are the top 20% of customers, medium are the next 30%, and light are the bottom 50% of customers. These values are chosen based on the proportions of the Pareto distribution. Often, purchase behavior follows this distribution, typified by the expression \"20% of your customers generate 80% of your sales.\" HML segmentation helps answer questions such as:</p> <ul> <li>How much more are your best customers worth?</li> <li>How much more could you spend acquiring your best customers?</li> <li>What is the concentration of sales with your top (heavy) customers?</li> </ul> <p>The module also handles customers with zero spend, with options to include them with light customers, exclude them entirely, or place them in a separate \"Zero\" segment.</p> <p>Example:</p> <pre><code>from pyretailscience.plots import bar\nfrom pyretailscience.segmentation.hml import HMLSegmentation\n\nseg = HMLSegmentation(df, zero_value_customers=\"include_with_light\")\n\nbar.plot(\n    seg.df.groupby(\"segment_name\")[\"unit_spend\"].sum(),\n    value_col=\"unit_spend\",\n    source_text=\"Source: PyRetailScience\",\n    sort_order=\"descending\",\n    x_label=\"\",\n    y_label=\"Segment Spend\",\n    title=\"What's the value of a Heavy customer?\",\n    rot=0,\n)\n</code></pre>"},{"location":"analysis_modules/#threshold-segmentation","title":"Threshold Segmentation","text":"<p>Threshold Segmentation offers a flexible approach to customer grouping based on custom-defined percentile thresholds. Unlike the fixed 20/30/50 split in HML segmentation, Threshold Segmentation allows you to specify your own thresholds and segment names, making it adaptable to various business needs.</p> <p>This flexibility enables you to:</p> <ul> <li>Create quartile segmentations (e.g., top 25%, next 25%, etc.)</li> <li>Define custom tiers based on your specific business model</li> <li>Segment customers based on alternative metrics beyond spend, such as visit frequency or product variety</li> </ul> <p>Like HML segmentation, the module provides options for handling customers with zero values, allowing you to include them with the lowest segment, exclude them entirely, or place them in a separate segment.</p> <p>Example:</p> <pre><code>from pyretailscience.plots import bar\nfrom pyretailscience.segmentation.threshold import ThresholdSegmentation\n\n# Create custom segmentation with quartiles\n# Define thresholds at 25%, 50%, 75%, and 100% (quartiles)\nthresholds = [0.25, 0.50, 0.75, 1.0]\nsegments = [\"Bronze\", \"Silver\", \"Gold\", \"Platinum\"]\n\nseg = ThresholdSegmentation(\n    df=df,\n    thresholds=thresholds,\n    segments=segments,\n    zero_value_customers=\"separate_segment\",\n)\n\nbar.plot(\n    seg.df.groupby(\"segment_name\")[\"unit_spend\"].sum(),\n    value_col=\"unit_spend\",\n    source_text=\"Source: PyRetailScience\",\n    sort_order=\"descending\",\n    x_label=\"\",\n    y_label=\"Segment Spend\",\n    title=\"Customer Value by Segment\",\n    rot=0,\n)\n</code></pre>"},{"location":"analysis_modules/#segmentation-stats","title":"Segmentation Stats","text":"<p>The Segmentation Stats module provides functionality to calculate transaction statistics by segment for a particular segmentation. It makes it easy to compare key metrics across different segments, helping you understand how your customer (or transactions or promotions) groups differ in terms of spending behavior and transaction patterns. This module calculates metrics such as total spend, number of transactions, average spend per customer, and transactions per customer for each segment. It's particularly useful when combined with other segmentation approaches like HML segmentation.</p> <p>Example:</p> <pre><code>from pyretailscience.segmentation.segstats import SegTransactionStats\nfrom pyretailscience.segmentation.hml import HMLSegmentation\n\nseg = HMLSegmentation(df, zero_value_customers=\"include_with_light\")\n\n# First, segment customers using HML segmentation\nsegmentation = HMLSegmentation(df)\n\n# Add segment labels to the transaction data\ndf_with_segments = segmentation.add_segment(df)\n\n# Calculate transaction statistics by segment\nsegment_stats = SegTransactionStats(df_with_segments)\n\n# Display the statistics\nsegment_stats.df\n</code></pre> segment_name spend transactions customers spend_per_customer spend_per_transaction transactions_per_customer customers_pct Heavy 2927.21 30 10 292.721 97.5735 3 0.2 Medium 1014.97 45 15 67.6644 22.5548 3 0.3 Light 662.107 75 25 26.4843 8.82809 3 0.5 Total 4604.28 150 50 92.0856 30.6952 3 1"},{"location":"analysis_modules/#rfm-segmentation","title":"RFM Segmentation","text":"<p>Recency, Frequency, Monetary (RFM) segmentation categorizes customers based on their purchasing behavior:</p> <ul> <li>Recency (R): How recently a customer made a purchase</li> <li>Frequency (F): How often a customer makes purchases</li> <li>Monetary (M): How much a customer spends</li> </ul> <p>Each metric is typically scored on a scale, and the combined RFM score helps businesses identify loyal customers, at-risk customers, and high-value buyers.</p> <p>RFM segmentation helps answer questions such as:</p> <ul> <li>Who are your most valuable customers?</li> <li>Which customers are at risk of churn?</li> <li>Which customers should be targeted for re-engagement?</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nfrom pyretailscience.segmentation.rfm import RFMSegmentation\n\ndata = pd.DataFrame({\n    \"customer_id\": [1, 1, 2, 2, 3, 3, 3],\n    \"transaction_id\": [101, 102, 201, 202, 301, 302, 303],\n    \"transaction_date\": [\"2024-03-01\", \"2024-03-10\", \"2024-02-20\", \"2024-02-25\", \"2024-01-15\", \"2024-01-20\", \"2024-02-05\"],\n    \"unit_spend\": [50, 75, 100, 150, 200, 250, 300]\n})\n\ndata[\"transaction_date\"] = pd.to_datetime(data[\"transaction_date\"])\ncurrent_date = \"2024-07-01\"\n\nrfm_segmenter = RFMSegmentation(df=data, current_date=current_date)\nrfm_results = rfm_segmenter.df\n</code></pre> customer_id recency_days frequency monetary r_score f_score m_score rfm_segment fm_segment 1 113 2 125 0 0 0 0 0 2 127 2 250 1 1 1 111 11 3 147 3 750 2 2 2 222 22"},{"location":"analysis_modules/#purchases-per-customer","title":"Purchases Per Customer","text":"<p>The Purchases Per Customer module analyzes and visualizes the distribution of transaction frequency across your customer base. This module helps you understand customer purchasing patterns by percentile and is useful for determining values like your churn window.</p> <p>Example:</p> <pre><code>from pyretailscience.analysis.customer import PurchasesPerCustomer\n\nppc = PurchasesPerCustomer(transactions)\n\nppc.plot(\n    title=\"Purchases per Customer\",\n    percentile_line=0.8,\n    source_text=\"Source: PyRetailScience\",\n)\n</code></pre>"},{"location":"analysis_modules/#days-between-purchases","title":"Days Between Purchases","text":"<p>The Days Between Purchases module analyzes the time intervals between customer transactions, providing valuable insights into purchasing frequency and shopping patterns. This analysis helps you understand:</p> <ul> <li>How frequently your customers typically return to make purchases</li> <li>The distribution of purchase intervals across your customer base</li> <li>Which customer segments have shorter or longer repurchase cycles</li> <li>Where intervention might be needed to prevent customer churn</li> </ul> <p>This information is critical for planning communication frequency, timing promotional campaigns, and developing effective retention strategies. The module can visualize both standard and cumulative distributions of days between purchases.</p> <p>Example:</p> <pre><code>dbp = DaysBetweenPurchases(transactions)\n\ndbp.plot(\n    bins=15,\n    title=\"Average Days Between Customer Purchases\",\n    percentile_line=0.5,  # Mark the median with a line\n)\n</code></pre>"},{"location":"analysis_modules/#transaction-churn","title":"Transaction Churn","text":"<p>The Transaction Churn module analyzes how customer churn rates vary based on the number of purchases customers have made. This helps reveal critical retention thresholds in the customer lifecycle when setting a churn window</p> <p>Example:</p> <pre><code>from pyretailscience.analysis.customer import TransactionChurn\n\ntc = TransactionChurn(transactions, churn_period=churn_period)\n\ntc.plot(\n    title=\"Churn Rate by Number of Purchases\",\n    cumulative=True,\n    source_text=\"Source: PyRetailScience\",\n)\n</code></pre>"},{"location":"analysis_modules/#composite-rank","title":"Composite Rank","text":"<p>The Composite Rank module creates a composite ranking of several columns by giving each column an individual rank and then combining those ranks together. Composite rankings are particularly useful for:</p> <ul> <li>Product range reviews when multiple factors need to be considered together</li> <li>Prioritizing actions based on multiple performance metrics</li> <li>Creating balanced scorecards that consider multiple dimensions</li> <li>Identifying outliers across multiple metrics</li> </ul> <p>This module allows you to specify different sort orders for each column (ascending or descending) and supports various aggregation functions to combine the ranks, such as mean, sum, min, or max.</p> <p>Key features:</p> <ul> <li>Supports both ascending and descending sort orders</li> <li>Handles ties in rankings with configurable options</li> <li>Combines multiple individual ranks into a single composite rank</li> <li>Works with both pandas DataFrames and ibis Tables</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nfrom pyretailscience.analysis.composite_rank import CompositeRank\n\n# Create sample data for products\ndf = pd.DataFrame({\n    \"product_id\": [1, 2, 3, 4, 5],\n    \"spend\": [100, 150, 75, 200, 125],\n    \"customers\": [20, 30, 15, 40, 25],\n    \"spend_per_customer\": [5.0, 5.0, 5.0, 5.0, 5.0],\n})\n\n# Create CompositeRank with multiple columns\ncr = CompositeRank(\n    df=df,\n    rank_cols=[\n        (\"spend\", \"desc\"),           # Higher spend is better\n        (\"customers\", \"desc\"),       # Higher customer count is better\n        (\"spend_per_customer\", \"desc\") # Higher spend per customer is better\n    ],\n    agg_func=\"mean\",     # Use mean to aggregate ranks\n    ignore_ties=False    # Keep ties (rows with same values get same rank)\n)\n\ncr.df.sort_values(\"composite_rank\")\n</code></pre> product_id spend customers spend_per_customer spend_rank customers_rank spend_per_customer_rank composite_rank 4 200 40 5.0 1 1 1 1.0 2 150 30 5.0 2 2 1 1.67 5 125 25 5.0 3 3 1 2.33 1 100 20 5.0 4 4 1 3.0 3 75 15 5.0 5 5 1 3.67"},{"location":"analysis_modules/#utils","title":"Utils","text":""},{"location":"analysis_modules/#filter-and-label-by-periods","title":"Filter and Label by Periods","text":"<p>The Filter and Label by Periods module allows you to:</p> <ul> <li>Filter transaction data to specific time periods (e.g., quarters, months, promotional periods)</li> <li>Add period labels to your data for easy segmentation and comparison</li> <li>Analyze before-and-after performance for events or promotions</li> <li>Compare metrics across different time frames consistently</li> </ul> <p>This functionality is particularly useful for:</p> <ul> <li>Comparing KPIs across fiscal quarters or years</li> <li>Analyzing seasonal performance patterns</li> <li>Measuring the impact of promotions or events</li> <li>Creating period-based visualizations with consistent data preparation</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nimport ibis\nfrom pyretailscience.utils.date import filter_and_label_by_periods\n\n# Create a sample transactions table\ndata = pd.DataFrame({\n    \"transaction_id\": range(1, 101),\n    \"transaction_date\": pd.date_range(start=\"2023-01-01\", periods=100, freq=\"D\"),\n    \"customer_id\": [f\"C{i % 20 + 1}\" for i in range(100)],\n    \"amount\": [float(i % 5 * 25 + 50) for i in range(100)]\n})\n\ntransactions = ibis.memtable(data)\n\n# Define period ranges for analysis\nperiod_ranges = {\n    \"Pre-Promotion\": (\"2023-01-01\", \"2023-01-31\"),\n    \"Promotion\": (\"2023-02-01\", \"2023-02-28\"),\n    \"Post-Promotion\": (\"2023-03-01\", \"2023-03-31\")\n}\n\n# Filter transactions to the defined periods and add period labels\nresult_df = filter_and_label_by_periods(transactions, period_ranges).execute()\n\n# Calculate KPIs by period\nresult_df.groupby(\"period_name\").agg(\n    transaction_count=(\"transaction_id\", \"count\"),\n    total_sales=(\"amount\", \"sum\"),\n    avg_transaction_value=(\"amount\", \"mean\")\n)\n</code></pre> period_name transaction_count total_sales avg_transaction_value Pre-Promotion 31 1937.5 62.50 Promotion 28 1750.0 62.50 Post-Promotion 31 1937.5 62.50"},{"location":"analysis_modules/#find-overlapping-periods","title":"Find Overlapping Periods","text":"<p>The Find Overlapping Periods module allows you to:</p> <ul> <li>Identify overlapping periods between a given start and end date.</li> <li>Split the date range into yearly periods that start from the given start date for the first period   and then yearly thereafter, ending on the provided end date.</li> <li>Return results either as ISO-formatted strings (<code>\"YYYY-MM-DD\"</code>) or as <code>datetime</code> objects.</li> </ul> <p>This functionality is particularly useful for:</p> <ul> <li>Analyzing seasonal or yearly patterns in datasets.</li> <li>Comparing data across specific date ranges.</li> <li>Structuring time-based segmentations efficiently.</li> </ul> <p>Example:</p> <pre><code>from datetime import datetime\nfrom pyretailscience.utils.date import find_overlapping_periods\n\n# Example with string input\noverlapping_periods = find_overlapping_periods(\"2022-06-15\", \"2025-03-10\")\nprint(overlapping_periods)\n</code></pre> Start Date End Date 2022-06-15 2023-03-10 2023-06-15 2024-03-10 2024-06-15 2025-03-10"},{"location":"analysis_modules/#filter-and-label-by-condition","title":"Filter and Label by Condition","text":"<p>The Filter and Label by Condition module allows you to:</p> <ul> <li>Filter data based on arbitrary conditions (e.g., category, region, price range)</li> <li>Add descriptive labels to filtered rows for easier segmentation</li> <li>Prepare labeled subsets for downstream analysis or visualization</li> <li>Combine multiple Boolean conditions into a single, labeled dataset</li> </ul> <p>This functionality is particularly useful for:</p> <ul> <li>Segmenting customers or products by custom-defined rules</li> <li>Categorizing transactions based on business logic</li> <li>Creating labeled training data for machine learning</li> <li>Analyzing metrics across different business segments</li> </ul> <p>Example:</p> <pre><code>import pandas as pd\nimport ibis\nfrom pyretailscience.utils.filter_and_label import filter_and_label_by_condition\n\n# Sample product table\ndf = pd.DataFrame({\n    \"product_id\": range(1, 9),\n    \"category\": [\"toys\", \"shoes\", \"toys\", \"books\", \"electronics\", \"toys\", \"shoes\", \"books\"],\n    \"price\": [15, 55, 25, 10, 200, 35, 60, 20]\n})\n\nproducts = ibis.memtable(df)\n\n# Define filter conditions\nconditions = {\n    \"Toys\": products[\"category\"] == \"toys\",\n    \"Shoes\": products[\"category\"] == \"shoes\",\n    \"Premium Electronics\": (products[\"category\"] == \"electronics\") &amp; (products[\"price\"] &gt; 100)\n}\n\n# Apply filtering and labeling\nlabeled_data = filter_and_label_by_condition(products, conditions).execute()\n</code></pre> product_id category price label 1 toys 15 Toys 2 shoes 55 Shoes 3 toys 25 Toys 5 electronics 200 Premium Electronics 6 toys 35 Toys 7 shoes 60 Shoes"},{"location":"versions/","title":"Documentation Versions","text":"<p>This documentation is available in multiple versions:</p> <ul> <li><code>latest</code> - The latest stable version of PyRetailScience</li> <li><code>dev</code> - The development version (from the main branch)</li> <li>Version-specific documentation (e.g., <code>1.0.0</code>, <code>1.1.0</code>, etc.)</li> </ul> <p>You can switch between versions using the version selector in the navigation bar.</p>"},{"location":"versions/#version-warning","title":"Version Warning","text":"<p>When viewing older versions of the documentation, you'll see a warning banner at the top of the page. This helps ensure you're aware that you might not be viewing the most recent documentation.</p>"},{"location":"versions/#accessing-specific-versions","title":"Accessing Specific Versions","text":"<p>You can directly access specific versions of the documentation with URLs in the format: <code>https://pyretailscience.datasimply.co/&lt;version&gt;/</code></p> <p>For example:</p> <ul> <li>Latest: <code>https://pyretailscience.datasimply.co/latest/</code></li> <li>Specific version: <code>https://pyretailscience.datasimply.co/1.0.0/</code></li> <li>Development: <code>https://pyretailscience.datasimply.co/dev/</code></li> </ul>"},{"location":"api/options/","title":"Options Analysis","text":"<p>This module provides a simplified implementation of a pandas-like options system.</p> <p>It allows users to get, set, and reset various options that control the behavior of data display and processing. The module also includes a context manager for temporarily changing options.</p> Example <p>set_option('display.max_rows', 100) print(get_option('display.max_rows')) 100 with option_context('display.max_rows', 10): ...     print(get_option('display.max_rows')) 10 print(get_option('display.max_rows')) 100</p>"},{"location":"api/options/#pyretailscience.options.ColumnHelper","title":"<code>ColumnHelper</code>","text":"<p>A class to help with column naming conventions.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>class ColumnHelper:\n    \"\"\"A class to help with column naming conventions.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"A class to help with column naming conventions.\"\"\"\n        # Date/Time\n        self.transaction_date = get_option(\"column.transaction_date\")\n        self.transaction_time = get_option(\"column.transaction_time\")\n        # Customers\n        self.customer_id = get_option(\"column.customer_id\")\n        self.agg_customer_id = get_option(\"column.agg.customer_id\")\n        self.agg_customer_id_p1 = self.join_options(\"column.agg.customer_id\", \"column.suffix.period_1\")\n        self.agg_customer_id_p2 = self.join_options(\"column.agg.customer_id\", \"column.suffix.period_2\")\n        self.agg_customer_id_diff = self.join_options(\"column.agg.customer_id\", \"column.suffix.difference\")\n        self.agg_customer_id_pct_diff = self.join_options(\"column.agg.customer_id\", \"column.suffix.percent_difference\")\n        self.agg_customer_id_contrib = self.join_options(\"column.agg.customer_id\", \"column.suffix.contribution\")\n        self.customers_pct = self.join_options(\"column.agg.customer_id\", \"column.suffix.percent\")\n        # Transactions\n        self.transaction_id = get_option(\"column.transaction_id\")\n        self.agg_transaction_id = get_option(\"column.agg.transaction_id\")\n        self.agg_transaction_id_p1 = self.join_options(\"column.agg.transaction_id\", \"column.suffix.period_1\")\n        self.agg_transaction_id_p2 = self.join_options(\"column.agg.transaction_id\", \"column.suffix.period_2\")\n        self.agg_transaction_id_diff = self.join_options(\"column.agg.transaction_id\", \"column.suffix.difference\")\n        self.agg_transaction_id_pct_diff = self.join_options(\n            \"column.agg.transaction_id\",\n            \"column.suffix.percent_difference\",\n        )\n        # Unit Spend\n        self.unit_spend = get_option(\"column.unit_spend\")\n        self.agg_unit_spend = get_option(\"column.agg.unit_spend\")\n        self.agg_unit_spend_p1 = self.join_options(\"column.agg.unit_spend\", \"column.suffix.period_1\")\n        self.agg_unit_spend_p2 = self.join_options(\"column.agg.unit_spend\", \"column.suffix.period_2\")\n        self.agg_unit_spend_diff = self.join_options(\"column.agg.unit_spend\", \"column.suffix.difference\")\n        self.agg_unit_spend_pct_diff = self.join_options(\"column.agg.unit_spend\", \"column.suffix.percent_difference\")\n        # Spend / Customer\n        self.calc_spend_per_cust = get_option(\"column.calc.spend_per_customer\")\n        self.calc_spend_per_cust_p1 = self.join_options(\"column.calc.spend_per_customer\", \"column.suffix.period_1\")\n        self.calc_spend_per_cust_p2 = self.join_options(\"column.calc.spend_per_customer\", \"column.suffix.period_2\")\n        self.calc_spend_per_cust_diff = self.join_options(\"column.calc.spend_per_customer\", \"column.suffix.difference\")\n        self.calc_spend_per_cust_pct_diff = self.join_options(\n            \"column.calc.spend_per_customer\",\n            \"column.suffix.percent_difference\",\n        )\n        self.calc_spend_per_cust_contrib = self.join_options(\n            \"column.calc.spend_per_customer\",\n            \"column.suffix.contribution\",\n        )\n        # Transactions / Customer\n        self.calc_trans_per_cust = get_option(\"column.calc.transactions_per_customer\")\n        self.calc_trans_per_cust_p1 = self.join_options(\n            \"column.calc.transactions_per_customer\",\n            \"column.suffix.period_1\",\n        )\n        self.calc_trans_per_cust_p2 = self.join_options(\n            \"column.calc.transactions_per_customer\",\n            \"column.suffix.period_2\",\n        )\n        self.calc_trans_per_cust_diff = self.join_options(\n            \"column.calc.transactions_per_customer\",\n            \"column.suffix.difference\",\n        )\n        self.calc_trans_per_cust_pct_diff = self.join_options(\n            \"column.calc.transactions_per_customer\",\n            \"column.suffix.percent_difference\",\n        )\n        self.calc_trans_per_cust_contrib = self.join_options(\n            \"column.calc.transactions_per_customer\",\n            \"column.suffix.contribution\",\n        )\n        # Spend / Transaction\n        self.calc_spend_per_trans = get_option(\"column.calc.spend_per_transaction\")\n        self.calc_spend_per_trans_p1 = self.join_options(\"column.calc.spend_per_transaction\", \"column.suffix.period_1\")\n        self.calc_spend_per_trans_p2 = self.join_options(\"column.calc.spend_per_transaction\", \"column.suffix.period_2\")\n        self.calc_spend_per_trans_diff = self.join_options(\n            \"column.calc.spend_per_transaction\",\n            \"column.suffix.difference\",\n        )\n        self.calc_spend_per_trans_pct_diff = self.join_options(\n            \"column.calc.spend_per_transaction\",\n            \"column.suffix.percent_difference\",\n        )\n        self.calc_spend_per_trans_contrib = self.join_options(\n            \"column.calc.spend_per_transaction\",\n            \"column.suffix.contribution\",\n        )\n        # Unit Quantity\n        self.unit_qty = get_option(\"column.unit_quantity\")\n        self.agg_unit_qty = get_option(\"column.agg.unit_quantity\")\n        self.agg_unit_qty_p1 = self.join_options(\"column.agg.unit_quantity\", \"column.suffix.period_1\")\n        self.agg_unit_qty_p2 = self.join_options(\"column.agg.unit_quantity\", \"column.suffix.period_2\")\n        self.agg_unit_qty_diff = self.join_options(\"column.agg.unit_quantity\", \"column.suffix.difference\")\n        self.agg_unit_qty_pct_diff = self.join_options(\"column.agg.unit_quantity\", \"column.suffix.percent_difference\")\n        # Units / Transaction\n        self.calc_units_per_trans = get_option(\"column.calc.units_per_transaction\")\n        self.calc_units_per_trans_p1 = self.join_options(\"column.calc.units_per_transaction\", \"column.suffix.period_1\")\n        self.calc_units_per_trans_p2 = self.join_options(\"column.calc.units_per_transaction\", \"column.suffix.period_2\")\n        self.calc_units_per_trans_diff = self.join_options(\n            \"column.calc.units_per_transaction\",\n            \"column.suffix.difference\",\n        )\n        self.calc_units_per_trans_pct_diff = self.join_options(\n            \"column.calc.units_per_transaction\",\n            \"column.suffix.percent_difference\",\n        )\n        self.calc_units_per_trans_contrib = self.join_options(\n            \"column.calc.units_per_transaction\",\n            \"column.suffix.contribution\",\n        )\n        # Price / Unit\n        self.calc_price_per_unit = get_option(\"column.calc.price_per_unit\")\n        self.calc_price_per_unit_p1 = self.join_options(\"column.calc.price_per_unit\", \"column.suffix.period_1\")\n        self.calc_price_per_unit_p2 = self.join_options(\"column.calc.price_per_unit\", \"column.suffix.period_2\")\n        self.calc_price_per_unit_diff = self.join_options(\"column.calc.price_per_unit\", \"column.suffix.difference\")\n        self.calc_price_per_unit_pct_diff = self.join_options(\n            \"column.calc.price_per_unit\",\n            \"column.suffix.percent_difference\",\n        )\n        self.calc_price_per_unit_contrib = self.join_options(\"column.calc.price_per_unit\", \"column.suffix.contribution\")\n        # Cost\n        self.unit_cost = get_option(\"column.unit_cost\")\n        self.agg_unit_cost = get_option(\"column.agg.unit_cost\")\n        self.agg_unit_cost_p1 = self.join_options(\"column.agg.unit_cost\", \"column.suffix.period_1\")\n        self.agg_unit_cost_p2 = self.join_options(\"column.agg.unit_cost\", \"column.suffix.period_2\")\n        self.agg_unit_cost_diff = self.join_options(\"column.agg.unit_cost\", \"column.suffix.difference\")\n        self.agg_unit_cost_pct_diff = self.join_options(\"column.agg.unit_cost\", \"column.suffix.percent_difference\")\n        # Promo Unit Spend\n        self.promo_unit_spend = get_option(\"column.promo_unit_spend\")\n        self.agg_promo_unit_spend = get_option(\"column.agg.promo_unit_spend\")\n        self.agg_promo_unit_spend_p1 = self.join_options(\"column.agg.promo_unit_spend\", \"column.suffix.period_1\")\n        self.agg_promo_unit_spend_p2 = self.join_options(\"column.agg.promo_unit_spend\", \"column.suffix.period_2\")\n        self.agg_promo_unit_spend_diff = self.join_options(\"column.agg.promo_unit_spend\", \"column.suffix.difference\")\n        self.agg_promo_unit_spend_pct_diff = self.join_options(\n            \"column.agg.promo_unit_spend\",\n            \"column.suffix.percent_difference\",\n        )\n        # Promo Unit Quantity\n        self.promo_unit_qty = get_option(\"column.promo_unit_quantity\")\n        self.agg_promo_unit_qty = get_option(\"column.agg.promo_unit_quantity\")\n        self.agg_promo_unit_qty_p1 = self.join_options(\"column.agg.promo_unit_quantity\", \"column.suffix.period_1\")\n        self.agg_promo_unit_qty_p2 = self.join_options(\"column.agg.promo_unit_quantity\", \"column.suffix.period_2\")\n        self.agg_promo_unit_qty_diff = self.join_options(\"column.agg.promo_unit_quantity\", \"column.suffix.difference\")\n        self.agg_promo_unit_qty_pct_diff = self.join_options(\n            \"column.agg.promo_unit_quantity\",\n            \"column.suffix.percent_difference\",\n        )\n        # Elasticity\n        self.calc_price_elasticity = get_option(\"column.calc.price_elasticity\")\n        self.calc_frequency_elasticity = get_option(\"column.calc.frequency_elasticity\")\n\n    @staticmethod\n    def join_options(*args: str, sep: str = \"_\") -&gt; str:\n        \"\"\"A helper function to join multiple options together.\"\"\"\n        return sep.join(map(get_option, args))\n</code></pre>"},{"location":"api/options/#pyretailscience.options.ColumnHelper.__init__","title":"<code>__init__()</code>","text":"<p>A class to help with column naming conventions.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"A class to help with column naming conventions.\"\"\"\n    # Date/Time\n    self.transaction_date = get_option(\"column.transaction_date\")\n    self.transaction_time = get_option(\"column.transaction_time\")\n    # Customers\n    self.customer_id = get_option(\"column.customer_id\")\n    self.agg_customer_id = get_option(\"column.agg.customer_id\")\n    self.agg_customer_id_p1 = self.join_options(\"column.agg.customer_id\", \"column.suffix.period_1\")\n    self.agg_customer_id_p2 = self.join_options(\"column.agg.customer_id\", \"column.suffix.period_2\")\n    self.agg_customer_id_diff = self.join_options(\"column.agg.customer_id\", \"column.suffix.difference\")\n    self.agg_customer_id_pct_diff = self.join_options(\"column.agg.customer_id\", \"column.suffix.percent_difference\")\n    self.agg_customer_id_contrib = self.join_options(\"column.agg.customer_id\", \"column.suffix.contribution\")\n    self.customers_pct = self.join_options(\"column.agg.customer_id\", \"column.suffix.percent\")\n    # Transactions\n    self.transaction_id = get_option(\"column.transaction_id\")\n    self.agg_transaction_id = get_option(\"column.agg.transaction_id\")\n    self.agg_transaction_id_p1 = self.join_options(\"column.agg.transaction_id\", \"column.suffix.period_1\")\n    self.agg_transaction_id_p2 = self.join_options(\"column.agg.transaction_id\", \"column.suffix.period_2\")\n    self.agg_transaction_id_diff = self.join_options(\"column.agg.transaction_id\", \"column.suffix.difference\")\n    self.agg_transaction_id_pct_diff = self.join_options(\n        \"column.agg.transaction_id\",\n        \"column.suffix.percent_difference\",\n    )\n    # Unit Spend\n    self.unit_spend = get_option(\"column.unit_spend\")\n    self.agg_unit_spend = get_option(\"column.agg.unit_spend\")\n    self.agg_unit_spend_p1 = self.join_options(\"column.agg.unit_spend\", \"column.suffix.period_1\")\n    self.agg_unit_spend_p2 = self.join_options(\"column.agg.unit_spend\", \"column.suffix.period_2\")\n    self.agg_unit_spend_diff = self.join_options(\"column.agg.unit_spend\", \"column.suffix.difference\")\n    self.agg_unit_spend_pct_diff = self.join_options(\"column.agg.unit_spend\", \"column.suffix.percent_difference\")\n    # Spend / Customer\n    self.calc_spend_per_cust = get_option(\"column.calc.spend_per_customer\")\n    self.calc_spend_per_cust_p1 = self.join_options(\"column.calc.spend_per_customer\", \"column.suffix.period_1\")\n    self.calc_spend_per_cust_p2 = self.join_options(\"column.calc.spend_per_customer\", \"column.suffix.period_2\")\n    self.calc_spend_per_cust_diff = self.join_options(\"column.calc.spend_per_customer\", \"column.suffix.difference\")\n    self.calc_spend_per_cust_pct_diff = self.join_options(\n        \"column.calc.spend_per_customer\",\n        \"column.suffix.percent_difference\",\n    )\n    self.calc_spend_per_cust_contrib = self.join_options(\n        \"column.calc.spend_per_customer\",\n        \"column.suffix.contribution\",\n    )\n    # Transactions / Customer\n    self.calc_trans_per_cust = get_option(\"column.calc.transactions_per_customer\")\n    self.calc_trans_per_cust_p1 = self.join_options(\n        \"column.calc.transactions_per_customer\",\n        \"column.suffix.period_1\",\n    )\n    self.calc_trans_per_cust_p2 = self.join_options(\n        \"column.calc.transactions_per_customer\",\n        \"column.suffix.period_2\",\n    )\n    self.calc_trans_per_cust_diff = self.join_options(\n        \"column.calc.transactions_per_customer\",\n        \"column.suffix.difference\",\n    )\n    self.calc_trans_per_cust_pct_diff = self.join_options(\n        \"column.calc.transactions_per_customer\",\n        \"column.suffix.percent_difference\",\n    )\n    self.calc_trans_per_cust_contrib = self.join_options(\n        \"column.calc.transactions_per_customer\",\n        \"column.suffix.contribution\",\n    )\n    # Spend / Transaction\n    self.calc_spend_per_trans = get_option(\"column.calc.spend_per_transaction\")\n    self.calc_spend_per_trans_p1 = self.join_options(\"column.calc.spend_per_transaction\", \"column.suffix.period_1\")\n    self.calc_spend_per_trans_p2 = self.join_options(\"column.calc.spend_per_transaction\", \"column.suffix.period_2\")\n    self.calc_spend_per_trans_diff = self.join_options(\n        \"column.calc.spend_per_transaction\",\n        \"column.suffix.difference\",\n    )\n    self.calc_spend_per_trans_pct_diff = self.join_options(\n        \"column.calc.spend_per_transaction\",\n        \"column.suffix.percent_difference\",\n    )\n    self.calc_spend_per_trans_contrib = self.join_options(\n        \"column.calc.spend_per_transaction\",\n        \"column.suffix.contribution\",\n    )\n    # Unit Quantity\n    self.unit_qty = get_option(\"column.unit_quantity\")\n    self.agg_unit_qty = get_option(\"column.agg.unit_quantity\")\n    self.agg_unit_qty_p1 = self.join_options(\"column.agg.unit_quantity\", \"column.suffix.period_1\")\n    self.agg_unit_qty_p2 = self.join_options(\"column.agg.unit_quantity\", \"column.suffix.period_2\")\n    self.agg_unit_qty_diff = self.join_options(\"column.agg.unit_quantity\", \"column.suffix.difference\")\n    self.agg_unit_qty_pct_diff = self.join_options(\"column.agg.unit_quantity\", \"column.suffix.percent_difference\")\n    # Units / Transaction\n    self.calc_units_per_trans = get_option(\"column.calc.units_per_transaction\")\n    self.calc_units_per_trans_p1 = self.join_options(\"column.calc.units_per_transaction\", \"column.suffix.period_1\")\n    self.calc_units_per_trans_p2 = self.join_options(\"column.calc.units_per_transaction\", \"column.suffix.period_2\")\n    self.calc_units_per_trans_diff = self.join_options(\n        \"column.calc.units_per_transaction\",\n        \"column.suffix.difference\",\n    )\n    self.calc_units_per_trans_pct_diff = self.join_options(\n        \"column.calc.units_per_transaction\",\n        \"column.suffix.percent_difference\",\n    )\n    self.calc_units_per_trans_contrib = self.join_options(\n        \"column.calc.units_per_transaction\",\n        \"column.suffix.contribution\",\n    )\n    # Price / Unit\n    self.calc_price_per_unit = get_option(\"column.calc.price_per_unit\")\n    self.calc_price_per_unit_p1 = self.join_options(\"column.calc.price_per_unit\", \"column.suffix.period_1\")\n    self.calc_price_per_unit_p2 = self.join_options(\"column.calc.price_per_unit\", \"column.suffix.period_2\")\n    self.calc_price_per_unit_diff = self.join_options(\"column.calc.price_per_unit\", \"column.suffix.difference\")\n    self.calc_price_per_unit_pct_diff = self.join_options(\n        \"column.calc.price_per_unit\",\n        \"column.suffix.percent_difference\",\n    )\n    self.calc_price_per_unit_contrib = self.join_options(\"column.calc.price_per_unit\", \"column.suffix.contribution\")\n    # Cost\n    self.unit_cost = get_option(\"column.unit_cost\")\n    self.agg_unit_cost = get_option(\"column.agg.unit_cost\")\n    self.agg_unit_cost_p1 = self.join_options(\"column.agg.unit_cost\", \"column.suffix.period_1\")\n    self.agg_unit_cost_p2 = self.join_options(\"column.agg.unit_cost\", \"column.suffix.period_2\")\n    self.agg_unit_cost_diff = self.join_options(\"column.agg.unit_cost\", \"column.suffix.difference\")\n    self.agg_unit_cost_pct_diff = self.join_options(\"column.agg.unit_cost\", \"column.suffix.percent_difference\")\n    # Promo Unit Spend\n    self.promo_unit_spend = get_option(\"column.promo_unit_spend\")\n    self.agg_promo_unit_spend = get_option(\"column.agg.promo_unit_spend\")\n    self.agg_promo_unit_spend_p1 = self.join_options(\"column.agg.promo_unit_spend\", \"column.suffix.period_1\")\n    self.agg_promo_unit_spend_p2 = self.join_options(\"column.agg.promo_unit_spend\", \"column.suffix.period_2\")\n    self.agg_promo_unit_spend_diff = self.join_options(\"column.agg.promo_unit_spend\", \"column.suffix.difference\")\n    self.agg_promo_unit_spend_pct_diff = self.join_options(\n        \"column.agg.promo_unit_spend\",\n        \"column.suffix.percent_difference\",\n    )\n    # Promo Unit Quantity\n    self.promo_unit_qty = get_option(\"column.promo_unit_quantity\")\n    self.agg_promo_unit_qty = get_option(\"column.agg.promo_unit_quantity\")\n    self.agg_promo_unit_qty_p1 = self.join_options(\"column.agg.promo_unit_quantity\", \"column.suffix.period_1\")\n    self.agg_promo_unit_qty_p2 = self.join_options(\"column.agg.promo_unit_quantity\", \"column.suffix.period_2\")\n    self.agg_promo_unit_qty_diff = self.join_options(\"column.agg.promo_unit_quantity\", \"column.suffix.difference\")\n    self.agg_promo_unit_qty_pct_diff = self.join_options(\n        \"column.agg.promo_unit_quantity\",\n        \"column.suffix.percent_difference\",\n    )\n    # Elasticity\n    self.calc_price_elasticity = get_option(\"column.calc.price_elasticity\")\n    self.calc_frequency_elasticity = get_option(\"column.calc.frequency_elasticity\")\n</code></pre>"},{"location":"api/options/#pyretailscience.options.ColumnHelper.join_options","title":"<code>join_options(*args, sep='_')</code>  <code>staticmethod</code>","text":"<p>A helper function to join multiple options together.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>@staticmethod\ndef join_options(*args: str, sep: str = \"_\") -&gt; str:\n    \"\"\"A helper function to join multiple options together.\"\"\"\n    return sep.join(map(get_option, args))\n</code></pre>"},{"location":"api/options/#pyretailscience.options.Options","title":"<code>Options</code>","text":"<p>A class to manage configurable options.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>class Options:\n    \"\"\"A class to manage configurable options.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes the options with default values.\"\"\"\n        self._options: dict[str, OptionTypes] = {\n            # Database columns\n            \"column.customer_id\": \"customer_id\",\n            \"column.transaction_id\": \"transaction_id\",\n            \"column.transaction_date\": \"transaction_date\",\n            \"column.transaction_time\": \"transaction_time\",\n            \"column.product_id\": \"product_id\",\n            \"column.unit_quantity\": \"unit_quantity\",\n            \"column.unit_price\": \"unit_price\",\n            \"column.unit_spend\": \"unit_spend\",\n            \"column.unit_cost\": \"unit_cost\",\n            \"column.promo_unit_spend\": \"promo_unit_spend\",\n            \"column.promo_unit_quantity\": \"promo_unit_quantity\",\n            \"column.store_id\": \"store_id\",\n            # Aggregation columns\n            \"column.agg.customer_id\": \"customers\",\n            \"column.agg.transaction_id\": \"transactions\",\n            \"column.agg.product_id\": \"products\",\n            \"column.agg.unit_quantity\": \"units\",\n            \"column.agg.unit_price\": \"prices\",\n            \"column.agg.unit_spend\": \"spend\",\n            \"column.agg.unit_cost\": \"costs\",\n            \"column.agg.promo_unit_spend\": \"promo_spend\",\n            \"column.agg.promo_unit_quantity\": \"promo_quantity\",\n            \"column.agg.store_id\": \"stores\",\n            # Calculated columns\n            \"column.calc.price_per_unit\": \"price_per_unit\",\n            \"column.calc.units_per_transaction\": \"units_per_transaction\",\n            \"column.calc.spend_per_customer\": \"spend_per_customer\",\n            \"column.calc.spend_per_transaction\": \"spend_per_transaction\",\n            \"column.calc.transactions_per_customer\": \"transactions_per_customer\",\n            \"column.calc.price_elasticity\": \"price_elasticity\",\n            \"column.calc.frequency_elasticity\": \"frequency_elasticity\",\n            # Abbreviation suffix\n            \"column.suffix.count\": \"cnt\",\n            \"column.suffix.percent\": \"pct\",\n            \"column.suffix.difference\": \"diff\",\n            \"column.suffix.percent_difference\": \"pct_diff\",\n            \"column.suffix.contribution\": \"contrib\",\n            \"column.suffix.period_1\": \"p1\",\n            \"column.suffix.period_2\": \"p2\",\n        }\n        self._descriptions: dict[str, str] = {\n            # Database columns\n            \"column.customer_id\": \"The name of the column containing customer IDs.\",\n            \"column.transaction_id\": \"The name of the column containing transaction IDs.\",\n            \"column.transaction_date\": \"The name of the column containing transaction dates.\",\n            \"column.transaction_time\": \"The name of the column containing transaction times.\",\n            \"column.product_id\": \"The name of the column containing product IDs.\",\n            \"column.unit_quantity\": \"The name of the column containing the number of units sold.\",\n            \"column.unit_price\": \"The name of the column containing the unit price of the product.\",\n            \"column.unit_spend\": (\n                \"The name of the column containing the total spend of the products in the transaction.\"\n                \"ie, unit_price * units\",\n            ),\n            \"column.unit_cost\": (\n                \"The name of the column containing the total cost of the products in the transaction. \"\n                \"ie, single unit cost * units\",\n            ),\n            \"column.promo_unit_spend\": (\n                \"The name of the column containing the total spend on promotion of the products in the transaction. \"\n                \"ie, promotional unit price * units\",\n            ),\n            \"column.promo_unit_quantity\": (\"The name of the column containing the number of units sold on promotion.\"),\n            \"column.store_id\": \"The name of the column containing store IDs of the transaction.\",\n            # Aggregation columns\n            \"column.agg.customer_id\": \"The name of the column containing the number of unique customers.\",\n            \"column.agg.transaction_id\": \"The name of the column containing the number of transactions.\",\n            \"column.agg.product_id\": \"The name of the column containing the number of unique products.\",\n            \"column.agg.unit_quantity\": \"The name of the column containing the total number of units sold.\",\n            \"column.agg.unit_price\": \"The name of the column containing the total unit price of products.\",\n            \"column.agg.unit_spend\": (\n                \"The name of the column containing the total spend of the units in the transaction.\"\n            ),\n            \"column.agg.unit_cost\": \"The name of the column containing the total unit cost of products.\",\n            \"column.agg.promo_unit_spend\": (\n                \"The name of the column containing the total promotional spend of the units in the transaction.\"\n            ),\n            \"column.agg.promo_unit_quantity\": (\n                \"The name of the column containing the total number of units sold on promotion.\"\n            ),\n            \"column.agg.store_id\": \"The name of the column containing the number of unique stores.\",\n            # Calculated columns\n            \"column.calc.price_per_unit\": \"The name of the column containing the price per unit.\",\n            \"column.calc.units_per_transaction\": \"The name of the column containing the units per transaction.\",\n            \"column.calc.spend_per_customer\": \"The name of the column containing the spend per customer.\",\n            \"column.calc.spend_per_transaction\": \"The name of the column containing the spend per transaction.\",\n            \"column.calc.transactions_per_customer\": \"The name of the column containing the transactions per customer.\",\n            \"column.calc.price_elasticity\": \"The name of the column containing the price elasticity calculation.\",\n            \"column.calc.frequency_elasticity\": \"The name of the column containing the price frequency calculation.\",\n            # Abbreviation suffixes\n            \"column.suffix.count\": \"The suffix to use for count columns.\",\n            \"column.suffix.percent\": \"The suffix to use for percentage columns.\",\n            \"column.suffix.difference\": \"The suffix to use for difference columns.\",\n            \"column.suffix.percent_difference\": \"The suffix to use for percentage difference columns.\",\n            \"column.suffix.contribution\": \"The suffix to use for revenue contribution columns.\",\n            \"column.suffix.period_1\": (\n                \"The suffix to use for period 1 columns. Often this could represent last year for instance.\"\n            ),\n            \"column.suffix.period_2\": (\n                \"The suffix to use for period 2 columns. Often this could represent this year for instance.\"\n            ),\n        }\n        self._default_options: dict[str, OptionTypes] = self._options.copy()\n\n    def set_option(self, pat: str, val: OptionTypes) -&gt; None:\n        \"\"\"Set the value of the specified option.\n\n        Args:\n            pat: The option name.\n            val: The value to set the option to.\n\n        Raises:\n            ValueError: If the option name is unknown.\n        \"\"\"\n        if pat not in self._options:\n            msg = f\"Unknown option: {pat}\"\n            raise ValueError(msg)\n\n        self._options[pat] = val\n\n    def get_option(self, pat: str) -&gt; OptionTypes:\n        \"\"\"Get the value of the specified option.\n\n        Args:\n            pat: The option name.\n\n        Returns:\n            The value of the option.\n\n        Raises:\n            ValueError: If the option name is unknown.\n        \"\"\"\n        if pat in self._options:\n            return self._options[pat]\n\n        msg = f\"Unknown option: {pat}\"\n        raise ValueError(msg)\n\n    def reset_option(self, pat: str) -&gt; None:\n        \"\"\"Reset the specified option to its default value.\n\n        Args:\n            pat: The option name.\n\n        Raises:\n            ValueError: If the option name is unknown.\n        \"\"\"\n        if pat not in self._options:\n            msg = f\"Unknown option: {pat}\"\n            raise ValueError(msg)\n\n        self._options[pat] = self._default_options[pat]\n\n    def list_options(self) -&gt; list[str]:\n        \"\"\"List all available options.\n\n        Returns:\n            A list of all option names.\n        \"\"\"\n        return list(self._options.keys())\n\n    def describe_option(self, pat: str) -&gt; str:\n        \"\"\"Describe the specified option.\n\n        Args:\n            pat: The option name.\n\n        Returns:\n            A string describing the option and its current value.\n\n        Raises:\n            ValueError: If the option name is unknown.\n        \"\"\"\n        if pat in self._descriptions:\n            return f\"{pat}: {self._descriptions[pat]} (current value: {self._options[pat]})\"\n\n        msg = f\"Unknown option: {pat}\"\n        raise ValueError(msg)\n\n    @staticmethod\n    def flatten_options(k: str, v: OptionTypes, parent_key: str = \"\") -&gt; dict[str, OptionTypes]:\n        \"\"\"Flatten nested options into a single dictionary.\"\"\"\n        if parent_key != \"\":\n            parent_key += \".\"\n\n        if isinstance(v, dict):\n            ret_dict = {}\n            for sub_key, sub_value in v.items():\n                ret_dict.update(Options.flatten_options(sub_key, sub_value, parent_key=f\"{parent_key}{k}\"))\n            return ret_dict\n\n        return {f\"{parent_key}{k}\": v}\n\n    @classmethod\n    def load_from_project(cls) -&gt; \"Options\":\n        \"\"\"Try to load options from a pyretailscience.toml file in the project root directory.\n\n        If the project root directory cannot be found, return a default Options instance.\n\n        Returns:\n            An Options instance with options loaded from the pyretailscience.toml file or default\n        \"\"\"\n        options_instance = cls()\n\n        project_root = find_project_root()\n        if project_root is None:\n            return options_instance\n\n        toml_file = Path(project_root) / \"pyretailscience.toml\"\n        if toml_file.is_file():\n            return Options.load_from_toml(toml_file)\n\n        return options_instance\n\n    @classmethod\n    def load_from_toml(cls, file_path: str) -&gt; \"Options\":\n        \"\"\"Load options from a TOML file.\n\n        Args:\n            file_path: The path to the TOML file.\n\n        Raises:\n            ValueError: If the TOML file contains unknown options.\n        \"\"\"\n        options_instance = cls()\n\n        with open(file_path) as f:\n            toml_data = toml.load(f)\n\n        for section, options in toml_data.items():\n            for option_name, option_value in Options.flatten_options(section, options).items():\n                if option_name in options_instance._options:\n                    options_instance.set_option(option_name, option_value)\n                else:\n                    msg = f\"Unknown option in TOML file: {option_name}\"\n                    raise ValueError(msg)\n\n        return options_instance\n</code></pre>"},{"location":"api/options/#pyretailscience.options.Options.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the options with default values.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the options with default values.\"\"\"\n    self._options: dict[str, OptionTypes] = {\n        # Database columns\n        \"column.customer_id\": \"customer_id\",\n        \"column.transaction_id\": \"transaction_id\",\n        \"column.transaction_date\": \"transaction_date\",\n        \"column.transaction_time\": \"transaction_time\",\n        \"column.product_id\": \"product_id\",\n        \"column.unit_quantity\": \"unit_quantity\",\n        \"column.unit_price\": \"unit_price\",\n        \"column.unit_spend\": \"unit_spend\",\n        \"column.unit_cost\": \"unit_cost\",\n        \"column.promo_unit_spend\": \"promo_unit_spend\",\n        \"column.promo_unit_quantity\": \"promo_unit_quantity\",\n        \"column.store_id\": \"store_id\",\n        # Aggregation columns\n        \"column.agg.customer_id\": \"customers\",\n        \"column.agg.transaction_id\": \"transactions\",\n        \"column.agg.product_id\": \"products\",\n        \"column.agg.unit_quantity\": \"units\",\n        \"column.agg.unit_price\": \"prices\",\n        \"column.agg.unit_spend\": \"spend\",\n        \"column.agg.unit_cost\": \"costs\",\n        \"column.agg.promo_unit_spend\": \"promo_spend\",\n        \"column.agg.promo_unit_quantity\": \"promo_quantity\",\n        \"column.agg.store_id\": \"stores\",\n        # Calculated columns\n        \"column.calc.price_per_unit\": \"price_per_unit\",\n        \"column.calc.units_per_transaction\": \"units_per_transaction\",\n        \"column.calc.spend_per_customer\": \"spend_per_customer\",\n        \"column.calc.spend_per_transaction\": \"spend_per_transaction\",\n        \"column.calc.transactions_per_customer\": \"transactions_per_customer\",\n        \"column.calc.price_elasticity\": \"price_elasticity\",\n        \"column.calc.frequency_elasticity\": \"frequency_elasticity\",\n        # Abbreviation suffix\n        \"column.suffix.count\": \"cnt\",\n        \"column.suffix.percent\": \"pct\",\n        \"column.suffix.difference\": \"diff\",\n        \"column.suffix.percent_difference\": \"pct_diff\",\n        \"column.suffix.contribution\": \"contrib\",\n        \"column.suffix.period_1\": \"p1\",\n        \"column.suffix.period_2\": \"p2\",\n    }\n    self._descriptions: dict[str, str] = {\n        # Database columns\n        \"column.customer_id\": \"The name of the column containing customer IDs.\",\n        \"column.transaction_id\": \"The name of the column containing transaction IDs.\",\n        \"column.transaction_date\": \"The name of the column containing transaction dates.\",\n        \"column.transaction_time\": \"The name of the column containing transaction times.\",\n        \"column.product_id\": \"The name of the column containing product IDs.\",\n        \"column.unit_quantity\": \"The name of the column containing the number of units sold.\",\n        \"column.unit_price\": \"The name of the column containing the unit price of the product.\",\n        \"column.unit_spend\": (\n            \"The name of the column containing the total spend of the products in the transaction.\"\n            \"ie, unit_price * units\",\n        ),\n        \"column.unit_cost\": (\n            \"The name of the column containing the total cost of the products in the transaction. \"\n            \"ie, single unit cost * units\",\n        ),\n        \"column.promo_unit_spend\": (\n            \"The name of the column containing the total spend on promotion of the products in the transaction. \"\n            \"ie, promotional unit price * units\",\n        ),\n        \"column.promo_unit_quantity\": (\"The name of the column containing the number of units sold on promotion.\"),\n        \"column.store_id\": \"The name of the column containing store IDs of the transaction.\",\n        # Aggregation columns\n        \"column.agg.customer_id\": \"The name of the column containing the number of unique customers.\",\n        \"column.agg.transaction_id\": \"The name of the column containing the number of transactions.\",\n        \"column.agg.product_id\": \"The name of the column containing the number of unique products.\",\n        \"column.agg.unit_quantity\": \"The name of the column containing the total number of units sold.\",\n        \"column.agg.unit_price\": \"The name of the column containing the total unit price of products.\",\n        \"column.agg.unit_spend\": (\n            \"The name of the column containing the total spend of the units in the transaction.\"\n        ),\n        \"column.agg.unit_cost\": \"The name of the column containing the total unit cost of products.\",\n        \"column.agg.promo_unit_spend\": (\n            \"The name of the column containing the total promotional spend of the units in the transaction.\"\n        ),\n        \"column.agg.promo_unit_quantity\": (\n            \"The name of the column containing the total number of units sold on promotion.\"\n        ),\n        \"column.agg.store_id\": \"The name of the column containing the number of unique stores.\",\n        # Calculated columns\n        \"column.calc.price_per_unit\": \"The name of the column containing the price per unit.\",\n        \"column.calc.units_per_transaction\": \"The name of the column containing the units per transaction.\",\n        \"column.calc.spend_per_customer\": \"The name of the column containing the spend per customer.\",\n        \"column.calc.spend_per_transaction\": \"The name of the column containing the spend per transaction.\",\n        \"column.calc.transactions_per_customer\": \"The name of the column containing the transactions per customer.\",\n        \"column.calc.price_elasticity\": \"The name of the column containing the price elasticity calculation.\",\n        \"column.calc.frequency_elasticity\": \"The name of the column containing the price frequency calculation.\",\n        # Abbreviation suffixes\n        \"column.suffix.count\": \"The suffix to use for count columns.\",\n        \"column.suffix.percent\": \"The suffix to use for percentage columns.\",\n        \"column.suffix.difference\": \"The suffix to use for difference columns.\",\n        \"column.suffix.percent_difference\": \"The suffix to use for percentage difference columns.\",\n        \"column.suffix.contribution\": \"The suffix to use for revenue contribution columns.\",\n        \"column.suffix.period_1\": (\n            \"The suffix to use for period 1 columns. Often this could represent last year for instance.\"\n        ),\n        \"column.suffix.period_2\": (\n            \"The suffix to use for period 2 columns. Often this could represent this year for instance.\"\n        ),\n    }\n    self._default_options: dict[str, OptionTypes] = self._options.copy()\n</code></pre>"},{"location":"api/options/#pyretailscience.options.Options.describe_option","title":"<code>describe_option(pat)</code>","text":"<p>Describe the specified option.</p> <p>Parameters:</p> Name Type Description Default <code>pat</code> <code>str</code> <p>The option name.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string describing the option and its current value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the option name is unknown.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def describe_option(self, pat: str) -&gt; str:\n    \"\"\"Describe the specified option.\n\n    Args:\n        pat: The option name.\n\n    Returns:\n        A string describing the option and its current value.\n\n    Raises:\n        ValueError: If the option name is unknown.\n    \"\"\"\n    if pat in self._descriptions:\n        return f\"{pat}: {self._descriptions[pat]} (current value: {self._options[pat]})\"\n\n    msg = f\"Unknown option: {pat}\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/options/#pyretailscience.options.Options.flatten_options","title":"<code>flatten_options(k, v, parent_key='')</code>  <code>staticmethod</code>","text":"<p>Flatten nested options into a single dictionary.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>@staticmethod\ndef flatten_options(k: str, v: OptionTypes, parent_key: str = \"\") -&gt; dict[str, OptionTypes]:\n    \"\"\"Flatten nested options into a single dictionary.\"\"\"\n    if parent_key != \"\":\n        parent_key += \".\"\n\n    if isinstance(v, dict):\n        ret_dict = {}\n        for sub_key, sub_value in v.items():\n            ret_dict.update(Options.flatten_options(sub_key, sub_value, parent_key=f\"{parent_key}{k}\"))\n        return ret_dict\n\n    return {f\"{parent_key}{k}\": v}\n</code></pre>"},{"location":"api/options/#pyretailscience.options.Options.get_option","title":"<code>get_option(pat)</code>","text":"<p>Get the value of the specified option.</p> <p>Parameters:</p> Name Type Description Default <code>pat</code> <code>str</code> <p>The option name.</p> required <p>Returns:</p> Type Description <code>OptionTypes</code> <p>The value of the option.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the option name is unknown.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def get_option(self, pat: str) -&gt; OptionTypes:\n    \"\"\"Get the value of the specified option.\n\n    Args:\n        pat: The option name.\n\n    Returns:\n        The value of the option.\n\n    Raises:\n        ValueError: If the option name is unknown.\n    \"\"\"\n    if pat in self._options:\n        return self._options[pat]\n\n    msg = f\"Unknown option: {pat}\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/options/#pyretailscience.options.Options.list_options","title":"<code>list_options()</code>","text":"<p>List all available options.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of all option names.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def list_options(self) -&gt; list[str]:\n    \"\"\"List all available options.\n\n    Returns:\n        A list of all option names.\n    \"\"\"\n    return list(self._options.keys())\n</code></pre>"},{"location":"api/options/#pyretailscience.options.Options.load_from_project","title":"<code>load_from_project()</code>  <code>classmethod</code>","text":"<p>Try to load options from a pyretailscience.toml file in the project root directory.</p> <p>If the project root directory cannot be found, return a default Options instance.</p> <p>Returns:</p> Type Description <code>Options</code> <p>An Options instance with options loaded from the pyretailscience.toml file or default</p> Source code in <code>pyretailscience/options.py</code> <pre><code>@classmethod\ndef load_from_project(cls) -&gt; \"Options\":\n    \"\"\"Try to load options from a pyretailscience.toml file in the project root directory.\n\n    If the project root directory cannot be found, return a default Options instance.\n\n    Returns:\n        An Options instance with options loaded from the pyretailscience.toml file or default\n    \"\"\"\n    options_instance = cls()\n\n    project_root = find_project_root()\n    if project_root is None:\n        return options_instance\n\n    toml_file = Path(project_root) / \"pyretailscience.toml\"\n    if toml_file.is_file():\n        return Options.load_from_toml(toml_file)\n\n    return options_instance\n</code></pre>"},{"location":"api/options/#pyretailscience.options.Options.load_from_toml","title":"<code>load_from_toml(file_path)</code>  <code>classmethod</code>","text":"<p>Load options from a TOML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the TOML file.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the TOML file contains unknown options.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>@classmethod\ndef load_from_toml(cls, file_path: str) -&gt; \"Options\":\n    \"\"\"Load options from a TOML file.\n\n    Args:\n        file_path: The path to the TOML file.\n\n    Raises:\n        ValueError: If the TOML file contains unknown options.\n    \"\"\"\n    options_instance = cls()\n\n    with open(file_path) as f:\n        toml_data = toml.load(f)\n\n    for section, options in toml_data.items():\n        for option_name, option_value in Options.flatten_options(section, options).items():\n            if option_name in options_instance._options:\n                options_instance.set_option(option_name, option_value)\n            else:\n                msg = f\"Unknown option in TOML file: {option_name}\"\n                raise ValueError(msg)\n\n    return options_instance\n</code></pre>"},{"location":"api/options/#pyretailscience.options.Options.reset_option","title":"<code>reset_option(pat)</code>","text":"<p>Reset the specified option to its default value.</p> <p>Parameters:</p> Name Type Description Default <code>pat</code> <code>str</code> <p>The option name.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the option name is unknown.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def reset_option(self, pat: str) -&gt; None:\n    \"\"\"Reset the specified option to its default value.\n\n    Args:\n        pat: The option name.\n\n    Raises:\n        ValueError: If the option name is unknown.\n    \"\"\"\n    if pat not in self._options:\n        msg = f\"Unknown option: {pat}\"\n        raise ValueError(msg)\n\n    self._options[pat] = self._default_options[pat]\n</code></pre>"},{"location":"api/options/#pyretailscience.options.Options.set_option","title":"<code>set_option(pat, val)</code>","text":"<p>Set the value of the specified option.</p> <p>Parameters:</p> Name Type Description Default <code>pat</code> <code>str</code> <p>The option name.</p> required <code>val</code> <code>OptionTypes</code> <p>The value to set the option to.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the option name is unknown.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def set_option(self, pat: str, val: OptionTypes) -&gt; None:\n    \"\"\"Set the value of the specified option.\n\n    Args:\n        pat: The option name.\n        val: The value to set the option to.\n\n    Raises:\n        ValueError: If the option name is unknown.\n    \"\"\"\n    if pat not in self._options:\n        msg = f\"Unknown option: {pat}\"\n        raise ValueError(msg)\n\n    self._options[pat] = val\n</code></pre>"},{"location":"api/options/#pyretailscience.options.describe_option","title":"<code>describe_option(pat)</code>","text":"<p>Describe the specified option.</p> <p>This is a global function that delegates to the _global_options instance.</p> <p>Parameters:</p> Name Type Description Default <code>pat</code> <code>str</code> <p>The option name.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string describing the option and its current value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the option name is unknown.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def describe_option(pat: str) -&gt; str:\n    \"\"\"Describe the specified option.\n\n    This is a global function that delegates to the _global_options instance.\n\n    Args:\n        pat: The option name.\n\n    Returns:\n        A string describing the option and its current value.\n\n    Raises:\n        ValueError: If the option name is unknown.\n    \"\"\"\n    return _global_options.describe_option(pat)\n</code></pre>"},{"location":"api/options/#pyretailscience.options.find_project_root","title":"<code>find_project_root()</code>  <code>cached</code>","text":"<p>Returns the directory containing .git, .hg, or pyproject.toml, starting from the current working directory.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>@lru_cache\ndef find_project_root() -&gt; str | None:\n    \"\"\"Returns the directory containing .git, .hg, or pyproject.toml, starting from the current working directory.\"\"\"\n    current_dir = Path.cwd()\n\n    while True:\n        if (Path(current_dir / \".git\")).is_dir() or (Path(current_dir / \"pyretailscience.toml\")).is_file():\n            return current_dir\n\n        parent_dir = Path(current_dir).parent\n        reached_root = parent_dir == current_dir\n        if reached_root:\n            return None\n\n        current_dir = parent_dir\n</code></pre>"},{"location":"api/options/#pyretailscience.options.get_option","title":"<code>get_option(pat)</code>","text":"<p>Get the value of the specified option.</p> <p>This is a global function that delegates to the _global_options instance.</p> <p>Parameters:</p> Name Type Description Default <code>pat</code> <code>str</code> <p>The option name.</p> required <p>Returns:</p> Type Description <code>OptionTypes</code> <p>The value of the option.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the option name is unknown.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def get_option(pat: str) -&gt; OptionTypes:\n    \"\"\"Get the value of the specified option.\n\n    This is a global function that delegates to the _global_options instance.\n\n    Args:\n        pat: The option name.\n\n    Returns:\n        The value of the option.\n\n    Raises:\n        ValueError: If the option name is unknown.\n    \"\"\"\n    return _global_options.get_option(pat)\n</code></pre>"},{"location":"api/options/#pyretailscience.options.list_options","title":"<code>list_options()</code>","text":"<p>List all available options.</p> <p>This is a global function that delegates to the _global_options instance.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of all option names.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def list_options() -&gt; list[str]:\n    \"\"\"List all available options.\n\n    This is a global function that delegates to the _global_options instance.\n\n    Returns:\n        A list of all option names.\n    \"\"\"\n    return _global_options.list_options()\n</code></pre>"},{"location":"api/options/#pyretailscience.options.option_context","title":"<code>option_context(*args)</code>","text":"<p>Context manager to temporarily set options.</p> <p>Temporarily set options and restore them to their previous values after the context exits. The arguments should be supplied as alternating option names and values.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>OptionTypes</code> <p>An even number of arguments, alternating between option names (str)    and their corresponding values.</p> <code>()</code> <p>Yields:</p> Type Description <code>Generator[None, None, None]</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an odd number of arguments is supplied.</p> Example <p>with option_context('display.max_rows', 10, 'display.max_columns', 5): ...     # Do something with modified options ...     pass</p> Source code in <code>pyretailscience/options.py</code> <pre><code>@contextmanager\ndef option_context(*args: OptionTypes) -&gt; Generator[None, None, None]:\n    \"\"\"Context manager to temporarily set options.\n\n    Temporarily set options and restore them to their previous values after the\n    context exits. The arguments should be supplied as alternating option names\n    and values.\n\n    Args:\n        *args: An even number of arguments, alternating between option names (str)\n               and their corresponding values.\n\n    Yields:\n        None\n\n    Raises:\n        ValueError: If an odd number of arguments is supplied.\n\n    Example:\n        &gt;&gt;&gt; with option_context('display.max_rows', 10, 'display.max_columns', 5):\n        ...     # Do something with modified options\n        ...     pass\n        &gt;&gt;&gt; # Options are restored to their previous values here\n    \"\"\"\n    if len(args) % 2 != 0:\n        raise ValueError(\"The context manager requires an even number of arguments\")\n\n    old_options: dict[str, OptionTypes] = {}\n    try:\n        for pat, val in zip(args[::2], args[1::2], strict=True):\n            old_options[pat] = get_option(pat)\n            set_option(pat, val)\n        yield\n    finally:\n        for pat, val in old_options.items():\n            set_option(pat, val)\n</code></pre>"},{"location":"api/options/#pyretailscience.options.option_context--options-are-restored-to-their-previous-values-here","title":"Options are restored to their previous values here","text":""},{"location":"api/options/#pyretailscience.options.reset_option","title":"<code>reset_option(pat)</code>","text":"<p>Reset the specified option to its default value.</p> <p>This is a global function that delegates to the _global_options instance.</p> <p>Parameters:</p> Name Type Description Default <code>pat</code> <code>str</code> <p>The option name.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the option name is unknown.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def reset_option(pat: str) -&gt; None:\n    \"\"\"Reset the specified option to its default value.\n\n    This is a global function that delegates to the _global_options instance.\n\n    Args:\n        pat: The option name.\n\n    Raises:\n        ValueError: If the option name is unknown.\n    \"\"\"\n    _global_options.reset_option(pat)\n</code></pre>"},{"location":"api/options/#pyretailscience.options.set_option","title":"<code>set_option(pat, val)</code>","text":"<p>Set the value of the specified option.</p> <p>This is a global function that delegates to the _global_options instance.</p> <p>Parameters:</p> Name Type Description Default <code>pat</code> <code>str</code> <p>The option name.</p> required <code>val</code> <code>OptionTypes</code> <p>The value to set the option to.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the option name is unknown.</p> Source code in <code>pyretailscience/options.py</code> <pre><code>def set_option(pat: str, val: OptionTypes) -&gt; None:\n    \"\"\"Set the value of the specified option.\n\n    This is a global function that delegates to the _global_options instance.\n\n    Args:\n        pat: The option name.\n        val: The value to set the option to.\n\n    Raises:\n        ValueError: If the option name is unknown.\n    \"\"\"\n    _global_options.set_option(pat, val)\n</code></pre>"},{"location":"api/analysis/cohort/","title":"Cohort Analysis","text":"<p>Cohort Analysis and User Segmentation.</p> <p>This module implements functionality for performing cohort analysis, a powerful technique used in customer analytics and retention strategies.</p> <p>Cohort analysis helps in understanding customer behavior over time by grouping users based on shared characteristics or experiences, such as sign-up date, first purchase, or marketing campaign interaction. This method provides valuable insights into user engagement, retention, and lifetime value, which businesses can leverage in various ways:</p> <ol> <li> <p>Customer retention analysis: By tracking how different cohorts behave over time, businesses can identify trends    in user engagement and develop strategies to improve customer loyalty.</p> </li> <li> <p>Marketing performance evaluation: Understanding how different user groups respond to marketing efforts helps in    optimizing campaigns for higher conversions and better ROI.</p> </li> <li> <p>Product lifecycle insights: Analyzing user activity across cohorts can reveal product adoption trends and inform    feature development or enhancements.</p> </li> <li> <p>Revenue forecasting: Cohort-based revenue tracking enables more accurate predictions of future earnings and    helps in financial planning.</p> </li> <li> <p>Personalization and segmentation: Businesses can tailor their offerings based on cohort behavior to enhance    customer experience and increase retention rates.</p> </li> </ol> <p>The module employs key metrics such as retention rate, churn rate, and customer lifetime value (CLV) to measure cohort performance and user engagement over time:</p> <ul> <li>Retention Rate: The percentage of users who continue to engage with a product or service over a given period.</li> <li>Churn Rate: The percentage of users who stop engaging with the product within a specific timeframe.</li> <li>Customer Lifetime Value (CLV): The predicted total revenue a customer will generate throughout their relationship   with the business.</li> </ul> <p>By leveraging cohort analysis, businesses can make data-driven decisions to enhance customer experience, improve marketing strategies, and drive long-term growth.</p>"},{"location":"api/analysis/cohort/#pyretailscience.analysis.cohort.CohortAnalysis","title":"<code>CohortAnalysis</code>","text":"<p>Class for performing cohort analysis and visualization.</p> Source code in <code>pyretailscience/analysis/cohort.py</code> <pre><code>class CohortAnalysis:\n    \"\"\"Class for performing cohort analysis and visualization.\"\"\"\n\n    VALID_PERIODS: ClassVar[set[str]] = {\"year\", \"quarter\", \"month\", \"week\", \"day\"}\n\n    def __init__(\n        self,\n        df: pd.DataFrame | ibis.Table,\n        aggregation_column: str,\n        agg_func: str = \"nunique\",\n        period: str = \"month\",\n        percentage: bool = False,\n    ) -&gt; None:\n        \"\"\"Initializes the Cohort Analysis object.\n\n        Args:\n            df (pd.DataFrame | ibis.Table): The dataset containing transaction data.\n            aggregation_column (str): The column to apply the aggregation function on (e.g., 'unit_spend').\n            agg_func (str, optional): Aggregation function (e.g., \"nunique\", \"sum\", \"mean\"). Defaults to \"nunique\".\n            period (str): Period for cohort analysis (must be \"year\", \"quarter\", \"month\", \"week\", or \"day\").\n            percentage (bool): If True, converts cohort values into retention percentages relative to the first period.\n\n        Raises:\n            ValueError: If `period` is not one of the allowed values.\n            ValueError: If `df` is missing required columns (`customer_id`, `transaction_date`, or `aggregation_column`).\n        \"\"\"\n        cols = ColumnHelper()\n\n        if period not in self.VALID_PERIODS:\n            error_message = f\"Invalid period '{period}'. Allowed values: {self.VALID_PERIODS}.\"\n            raise ValueError(error_message)\n\n        required_cols = [\n            cols.customer_id,\n            cols.transaction_date,\n            aggregation_column,\n        ]\n        missing_cols = [col for col in required_cols if col not in df.columns]\n\n        if missing_cols:\n            error_message = f\"Missing required columns: {missing_cols}\"\n            raise ValueError(error_message)\n\n        self.table = self._calculate_cohorts(\n            df=df,\n            agg_func=agg_func,\n            period=period,\n            aggregation_column=aggregation_column,\n            percentage=percentage,\n        )\n\n    def _fill_cohort_gaps(\n        self,\n        cohort_analysis_table: pd.DataFrame,\n        period: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fills gaps in the cohort analysis table for missing periods.\n\n        Args:\n            cohort_analysis_table (pd.DataFrame): The cohort analysis table to fill gaps in.\n            period (str): The period of analysis (year, quarter, month, week, or day).\n\n        Returns:\n            pd.DataFrame: Cohort table with missing periods filled.\n        \"\"\"\n        cohort_analysis_table.index = pd.to_datetime(cohort_analysis_table.index)\n\n        min_period = cohort_analysis_table.index.min()\n        max_period = cohort_analysis_table.index.max()\n\n        period_lookup = {\"year\": \"YS\", \"quarter\": \"QS\", \"month\": \"MS\", \"week\": \"W\", \"day\": \"D\"}\n        full_range = pd.date_range(start=min_period, end=max_period, freq=period_lookup[period])\n        cohort_analysis_table = cohort_analysis_table.reindex(full_range, fill_value=0)\n        if cohort_analysis_table.shape[1] &gt; 0:\n            max_period_since = cohort_analysis_table.columns.max()\n            all_periods = range(max_period_since + 1)\n            cohort_analysis_table = cohort_analysis_table.reindex(columns=all_periods, fill_value=0)\n        return cohort_analysis_table\n\n    def _calculate_cohorts(\n        self,\n        df: pd.DataFrame | ibis.Table,\n        aggregation_column: str,\n        agg_func: str = \"nunique\",\n        period: str = \"month\",\n        percentage: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Computes a cohort analysis table based on transaction data.\n\n        Args:\n            df (pd.DataFrame | ibis.Table): The dataset containing transaction data.\n            aggregation_column (str): The column to apply the aggregation function on (e.g., 'unit_spend').\n            agg_func (str, optional): Aggregation function (e.g., \"nunique\", \"sum\", \"mean\"). Defaults to \"nunique\".\n            period (str): Period for cohort analysis (must be \"year\", \"quarter\", \"month\", \"week\", or \"day\").\n            percentage (bool): If True, converts cohort values into retention percentages relative to the first period.\n\n        Returns:\n            pd.DataFrame: Cohort analysis table with user retention values.\n        \"\"\"\n        cols = ColumnHelper()\n\n        ibis_table = ibis.memtable(df) if isinstance(df, pd.DataFrame) else df\n\n        filtered_table = ibis_table.mutate(\n            period_shopped=ibis_table[cols.transaction_date].truncate(period),\n            period_value=ibis_table[aggregation_column],\n        )\n\n        customer_cohort = filtered_table.group_by(cols.customer_id).aggregate(\n            min_period_shopped=filtered_table.period_shopped.min(),\n        )\n\n        cohort_table = (\n            filtered_table.join(customer_cohort, [cols.customer_id])\n            .group_by(\"min_period_shopped\", \"period_shopped\")\n            .aggregate(period_value=getattr(filtered_table.period_value, agg_func)())\n        )\n\n        cohort_table = cohort_table.mutate(\n            period_since=cohort_table.period_shopped.delta(cohort_table.min_period_shopped, unit=period),\n        )\n\n        cohort_df = cohort_table.execute().drop_duplicates(subset=[\"min_period_shopped\", \"period_since\"])\n\n        cohort_analysis_table = cohort_df.pivot(\n            index=\"min_period_shopped\",\n            columns=\"period_since\",\n            values=\"period_value\",\n        )\n\n        if percentage:\n            cohort_analysis_table = cohort_analysis_table.div(cohort_analysis_table.iloc[0], axis=1).round(2)\n\n        cohort_analysis_table = cohort_analysis_table.fillna(0)\n        cohort_analysis_table = self._fill_cohort_gaps(cohort_analysis_table, period)\n        cohort_analysis_table.index.name = \"min_period_shopped\"\n\n        return cohort_analysis_table\n</code></pre>"},{"location":"api/analysis/cohort/#pyretailscience.analysis.cohort.CohortAnalysis.__init__","title":"<code>__init__(df, aggregation_column, agg_func='nunique', period='month', percentage=False)</code>","text":"<p>Initializes the Cohort Analysis object.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame | Table</code> <p>The dataset containing transaction data.</p> required <code>aggregation_column</code> <code>str</code> <p>The column to apply the aggregation function on (e.g., 'unit_spend').</p> required <code>agg_func</code> <code>str</code> <p>Aggregation function (e.g., \"nunique\", \"sum\", \"mean\"). Defaults to \"nunique\".</p> <code>'nunique'</code> <code>period</code> <code>str</code> <p>Period for cohort analysis (must be \"year\", \"quarter\", \"month\", \"week\", or \"day\").</p> <code>'month'</code> <code>percentage</code> <code>bool</code> <p>If True, converts cohort values into retention percentages relative to the first period.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>period</code> is not one of the allowed values.</p> <code>ValueError</code> <p>If <code>df</code> is missing required columns (<code>customer_id</code>, <code>transaction_date</code>, or <code>aggregation_column</code>).</p> Source code in <code>pyretailscience/analysis/cohort.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame | ibis.Table,\n    aggregation_column: str,\n    agg_func: str = \"nunique\",\n    period: str = \"month\",\n    percentage: bool = False,\n) -&gt; None:\n    \"\"\"Initializes the Cohort Analysis object.\n\n    Args:\n        df (pd.DataFrame | ibis.Table): The dataset containing transaction data.\n        aggregation_column (str): The column to apply the aggregation function on (e.g., 'unit_spend').\n        agg_func (str, optional): Aggregation function (e.g., \"nunique\", \"sum\", \"mean\"). Defaults to \"nunique\".\n        period (str): Period for cohort analysis (must be \"year\", \"quarter\", \"month\", \"week\", or \"day\").\n        percentage (bool): If True, converts cohort values into retention percentages relative to the first period.\n\n    Raises:\n        ValueError: If `period` is not one of the allowed values.\n        ValueError: If `df` is missing required columns (`customer_id`, `transaction_date`, or `aggregation_column`).\n    \"\"\"\n    cols = ColumnHelper()\n\n    if period not in self.VALID_PERIODS:\n        error_message = f\"Invalid period '{period}'. Allowed values: {self.VALID_PERIODS}.\"\n        raise ValueError(error_message)\n\n    required_cols = [\n        cols.customer_id,\n        cols.transaction_date,\n        aggregation_column,\n    ]\n    missing_cols = [col for col in required_cols if col not in df.columns]\n\n    if missing_cols:\n        error_message = f\"Missing required columns: {missing_cols}\"\n        raise ValueError(error_message)\n\n    self.table = self._calculate_cohorts(\n        df=df,\n        agg_func=agg_func,\n        period=period,\n        aggregation_column=aggregation_column,\n        percentage=percentage,\n    )\n</code></pre>"},{"location":"api/analysis/composite_rank/","title":"Composite Rank","text":"<p>Composite Rank Analysis Module.</p> <p>This module provides the <code>CompositeRank</code> class which creates a composite ranking of several columns by giving each column an individual rank and then combining those ranks together into a single composite rank.</p> <p>Key Features: - Creates individual ranks for multiple columns - Supports both ascending and descending sort orders for each column - Combines individual ranks using a specified aggregation function - Can handle tie values with configurable options - Utilizes Ibis for efficient query execution</p>"},{"location":"api/analysis/composite_rank/#pyretailscience.analysis.composite_rank.CompositeRank","title":"<code>CompositeRank</code>","text":"<p>Creates a composite rank from multiple columns.</p> <p>This class creates a composite rank of several columns by giving each column an individual rank, and then combining those ranks together into a single composite rank. Composite ranks are often used in product range reviews when there are several important factors to consider when deciding to list or delist a product.</p> Source code in <code>pyretailscience/analysis/composite_rank.py</code> <pre><code>class CompositeRank:\n    \"\"\"Creates a composite rank from multiple columns.\n\n    This class creates a composite rank of several columns by giving each column an\n    individual rank, and then combining those ranks together into a single composite rank.\n    Composite ranks are often used in product range reviews when there are several important\n    factors to consider when deciding to list or delist a product.\n    \"\"\"\n\n    _df: pd.DataFrame | None = None\n\n    def __init__(\n        self,\n        df: pd.DataFrame | ibis.Table,\n        rank_cols: list[tuple[str, str] | str],\n        agg_func: str,\n        ignore_ties: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the CompositeRank class.\n\n        Args:\n            df (pd.DataFrame | ibis.Table): An ibis table or pandas DataFrame containing the data.\n            rank_cols (List[Union[Tuple[str, str], str]]): A list of columns to create the composite rank on.\n                Can be specified as tuples of (column_name, sort_order) where sort_order is 'asc', 'ascending',\n                'desc', or 'descending'. If just a string is provided, ascending order is assumed.\n            agg_func (str): The aggregation function to use when combining ranks.\n                Supported values are \"mean\", \"sum\", \"min\", \"max\".\n            ignore_ties (bool, optional): Whether to ignore ties when calculating ranks. If True, will use\n                row_number (each row gets a unique rank). If False (default), will use rank (ties get the same rank).\n\n        Raises:\n            ValueError: If any of the specified columns are not in the DataFrame or if a sort order is invalid.\n            ValueError: If the aggregation function is not one of the supported values.\n        \"\"\"\n        if isinstance(df, pd.DataFrame):\n            df = ibis.memtable(df)\n\n        # Validate columns and sort orders\n        valid_sort_orders = [\"asc\", \"ascending\", \"desc\", \"descending\"]\n\n        rank_mutates = {}\n        for col_spec in rank_cols:\n            if isinstance(col_spec, str):\n                col_name = col_spec\n                sort_order = \"asc\"\n            else:\n                if len(col_spec) != 2:  # noqa: PLR2004 - Error message below explains the value\n                    msg = (\n                        f\"Column specification must be a string or a tuple of (column_name, sort_order). Got {col_spec}\"\n                    )\n                    raise ValueError(msg)\n                col_name, sort_order = col_spec\n\n            if col_name not in df.columns:\n                msg = f\"Column '{col_name}' not found in the DataFrame.\"\n                raise ValueError(msg)\n\n            if sort_order.lower() not in valid_sort_orders:\n                msg = f\"Sort order must be one of {valid_sort_orders}. Got '{sort_order}'\"\n                raise ValueError(msg)\n\n            order_by = ibis.asc(df[col_name]) if sort_order in [\"asc\", \"ascending\"] else ibis.desc(df[col_name])\n            window = ibis.window(order_by=order_by)\n\n            # Calculate rank based on ignore_ties parameter (using 1-based ranks)\n            # ibis.row_number() is 1-based, ibis.rank() is 0-based so we add 1\n            rank_col = ibis.row_number().over(window) if ignore_ties else ibis.rank().over(window) + 1\n\n            # Add the rank column to the result table\n            rank_mutates[f\"{col_name}_rank\"] = rank_col\n\n        df = df.mutate(**rank_mutates)\n\n        column_refs = [df[col] for col in rank_mutates]\n        agg_expr = {\n            \"mean\": sum(column_refs) / len(column_refs),\n            \"sum\": sum(column_refs),\n            \"min\": ibis.least(*column_refs),\n            \"max\": ibis.greatest(*column_refs),\n        }\n\n        if agg_func.lower() not in agg_expr:\n            msg = f\"Aggregation function must be one of {list(agg_expr.keys())}. Got '{agg_func}'\"\n            raise ValueError(msg)\n\n        self.table = df.mutate(composite_rank=agg_expr[agg_func])\n\n    @property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"Returns the pandas DataFrame representation of the table.\n\n        Returns:\n            pd.DataFrame: A pandas DataFrame containing the original data,\n                individual column ranks, and the composite rank.\n        \"\"\"\n        if self._df is None:\n            self._df = self.table.execute()\n        return self._df\n</code></pre>"},{"location":"api/analysis/composite_rank/#pyretailscience.analysis.composite_rank.CompositeRank.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Returns the pandas DataFrame representation of the table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A pandas DataFrame containing the original data, individual column ranks, and the composite rank.</p>"},{"location":"api/analysis/composite_rank/#pyretailscience.analysis.composite_rank.CompositeRank.__init__","title":"<code>__init__(df, rank_cols, agg_func, ignore_ties=False)</code>","text":"<p>Initialize the CompositeRank class.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame | Table</code> <p>An ibis table or pandas DataFrame containing the data.</p> required <code>rank_cols</code> <code>List[Union[Tuple[str, str], str]]</code> <p>A list of columns to create the composite rank on. Can be specified as tuples of (column_name, sort_order) where sort_order is 'asc', 'ascending', 'desc', or 'descending'. If just a string is provided, ascending order is assumed.</p> required <code>agg_func</code> <code>str</code> <p>The aggregation function to use when combining ranks. Supported values are \"mean\", \"sum\", \"min\", \"max\".</p> required <code>ignore_ties</code> <code>bool</code> <p>Whether to ignore ties when calculating ranks. If True, will use row_number (each row gets a unique rank). If False (default), will use rank (ties get the same rank).</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the specified columns are not in the DataFrame or if a sort order is invalid.</p> <code>ValueError</code> <p>If the aggregation function is not one of the supported values.</p> Source code in <code>pyretailscience/analysis/composite_rank.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame | ibis.Table,\n    rank_cols: list[tuple[str, str] | str],\n    agg_func: str,\n    ignore_ties: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the CompositeRank class.\n\n    Args:\n        df (pd.DataFrame | ibis.Table): An ibis table or pandas DataFrame containing the data.\n        rank_cols (List[Union[Tuple[str, str], str]]): A list of columns to create the composite rank on.\n            Can be specified as tuples of (column_name, sort_order) where sort_order is 'asc', 'ascending',\n            'desc', or 'descending'. If just a string is provided, ascending order is assumed.\n        agg_func (str): The aggregation function to use when combining ranks.\n            Supported values are \"mean\", \"sum\", \"min\", \"max\".\n        ignore_ties (bool, optional): Whether to ignore ties when calculating ranks. If True, will use\n            row_number (each row gets a unique rank). If False (default), will use rank (ties get the same rank).\n\n    Raises:\n        ValueError: If any of the specified columns are not in the DataFrame or if a sort order is invalid.\n        ValueError: If the aggregation function is not one of the supported values.\n    \"\"\"\n    if isinstance(df, pd.DataFrame):\n        df = ibis.memtable(df)\n\n    # Validate columns and sort orders\n    valid_sort_orders = [\"asc\", \"ascending\", \"desc\", \"descending\"]\n\n    rank_mutates = {}\n    for col_spec in rank_cols:\n        if isinstance(col_spec, str):\n            col_name = col_spec\n            sort_order = \"asc\"\n        else:\n            if len(col_spec) != 2:  # noqa: PLR2004 - Error message below explains the value\n                msg = (\n                    f\"Column specification must be a string or a tuple of (column_name, sort_order). Got {col_spec}\"\n                )\n                raise ValueError(msg)\n            col_name, sort_order = col_spec\n\n        if col_name not in df.columns:\n            msg = f\"Column '{col_name}' not found in the DataFrame.\"\n            raise ValueError(msg)\n\n        if sort_order.lower() not in valid_sort_orders:\n            msg = f\"Sort order must be one of {valid_sort_orders}. Got '{sort_order}'\"\n            raise ValueError(msg)\n\n        order_by = ibis.asc(df[col_name]) if sort_order in [\"asc\", \"ascending\"] else ibis.desc(df[col_name])\n        window = ibis.window(order_by=order_by)\n\n        # Calculate rank based on ignore_ties parameter (using 1-based ranks)\n        # ibis.row_number() is 1-based, ibis.rank() is 0-based so we add 1\n        rank_col = ibis.row_number().over(window) if ignore_ties else ibis.rank().over(window) + 1\n\n        # Add the rank column to the result table\n        rank_mutates[f\"{col_name}_rank\"] = rank_col\n\n    df = df.mutate(**rank_mutates)\n\n    column_refs = [df[col] for col in rank_mutates]\n    agg_expr = {\n        \"mean\": sum(column_refs) / len(column_refs),\n        \"sum\": sum(column_refs),\n        \"min\": ibis.least(*column_refs),\n        \"max\": ibis.greatest(*column_refs),\n    }\n\n    if agg_func.lower() not in agg_expr:\n        msg = f\"Aggregation function must be one of {list(agg_expr.keys())}. Got '{agg_func}'\"\n        raise ValueError(msg)\n\n    self.table = df.mutate(composite_rank=agg_expr[agg_func])\n</code></pre>"},{"location":"api/analysis/cross_shop/","title":"Cross Shop Analysis","text":"<p>This module contains the CrossShop class that is used to create a cross-shop diagram.</p>"},{"location":"api/analysis/cross_shop/#pyretailscience.analysis.cross_shop.CrossShop","title":"<code>CrossShop</code>","text":"<p>A class to create a cross-shop diagram.</p> Source code in <code>pyretailscience/analysis/cross_shop.py</code> <pre><code>class CrossShop:\n    \"\"\"A class to create a cross-shop diagram.\"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame | ibis.Table,\n        group_1_col: str,\n        group_1_val: str,\n        group_2_col: str,\n        group_2_val: str,\n        group_3_col: str | None = None,\n        group_3_val: str | None = None,\n        labels: list[str] | None = None,\n        value_col: str = get_option(\"column.unit_spend\"),\n        agg_func: str = \"sum\",\n    ) -&gt; None:\n        \"\"\"Creates a cross-shop diagram that is used to show the overlap of customers between different groups.\n\n        Args:\n            df (pd.DataFrame | ibis.Table):  The input DataFrame or ibis Table containing transactional data.\n            group_1_col (str): The column name for the first group.\n            group_1_val (str): The value of the first group to match.\n            group_2_col (str): The column name for the second group.\n            group_2_val (str): The value of the second group to match.\n            group_3_col (str, optional): The column name for the third group. Defaults to None.\n            group_3_val (str, optional): The value of the third group to match. Defaults to None.\n            labels (list[str], optional): The labels for the groups. Defaults to None.\n            value_col (str, optional): The column to aggregate. Defaults to the option column.unit_spend.\n            agg_func (str, optional): The aggregation function. Defaults to \"sum\".\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the dataframe does not contain the required columns or if the number of labels does not match\n                the number of group indexes given.\n        \"\"\"\n        required_cols = [get_option(\"column.customer_id\"), value_col]\n        missing_cols = set(required_cols) - set(df.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        self.group_count = 2 if group_3_col is None else 3\n\n        if (labels is not None) and (len(labels) != self.group_count):\n            raise ValueError(\"The number of labels must be equal to the number of group indexes given\")\n\n        self.labels = labels if labels is not None else [chr(65 + i) for i in range(self.group_count)]\n\n        self.cross_shop_df = self._calc_cross_shop(\n            df=df,\n            group_1_col=group_1_col,\n            group_1_val=group_1_val,\n            group_2_col=group_2_col,\n            group_2_val=group_2_val,\n            group_3_col=group_3_col,\n            group_3_val=group_3_val,\n            value_col=value_col,\n            agg_func=agg_func,\n        )\n        self.cross_shop_table_df = self._calc_cross_shop_table(\n            df=self.cross_shop_df,\n            value_col=value_col,\n        )\n\n    @staticmethod\n    def _calc_cross_shop(\n        df: pd.DataFrame | ibis.Table,\n        group_1_col: str,\n        group_1_val: str,\n        group_2_col: str,\n        group_2_val: str,\n        group_3_col: str | None = None,\n        group_3_val: str | None = None,\n        value_col: str = get_option(\"column.unit_spend\"),\n        agg_func: str = \"sum\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate the cross-shop dataframe that will be used to plot the diagram.\n\n        Args:\n            df (pd.DataFrame | ibis.Table):  The input DataFrame or ibis Table containing transactional data.\n            group_1_col (str): Column name for the first group.\n            group_1_val (str): Value to filter for the first group.\n            group_2_col (str): Column name for the second group.\n            group_2_val (str): Value to filter for the second group.\n            group_3_col (str, optional): Column name for the third group. Defaults to None.\n            group_3_val (str, optional): Value to filter for the third group. Defaults to None.\n            value_col (str, optional): The column to aggregate. Defaults to option column.unit_spend.\n            agg_func (str, optional): The aggregation function. Defaults to \"sum\".\n\n        Returns:\n            pd.DataFrame: The cross-shop dataframe.\n\n        Raises:\n            ValueError: If group_3_col or group_3_val is populated, then the other must be as well.\n        \"\"\"\n        cols = ColumnHelper()\n\n        if isinstance(df, pd.DataFrame):\n            df: ibis.Table = ibis.memtable(df)\n        if (group_3_col is None) != (group_3_val is None):\n            raise ValueError(\"If group_3_col or group_3_val is populated, then the other must be as well\")\n\n        # Using a temporary value column to avoid duplicate column errors during selection. This happens when `value_col` has the same name as `customer_id`, causing conflicts in `.select()`.\n        temp_value_col = \"temp_value_col\"\n        df = df.mutate(**{temp_value_col: df[value_col]})\n\n        group_1 = (df[group_1_col] == group_1_val).cast(\"int32\").name(\"group_1\")\n        group_2 = (df[group_2_col] == group_2_val).cast(\"int32\").name(\"group_2\")\n        group_3 = (df[group_3_col] == group_3_val).cast(\"int32\").name(\"group_3\") if group_3_col else None\n\n        group_cols = [\"group_1\", \"group_2\"]\n        select_cols = [df[cols.customer_id], group_1, group_2]\n        if group_3 is not None:\n            group_cols.append(\"group_3\")\n            select_cols.append(group_3)\n\n        cs_df = df.select([*select_cols, df[temp_value_col]]).order_by(cols.customer_id)\n        cs_df = (\n            cs_df.group_by(cols.customer_id)\n            .aggregate(\n                **{col: cs_df[col].max().name(col) for col in group_cols},\n                **{temp_value_col: getattr(cs_df[temp_value_col], agg_func)().name(temp_value_col)},\n            )\n            .order_by(cols.customer_id)\n        ).execute()\n\n        cs_df[\"groups\"] = cs_df[group_cols].apply(lambda x: tuple(x), axis=1)\n        column_order = [cols.customer_id, *group_cols, \"groups\", temp_value_col]\n        cs_df = cs_df[column_order]\n        cs_df.set_index(cols.customer_id, inplace=True)\n        return cs_df.rename(columns={temp_value_col: value_col})\n\n    @staticmethod\n    def _calc_cross_shop_table(\n        df: pd.DataFrame,\n        value_col: str = get_option(\"column.unit_spend\"),\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate the aggregated cross-shop table that will be used to plot the diagram.\n\n        Args:\n            df (pd.DataFrame): The cross-shop dataframe.\n            value_col (str, optional): The column to aggregate. Defaults to option column.unit_spend.\n\n        Returns:\n            pd.DataFrame: The cross-shop table.\n        \"\"\"\n        df = df.groupby([\"groups\"], dropna=False)[value_col].sum().reset_index().copy()\n        df[\"percent\"] = df[value_col] / df[value_col].sum()\n        return df\n\n    def plot(\n        self,\n        title: str | None = None,\n        source_text: str | None = None,\n        vary_size: bool = False,\n        figsize: tuple[int, int] | None = None,\n        ax: Axes | None = None,\n        subset_label_formatter: Callable | None = None,\n        **kwargs: dict[str, any],\n    ) -&gt; SubplotBase:\n        \"\"\"Plot the cross-shop diagram.\n\n        Args:\n            title (str, optional): The title of the plot. Defaults to None.\n            source_text (str, optional): The source text for the plot. Defaults to None.\n            vary_size (bool, optional): Whether to vary the size of the circles based on their values. Defaults to\n                False.\n            figsize (tuple[int, int], optional): The size of the plot. Defaults to None.\n            ax (Axes, optional): The axes to plot on. Defaults to None.\n            subset_label_formatter (callable, optional): Function to format the subset labels.\n            **kwargs (dict[str, any]): Additional keyword arguments to pass to the diagram.\n\n        Returns:\n            SubplotBase: The axes of the plot.\n        \"\"\"\n        return venn.plot(\n            df=self.cross_shop_table_df,\n            labels=self.labels,\n            title=title,\n            source_text=source_text,\n            vary_size=vary_size,\n            figsize=figsize,\n            ax=ax,\n            subset_label_formatter=subset_label_formatter if subset_label_formatter else lambda x: f\"{x:.1%}\",\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/analysis/cross_shop/#pyretailscience.analysis.cross_shop.CrossShop.__init__","title":"<code>__init__(df, group_1_col, group_1_val, group_2_col, group_2_val, group_3_col=None, group_3_val=None, labels=None, value_col=get_option('column.unit_spend'), agg_func='sum')</code>","text":"<p>Creates a cross-shop diagram that is used to show the overlap of customers between different groups.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame | Table</code> <p>The input DataFrame or ibis Table containing transactional data.</p> required <code>group_1_col</code> <code>str</code> <p>The column name for the first group.</p> required <code>group_1_val</code> <code>str</code> <p>The value of the first group to match.</p> required <code>group_2_col</code> <code>str</code> <p>The column name for the second group.</p> required <code>group_2_val</code> <code>str</code> <p>The value of the second group to match.</p> required <code>group_3_col</code> <code>str</code> <p>The column name for the third group. Defaults to None.</p> <code>None</code> <code>group_3_val</code> <code>str</code> <p>The value of the third group to match. Defaults to None.</p> <code>None</code> <code>labels</code> <code>list[str]</code> <p>The labels for the groups. Defaults to None.</p> <code>None</code> <code>value_col</code> <code>str</code> <p>The column to aggregate. Defaults to the option column.unit_spend.</p> <code>get_option('column.unit_spend')</code> <code>agg_func</code> <code>str</code> <p>The aggregation function. Defaults to \"sum\".</p> <code>'sum'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataframe does not contain the required columns or if the number of labels does not match the number of group indexes given.</p> Source code in <code>pyretailscience/analysis/cross_shop.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame | ibis.Table,\n    group_1_col: str,\n    group_1_val: str,\n    group_2_col: str,\n    group_2_val: str,\n    group_3_col: str | None = None,\n    group_3_val: str | None = None,\n    labels: list[str] | None = None,\n    value_col: str = get_option(\"column.unit_spend\"),\n    agg_func: str = \"sum\",\n) -&gt; None:\n    \"\"\"Creates a cross-shop diagram that is used to show the overlap of customers between different groups.\n\n    Args:\n        df (pd.DataFrame | ibis.Table):  The input DataFrame or ibis Table containing transactional data.\n        group_1_col (str): The column name for the first group.\n        group_1_val (str): The value of the first group to match.\n        group_2_col (str): The column name for the second group.\n        group_2_val (str): The value of the second group to match.\n        group_3_col (str, optional): The column name for the third group. Defaults to None.\n        group_3_val (str, optional): The value of the third group to match. Defaults to None.\n        labels (list[str], optional): The labels for the groups. Defaults to None.\n        value_col (str, optional): The column to aggregate. Defaults to the option column.unit_spend.\n        agg_func (str, optional): The aggregation function. Defaults to \"sum\".\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the dataframe does not contain the required columns or if the number of labels does not match\n            the number of group indexes given.\n    \"\"\"\n    required_cols = [get_option(\"column.customer_id\"), value_col]\n    missing_cols = set(required_cols) - set(df.columns)\n    if len(missing_cols) &gt; 0:\n        msg = f\"The following columns are required but missing: {missing_cols}\"\n        raise ValueError(msg)\n\n    self.group_count = 2 if group_3_col is None else 3\n\n    if (labels is not None) and (len(labels) != self.group_count):\n        raise ValueError(\"The number of labels must be equal to the number of group indexes given\")\n\n    self.labels = labels if labels is not None else [chr(65 + i) for i in range(self.group_count)]\n\n    self.cross_shop_df = self._calc_cross_shop(\n        df=df,\n        group_1_col=group_1_col,\n        group_1_val=group_1_val,\n        group_2_col=group_2_col,\n        group_2_val=group_2_val,\n        group_3_col=group_3_col,\n        group_3_val=group_3_val,\n        value_col=value_col,\n        agg_func=agg_func,\n    )\n    self.cross_shop_table_df = self._calc_cross_shop_table(\n        df=self.cross_shop_df,\n        value_col=value_col,\n    )\n</code></pre>"},{"location":"api/analysis/cross_shop/#pyretailscience.analysis.cross_shop.CrossShop.plot","title":"<code>plot(title=None, source_text=None, vary_size=False, figsize=None, ax=None, subset_label_formatter=None, **kwargs)</code>","text":"<p>Plot the cross-shop diagram.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text for the plot. Defaults to None.</p> <code>None</code> <code>vary_size</code> <code>bool</code> <p>Whether to vary the size of the circles based on their values. Defaults to False.</p> <code>False</code> <code>figsize</code> <code>tuple[int, int]</code> <p>The size of the plot. Defaults to None.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>The axes to plot on. Defaults to None.</p> <code>None</code> <code>subset_label_formatter</code> <code>callable</code> <p>Function to format the subset labels.</p> <code>None</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the diagram.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The axes of the plot.</p> Source code in <code>pyretailscience/analysis/cross_shop.py</code> <pre><code>def plot(\n    self,\n    title: str | None = None,\n    source_text: str | None = None,\n    vary_size: bool = False,\n    figsize: tuple[int, int] | None = None,\n    ax: Axes | None = None,\n    subset_label_formatter: Callable | None = None,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plot the cross-shop diagram.\n\n    Args:\n        title (str, optional): The title of the plot. Defaults to None.\n        source_text (str, optional): The source text for the plot. Defaults to None.\n        vary_size (bool, optional): Whether to vary the size of the circles based on their values. Defaults to\n            False.\n        figsize (tuple[int, int], optional): The size of the plot. Defaults to None.\n        ax (Axes, optional): The axes to plot on. Defaults to None.\n        subset_label_formatter (callable, optional): Function to format the subset labels.\n        **kwargs (dict[str, any]): Additional keyword arguments to pass to the diagram.\n\n    Returns:\n        SubplotBase: The axes of the plot.\n    \"\"\"\n    return venn.plot(\n        df=self.cross_shop_table_df,\n        labels=self.labels,\n        title=title,\n        source_text=source_text,\n        vary_size=vary_size,\n        figsize=figsize,\n        ax=ax,\n        subset_label_formatter=subset_label_formatter if subset_label_formatter else lambda x: f\"{x:.1%}\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/analysis/customer/","title":"Customer Analysis","text":"<p>Classes and function to assist with customer retention analysis.</p>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.DaysBetweenPurchases","title":"<code>DaysBetweenPurchases</code>","text":"<p>A class to plot the distribution of the average number of days between purchases per customer.</p> <p>Attributes:</p> Name Type Description <code>purchase_dist_s</code> <code>Series</code> <p>The average number of days between purchases per customer.</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>class DaysBetweenPurchases:\n    \"\"\"A class to plot the distribution of the average number of days between purchases per customer.\n\n    Attributes:\n        purchase_dist_s (pd.Series): The average number of days between purchases per customer.\n    \"\"\"\n\n    def __init__(self, df: pd.DataFrame) -&gt; None:\n        \"\"\"Initialize the DaysBetweenPurchases class.\n\n        Args:\n            df (pd.DataFrame): A dataframe with the transaction data. The dataframe must have the columns customer_id\n                and transaction_date, which must be non-null.\n\n        Raises:\n            ValueError: If the dataframe does doesn't contain the columns customer_id and transaction_id, or if the\n                columns are null.\n\n        \"\"\"\n        cols = ColumnHelper()\n        required_cols = [cols.customer_id, cols.transaction_date]\n        missing_cols = set(required_cols) - set(df.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        self.purchase_dist_s = self._calculate_days_between_purchases(df)\n\n    @staticmethod\n    def _calculate_days_between_purchases(df: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Calculate the average number of days between purchases per customer.\n\n        Args:\n            df (pd.DataFrame): A dataframe with the transaction data. The dataframe must have the columns customer_id\n                and transaction_date, which must be non-null.\n\n        Returns:\n            pd.Series: The average number of days between purchases per customer.\n        \"\"\"\n        cols = ColumnHelper()\n        required_cols = [cols.customer_id, cols.transaction_date]\n        missing_cols = set(required_cols) - set(df.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        purchase_dist_df = df[[cols.customer_id, cols.transaction_date]].copy()\n        purchase_dist_df[cols.transaction_date] = df[cols.transaction_date].dt.floor(\"D\")\n        purchase_dist_df = purchase_dist_df.drop_duplicates().sort_values([cols.customer_id, cols.transaction_date])\n        purchase_dist_df[\"diff\"] = purchase_dist_df[cols.transaction_date].diff()\n        new_cust_mask = purchase_dist_df[cols.customer_id] != purchase_dist_df[cols.customer_id].shift(1)\n        purchase_dist_df = purchase_dist_df[~new_cust_mask]\n        purchase_dist_df[\"diff\"] = purchase_dist_df[\"diff\"].dt.days\n        return purchase_dist_df.groupby(cols.customer_id)[\"diff\"].mean()\n\n    def plot(\n        self,\n        bins: int = 10,\n        cumulative: bool = False,\n        ax: Axes | None = None,\n        percentile_line: float | None = None,\n        title: str | None = None,\n        x_label: str | None = None,\n        y_label: str | None = None,\n        source_text: str | None = None,\n        **kwargs: dict[str, any],\n    ) -&gt; SubplotBase:\n        \"\"\"Plot the distribution of the average number of days between purchases per customer.\n\n        Args:\n            bins (int, optional): The number of bins to plot. Defaults to 10.\n            cumulative (bool, optional): Whether to plot the cumulative distribution. Defaults to False.\n            ax (Axes, optional): The Matplotlib axes to plot the graph on. Defaults to None.\n            percentile_line (float, optional): The percentile to draw a line at. Defaults to None. When None then no\n                line is drawn.\n            title (str, optional): The title of the plot. Defaults to None.\n            x_label (str, optional): The x-axis label. Defaults to None.\n            y_label (str, optional): The y-axis label. Defaults to None.\n            source_text (str, optional): The source text to add to the plot. Defaults to None.\n            kwargs (dict[str, any]): Additional keyword arguments to pass to the plot\n\n        Returns:\n            SubplotBase: The Matplotlib axes of the plot\n        \"\"\"\n        density = False\n        if cumulative:\n            density = True\n\n        ax = self.purchase_dist_s.hist(\n            bins=bins,\n            cumulative=cumulative,\n            ax=ax,\n            density=density,\n            color=COLORS[\"green\"][500],\n            **kwargs,\n        )\n\n        ax.xaxis.set_major_formatter(lambda x, pos: human_format(x, pos, decimals=0))\n\n        ax = standard_graph_styles(ax)\n\n        if cumulative:\n            default_title = \"Average Days Between Purchases cumulative Distribution\"\n            default_y_label = \"Percentage of Customers\"\n            ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n\n        else:\n            default_title = \"Average Days Between Purchases Distribution\"\n            default_y_label = \"Number of Customers\"\n            ax.yaxis.set_major_formatter(lambda x, pos: human_format(x, pos, decimals=0))\n\n        ax = gu.standard_graph_styles(\n            ax,\n            title=gu.not_none(title, default_title),\n            y_label=gu.not_none(y_label, default_y_label),\n            x_label=gu.not_none(x_label, \"Average Number of Days Between Purchases\"),\n        )\n\n        if percentile_line is not None:\n            if percentile_line &gt; 1 or percentile_line &lt; 0:\n                raise ValueError(\"Percentile line must be between 0 and 1\")\n            ax.axvline(\n                x=self.purchases_percentile(percentile_line),\n                color=COLORS[\"red\"][500],\n                linestyle=\"--\",\n                lw=2,\n                label=f\"{percentile_line:.1%} of customers\",\n                ymax=0.96,\n            )\n            ax.legend(frameon=False)\n\n        if source_text:\n            gu.add_source_text(ax=ax, source_text=source_text)\n\n        gu.standard_tick_styles(ax)\n\n        return ax\n\n    def purchases_percentile(self, percentile: float = 0.5) -&gt; float:\n        \"\"\"Get the average number of days between purchases at a given percentile.\n\n        Args:\n            percentile (float): The percentile to get the average number of days between purchases at.\n\n        Returns:\n            float: The average number of days between purchases at the given percentile.\n        \"\"\"\n        return self.purchase_dist_s.quantile(percentile)\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.DaysBetweenPurchases.__init__","title":"<code>__init__(df)</code>","text":"<p>Initialize the DaysBetweenPurchases class.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe with the transaction data. The dataframe must have the columns customer_id and transaction_date, which must be non-null.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataframe does doesn't contain the columns customer_id and transaction_id, or if the columns are null.</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>def __init__(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Initialize the DaysBetweenPurchases class.\n\n    Args:\n        df (pd.DataFrame): A dataframe with the transaction data. The dataframe must have the columns customer_id\n            and transaction_date, which must be non-null.\n\n    Raises:\n        ValueError: If the dataframe does doesn't contain the columns customer_id and transaction_id, or if the\n            columns are null.\n\n    \"\"\"\n    cols = ColumnHelper()\n    required_cols = [cols.customer_id, cols.transaction_date]\n    missing_cols = set(required_cols) - set(df.columns)\n    if len(missing_cols) &gt; 0:\n        msg = f\"The following columns are required but missing: {missing_cols}\"\n        raise ValueError(msg)\n\n    self.purchase_dist_s = self._calculate_days_between_purchases(df)\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.DaysBetweenPurchases.plot","title":"<code>plot(bins=10, cumulative=False, ax=None, percentile_line=None, title=None, x_label=None, y_label=None, source_text=None, **kwargs)</code>","text":"<p>Plot the distribution of the average number of days between purchases per customer.</p> <p>Parameters:</p> Name Type Description Default <code>bins</code> <code>int</code> <p>The number of bins to plot. Defaults to 10.</p> <code>10</code> <code>cumulative</code> <code>bool</code> <p>Whether to plot the cumulative distribution. Defaults to False.</p> <code>False</code> <code>ax</code> <code>Axes</code> <p>The Matplotlib axes to plot the graph on. Defaults to None.</p> <code>None</code> <code>percentile_line</code> <code>float</code> <p>The percentile to draw a line at. Defaults to None. When None then no line is drawn.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>x_label</code> <code>str</code> <p>The x-axis label. Defaults to None.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label. Defaults to None.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the plot</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The Matplotlib axes of the plot</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>def plot(\n    self,\n    bins: int = 10,\n    cumulative: bool = False,\n    ax: Axes | None = None,\n    percentile_line: float | None = None,\n    title: str | None = None,\n    x_label: str | None = None,\n    y_label: str | None = None,\n    source_text: str | None = None,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plot the distribution of the average number of days between purchases per customer.\n\n    Args:\n        bins (int, optional): The number of bins to plot. Defaults to 10.\n        cumulative (bool, optional): Whether to plot the cumulative distribution. Defaults to False.\n        ax (Axes, optional): The Matplotlib axes to plot the graph on. Defaults to None.\n        percentile_line (float, optional): The percentile to draw a line at. Defaults to None. When None then no\n            line is drawn.\n        title (str, optional): The title of the plot. Defaults to None.\n        x_label (str, optional): The x-axis label. Defaults to None.\n        y_label (str, optional): The y-axis label. Defaults to None.\n        source_text (str, optional): The source text to add to the plot. Defaults to None.\n        kwargs (dict[str, any]): Additional keyword arguments to pass to the plot\n\n    Returns:\n        SubplotBase: The Matplotlib axes of the plot\n    \"\"\"\n    density = False\n    if cumulative:\n        density = True\n\n    ax = self.purchase_dist_s.hist(\n        bins=bins,\n        cumulative=cumulative,\n        ax=ax,\n        density=density,\n        color=COLORS[\"green\"][500],\n        **kwargs,\n    )\n\n    ax.xaxis.set_major_formatter(lambda x, pos: human_format(x, pos, decimals=0))\n\n    ax = standard_graph_styles(ax)\n\n    if cumulative:\n        default_title = \"Average Days Between Purchases cumulative Distribution\"\n        default_y_label = \"Percentage of Customers\"\n        ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n\n    else:\n        default_title = \"Average Days Between Purchases Distribution\"\n        default_y_label = \"Number of Customers\"\n        ax.yaxis.set_major_formatter(lambda x, pos: human_format(x, pos, decimals=0))\n\n    ax = gu.standard_graph_styles(\n        ax,\n        title=gu.not_none(title, default_title),\n        y_label=gu.not_none(y_label, default_y_label),\n        x_label=gu.not_none(x_label, \"Average Number of Days Between Purchases\"),\n    )\n\n    if percentile_line is not None:\n        if percentile_line &gt; 1 or percentile_line &lt; 0:\n            raise ValueError(\"Percentile line must be between 0 and 1\")\n        ax.axvline(\n            x=self.purchases_percentile(percentile_line),\n            color=COLORS[\"red\"][500],\n            linestyle=\"--\",\n            lw=2,\n            label=f\"{percentile_line:.1%} of customers\",\n            ymax=0.96,\n        )\n        ax.legend(frameon=False)\n\n    if source_text:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    gu.standard_tick_styles(ax)\n\n    return ax\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.DaysBetweenPurchases.purchases_percentile","title":"<code>purchases_percentile(percentile=0.5)</code>","text":"<p>Get the average number of days between purchases at a given percentile.</p> <p>Parameters:</p> Name Type Description Default <code>percentile</code> <code>float</code> <p>The percentile to get the average number of days between purchases at.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The average number of days between purchases at the given percentile.</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>def purchases_percentile(self, percentile: float = 0.5) -&gt; float:\n    \"\"\"Get the average number of days between purchases at a given percentile.\n\n    Args:\n        percentile (float): The percentile to get the average number of days between purchases at.\n\n    Returns:\n        float: The average number of days between purchases at the given percentile.\n    \"\"\"\n    return self.purchase_dist_s.quantile(percentile)\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.PurchasesPerCustomer","title":"<code>PurchasesPerCustomer</code>","text":"<p>A class to plot the distribution of the number of purchases per customer.</p> <p>Attributes:</p> Name Type Description <code>cust_purchases_s</code> <code>Series</code> <p>The number of purchases per customer.</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>class PurchasesPerCustomer:\n    \"\"\"A class to plot the distribution of the number of purchases per customer.\n\n    Attributes:\n        cust_purchases_s (pd.Series): The number of purchases per customer.\n    \"\"\"\n\n    def __init__(self, df: pd.DataFrame) -&gt; None:\n        \"\"\"Initialize the PurchasesPerCustomer class.\n\n        Args:\n            df (pd.DataFrame): A dataframe with the transaction data. The dataframe must comply with the\n                contain customer_id and transaction_id columns, which must be non-null.\n\n        Raises:\n            ValueError: If the dataframe doesn't contain the columns customer_id and transaction_id, or if the columns\n                are null.\n\n        \"\"\"\n        cols = ColumnHelper()\n        required_cols = [cols.customer_id, cols.transaction_id]\n        missing_cols = set(required_cols) - set(df.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        self.cust_purchases_s = df.groupby(cols.customer_id)[cols.transaction_id].nunique()\n\n    def plot(\n        self,\n        bins: int = 10,\n        cumulative: bool = False,\n        ax: Axes | None = None,\n        percentile_line: float | None = None,\n        source_text: str | None = None,\n        title: str | None = None,\n        x_label: str | None = None,\n        y_label: str | None = None,\n        **kwargs: dict[str, any],\n    ) -&gt; SubplotBase:\n        \"\"\"Plot the distribution of the number of purchases per customer.\n\n        Args:\n            bins (int, optional): The number of bins to plot. Defaults to 10.\n            cumulative (bool, optional): Whether to plot the cumulative distribution. Defaults to False.\n            ax (Axes, optional): The Matplotlib axes to plot the graph on. Defaults to None.\n            percentile_line (float, optional): The percentile to draw a line at. Defaults to None. When None then no\n                line is drawn.\n            source_text (str, optional): The source text to add to the plot. Defaults to None.\n            title (str, optional): The title of the plot. Defaults to None.\n            x_label (str, optional): The x-axis label. Defaults to None.\n            y_label (str, optional): The y-axis label. Defaults to None.\n            kwargs (dict[str, any]): Additional keyword arguments to pass to the plot function.\n\n        Returns:\n            SubplotBase: The Matplotlib axes of the plot\n        \"\"\"\n        density = False\n        if cumulative:\n            density = True\n\n        if x_label is None:\n            x_label = \"Number of purchases\"\n\n        ax = self.cust_purchases_s.hist(\n            bins=bins,\n            cumulative=cumulative,\n            ax=ax,\n            density=density,\n            color=COLORS[\"green\"][500],\n            **kwargs,\n        )\n\n        ax.xaxis.set_major_formatter(lambda x, pos: human_format(x, pos, decimals=0))\n\n        if cumulative:\n            default_title = \"Number of Purchases cumulative Distribution\"\n            default_y_label = \"Percentage of customers\"\n            ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n\n        else:\n            default_title = \"Number of Purchases Distribution\"\n            default_y_label = \"Number of customers\"\n            ax.yaxis.set_major_formatter(lambda x, pos: human_format(x, pos, decimals=0))\n\n        ax = standard_graph_styles(\n            ax,\n            title=gu.not_none(title, default_title),\n            x_label=x_label,\n            y_label=gu.not_none(y_label, default_y_label),\n        )\n\n        if percentile_line is not None:\n            if percentile_line &gt; 1 or percentile_line &lt; 0:\n                raise ValueError(\"Percentile line must be between 0 and 1\")\n            ax.axvline(\n                x=self.purchases_percentile(percentile_line),\n                color=COLORS[\"red\"][500],\n                linestyle=\"--\",\n                lw=2,\n                label=f\"{percentile_line:.1%} of customers\",\n            )\n            ax.legend(frameon=False)\n\n        if source_text:\n            gu.add_source_text(ax=ax, source_text=source_text)\n\n        return ax\n\n    def purchases_percentile(self, percentile: float = 0.5) -&gt; float:\n        \"\"\"Get the number of purchases at a given percentile.\n\n        Args:\n            percentile (float): The percentile to get the number of purchases at.\n\n        Returns:\n            float: The number of purchases at the given percentile.\n        \"\"\"\n        return self.cust_purchases_s.quantile(percentile)\n\n    def find_purchase_percentile(self, number_of_purchases: int, comparison: str = \"less_than_equal_to\") -&gt; float:\n        \"\"\"Find the percentile of the number of purchases.\n\n        Args:\n            number_of_purchases (int): The number of purchases to find the percentile of.\n            comparison (str, optional): The comparison to use. Defaults to \"less_than_equal_to\". Must be one of\n                less_than, less_than_equal_to, equal_to, not_equal_to, greater_than, or greater_than_equal_to.\n\n        Returns:\n            float: The percentile of the number of purchases.\n        \"\"\"\n        ops = {\n            \"less_than\": operator.lt,\n            \"less_than_equal_to\": operator.le,\n            \"equal_to\": operator.eq,\n            \"not_equal_to\": operator.ne,\n            \"greater_than\": operator.gt,\n            \"greater_than_equal_to\": operator.ge,\n        }\n\n        if comparison not in ops:\n            raise ValueError(\n                \"Comparison must be one of 'less_than', 'less_than_equal_to', 'equal_to', 'not_equal_to',\",\n                \"'greater_than', 'greater_than_equal_to'\",\n            )\n\n        return len(self.cust_purchases_s[ops[comparison](self.cust_purchases_s, number_of_purchases)]) / len(\n            self.cust_purchases_s,\n        )\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.PurchasesPerCustomer.__init__","title":"<code>__init__(df)</code>","text":"<p>Initialize the PurchasesPerCustomer class.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe with the transaction data. The dataframe must comply with the contain customer_id and transaction_id columns, which must be non-null.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataframe doesn't contain the columns customer_id and transaction_id, or if the columns are null.</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>def __init__(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Initialize the PurchasesPerCustomer class.\n\n    Args:\n        df (pd.DataFrame): A dataframe with the transaction data. The dataframe must comply with the\n            contain customer_id and transaction_id columns, which must be non-null.\n\n    Raises:\n        ValueError: If the dataframe doesn't contain the columns customer_id and transaction_id, or if the columns\n            are null.\n\n    \"\"\"\n    cols = ColumnHelper()\n    required_cols = [cols.customer_id, cols.transaction_id]\n    missing_cols = set(required_cols) - set(df.columns)\n    if len(missing_cols) &gt; 0:\n        msg = f\"The following columns are required but missing: {missing_cols}\"\n        raise ValueError(msg)\n\n    self.cust_purchases_s = df.groupby(cols.customer_id)[cols.transaction_id].nunique()\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.PurchasesPerCustomer.find_purchase_percentile","title":"<code>find_purchase_percentile(number_of_purchases, comparison='less_than_equal_to')</code>","text":"<p>Find the percentile of the number of purchases.</p> <p>Parameters:</p> Name Type Description Default <code>number_of_purchases</code> <code>int</code> <p>The number of purchases to find the percentile of.</p> required <code>comparison</code> <code>str</code> <p>The comparison to use. Defaults to \"less_than_equal_to\". Must be one of less_than, less_than_equal_to, equal_to, not_equal_to, greater_than, or greater_than_equal_to.</p> <code>'less_than_equal_to'</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The percentile of the number of purchases.</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>def find_purchase_percentile(self, number_of_purchases: int, comparison: str = \"less_than_equal_to\") -&gt; float:\n    \"\"\"Find the percentile of the number of purchases.\n\n    Args:\n        number_of_purchases (int): The number of purchases to find the percentile of.\n        comparison (str, optional): The comparison to use. Defaults to \"less_than_equal_to\". Must be one of\n            less_than, less_than_equal_to, equal_to, not_equal_to, greater_than, or greater_than_equal_to.\n\n    Returns:\n        float: The percentile of the number of purchases.\n    \"\"\"\n    ops = {\n        \"less_than\": operator.lt,\n        \"less_than_equal_to\": operator.le,\n        \"equal_to\": operator.eq,\n        \"not_equal_to\": operator.ne,\n        \"greater_than\": operator.gt,\n        \"greater_than_equal_to\": operator.ge,\n    }\n\n    if comparison not in ops:\n        raise ValueError(\n            \"Comparison must be one of 'less_than', 'less_than_equal_to', 'equal_to', 'not_equal_to',\",\n            \"'greater_than', 'greater_than_equal_to'\",\n        )\n\n    return len(self.cust_purchases_s[ops[comparison](self.cust_purchases_s, number_of_purchases)]) / len(\n        self.cust_purchases_s,\n    )\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.PurchasesPerCustomer.plot","title":"<code>plot(bins=10, cumulative=False, ax=None, percentile_line=None, source_text=None, title=None, x_label=None, y_label=None, **kwargs)</code>","text":"<p>Plot the distribution of the number of purchases per customer.</p> <p>Parameters:</p> Name Type Description Default <code>bins</code> <code>int</code> <p>The number of bins to plot. Defaults to 10.</p> <code>10</code> <code>cumulative</code> <code>bool</code> <p>Whether to plot the cumulative distribution. Defaults to False.</p> <code>False</code> <code>ax</code> <code>Axes</code> <p>The Matplotlib axes to plot the graph on. Defaults to None.</p> <code>None</code> <code>percentile_line</code> <code>float</code> <p>The percentile to draw a line at. Defaults to None. When None then no line is drawn.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>x_label</code> <code>str</code> <p>The x-axis label. Defaults to None.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the plot function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The Matplotlib axes of the plot</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>def plot(\n    self,\n    bins: int = 10,\n    cumulative: bool = False,\n    ax: Axes | None = None,\n    percentile_line: float | None = None,\n    source_text: str | None = None,\n    title: str | None = None,\n    x_label: str | None = None,\n    y_label: str | None = None,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plot the distribution of the number of purchases per customer.\n\n    Args:\n        bins (int, optional): The number of bins to plot. Defaults to 10.\n        cumulative (bool, optional): Whether to plot the cumulative distribution. Defaults to False.\n        ax (Axes, optional): The Matplotlib axes to plot the graph on. Defaults to None.\n        percentile_line (float, optional): The percentile to draw a line at. Defaults to None. When None then no\n            line is drawn.\n        source_text (str, optional): The source text to add to the plot. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to None.\n        x_label (str, optional): The x-axis label. Defaults to None.\n        y_label (str, optional): The y-axis label. Defaults to None.\n        kwargs (dict[str, any]): Additional keyword arguments to pass to the plot function.\n\n    Returns:\n        SubplotBase: The Matplotlib axes of the plot\n    \"\"\"\n    density = False\n    if cumulative:\n        density = True\n\n    if x_label is None:\n        x_label = \"Number of purchases\"\n\n    ax = self.cust_purchases_s.hist(\n        bins=bins,\n        cumulative=cumulative,\n        ax=ax,\n        density=density,\n        color=COLORS[\"green\"][500],\n        **kwargs,\n    )\n\n    ax.xaxis.set_major_formatter(lambda x, pos: human_format(x, pos, decimals=0))\n\n    if cumulative:\n        default_title = \"Number of Purchases cumulative Distribution\"\n        default_y_label = \"Percentage of customers\"\n        ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n\n    else:\n        default_title = \"Number of Purchases Distribution\"\n        default_y_label = \"Number of customers\"\n        ax.yaxis.set_major_formatter(lambda x, pos: human_format(x, pos, decimals=0))\n\n    ax = standard_graph_styles(\n        ax,\n        title=gu.not_none(title, default_title),\n        x_label=x_label,\n        y_label=gu.not_none(y_label, default_y_label),\n    )\n\n    if percentile_line is not None:\n        if percentile_line &gt; 1 or percentile_line &lt; 0:\n            raise ValueError(\"Percentile line must be between 0 and 1\")\n        ax.axvline(\n            x=self.purchases_percentile(percentile_line),\n            color=COLORS[\"red\"][500],\n            linestyle=\"--\",\n            lw=2,\n            label=f\"{percentile_line:.1%} of customers\",\n        )\n        ax.legend(frameon=False)\n\n    if source_text:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    return ax\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.PurchasesPerCustomer.purchases_percentile","title":"<code>purchases_percentile(percentile=0.5)</code>","text":"<p>Get the number of purchases at a given percentile.</p> <p>Parameters:</p> Name Type Description Default <code>percentile</code> <code>float</code> <p>The percentile to get the number of purchases at.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The number of purchases at the given percentile.</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>def purchases_percentile(self, percentile: float = 0.5) -&gt; float:\n    \"\"\"Get the number of purchases at a given percentile.\n\n    Args:\n        percentile (float): The percentile to get the number of purchases at.\n\n    Returns:\n        float: The number of purchases at the given percentile.\n    \"\"\"\n    return self.cust_purchases_s.quantile(percentile)\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.TransactionChurn","title":"<code>TransactionChurn</code>","text":"<p>A class to plot the churn rate by number of purchases.</p> <p>Attributes:</p> Name Type Description <code>purchase_dist_df</code> <code>DataFrame</code> <p>The churn rate by number of purchases.</p> <code>n_unique_customers</code> <code>int</code> <p>The number of unique customers in the dataframe.</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>class TransactionChurn:\n    \"\"\"A class to plot the churn rate by number of purchases.\n\n    Attributes:\n        purchase_dist_df (pd.DataFrame): The churn rate by number of purchases.\n        n_unique_customers (int): The number of unique customers in the dataframe.\n    \"\"\"\n\n    def __init__(self, df: pd.DataFrame, churn_period: float) -&gt; None:\n        \"\"\"Initialize the TransactionChurn class.\n\n        Args:\n            df (pd.DataFrame): A dataframe with the transaction data. The dataframe must have the columns customer_id\n                and transaction_date.\n            churn_period (float): The number of days to consider a customer churned.\n\n        Raises:\n            ValueError: If the dataframe does doesn't contain the columns customer_id and transaction_id.\n        \"\"\"\n        cols = ColumnHelper()\n        required_cols = [cols.customer_id, cols.transaction_date]\n        missing_cols = set(required_cols) - set(df.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        purchase_dist_df = df[[cols.customer_id, cols.transaction_date]].copy()\n        # Truncate the transaction_date to the day\n        purchase_dist_df[cols.transaction_date] = df[cols.transaction_date].dt.floor(\"D\")\n        purchase_dist_df = purchase_dist_df.drop_duplicates()\n        purchase_dist_df = purchase_dist_df.sort_values([cols.customer_id, cols.transaction_date])\n        purchase_dist_df[\"transaction_number\"] = purchase_dist_df.groupby(cols.customer_id).cumcount() + 1\n\n        purchase_dist_df[\"last_transaction\"] = (\n            purchase_dist_df.groupby(cols.customer_id)[cols.transaction_date].shift(-1).isna()\n        )\n        purchase_dist_df[\"transaction_before_churn_window\"] = purchase_dist_df[cols.transaction_date] &lt; (\n            purchase_dist_df[cols.transaction_date].max() - pd.Timedelta(days=churn_period)\n        )\n        purchase_dist_df[\"churned\"] = (\n            purchase_dist_df[\"last_transaction\"] &amp; purchase_dist_df[\"transaction_before_churn_window\"]\n        )\n\n        purchase_dist_df = (\n            purchase_dist_df[purchase_dist_df[\"transaction_before_churn_window\"]]\n            .groupby([\"transaction_number\"])[\"churned\"]\n            .value_counts()\n            .unstack()\n        )\n        purchase_dist_df.columns = [\"retained\", \"churned\"]\n        purchase_dist_df[\"churned_pct\"] = purchase_dist_df[\"churned\"].div(purchase_dist_df.sum(axis=1))\n        self.purchase_dist_df = purchase_dist_df\n\n        self.n_unique_customers = df[cols.customer_id].nunique()\n\n    def plot(\n        self,\n        cumulative: bool = False,\n        ax: Axes | None = None,\n        title: str | None = None,\n        x_label: str | None = None,\n        y_label: str | None = None,\n        source_text: str | None = None,\n        **kwargs: dict[str, any],\n    ) -&gt; SubplotBase:\n        \"\"\"Plot the churn rate by number of purchases.\n\n        Args:\n            cumulative (bool, optional): Whether to plot the cumulative distribution. Defaults to False.\n            ax (Axes, optional): The Matplotlib axes to plot the graph on. Defaults to None.\n            title (str, optional): The title of the plot. Defaults to None.\n            x_label (str, optional): The x-axis label. Defaults to None.\n            y_label (str, optional): The y-axis label. Defaults to None.\n            source_text (str, optional): The source text to add to the plot. Defaults to None.\n            kwargs (dict[str, any]): Additional keyword arguments to pass to the plot function.\n\n        Returns:\n            SubplotBase: The Matplotlib axes of the plot\n        \"\"\"\n        if cumulative:\n            cumulative_churn_rate_s = self.purchase_dist_df[\"churned\"].cumsum().div(self.n_unique_customers)\n            ax = cumulative_churn_rate_s.plot.area(\n                color=COLORS[\"green\"][500],\n                **kwargs,\n            )\n            ax.set_xlim(self.purchase_dist_df.index.min(), self.purchase_dist_df.index.max())\n        else:\n            ax = self.purchase_dist_df[\"churned_pct\"].plot.bar(\n                rot=0,\n                color=COLORS[\"green\"][500],\n                width=0.8,\n                **kwargs,\n            )\n\n        standard_graph_styles(\n            ax,\n            title=gu.not_none(title, \"Churn Rate by Number of Purchases\"),\n            x_label=gu.not_none(x_label, \"Number of Purchases\"),\n            y_label=gu.not_none(y_label, \"% Churned\"),\n        )\n\n        ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n\n        if source_text:\n            gu.add_source_text(ax=ax, source_text=source_text)\n\n        return ax\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.TransactionChurn.__init__","title":"<code>__init__(df, churn_period)</code>","text":"<p>Initialize the TransactionChurn class.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe with the transaction data. The dataframe must have the columns customer_id and transaction_date.</p> required <code>churn_period</code> <code>float</code> <p>The number of days to consider a customer churned.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataframe does doesn't contain the columns customer_id and transaction_id.</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>def __init__(self, df: pd.DataFrame, churn_period: float) -&gt; None:\n    \"\"\"Initialize the TransactionChurn class.\n\n    Args:\n        df (pd.DataFrame): A dataframe with the transaction data. The dataframe must have the columns customer_id\n            and transaction_date.\n        churn_period (float): The number of days to consider a customer churned.\n\n    Raises:\n        ValueError: If the dataframe does doesn't contain the columns customer_id and transaction_id.\n    \"\"\"\n    cols = ColumnHelper()\n    required_cols = [cols.customer_id, cols.transaction_date]\n    missing_cols = set(required_cols) - set(df.columns)\n    if len(missing_cols) &gt; 0:\n        msg = f\"The following columns are required but missing: {missing_cols}\"\n        raise ValueError(msg)\n\n    purchase_dist_df = df[[cols.customer_id, cols.transaction_date]].copy()\n    # Truncate the transaction_date to the day\n    purchase_dist_df[cols.transaction_date] = df[cols.transaction_date].dt.floor(\"D\")\n    purchase_dist_df = purchase_dist_df.drop_duplicates()\n    purchase_dist_df = purchase_dist_df.sort_values([cols.customer_id, cols.transaction_date])\n    purchase_dist_df[\"transaction_number\"] = purchase_dist_df.groupby(cols.customer_id).cumcount() + 1\n\n    purchase_dist_df[\"last_transaction\"] = (\n        purchase_dist_df.groupby(cols.customer_id)[cols.transaction_date].shift(-1).isna()\n    )\n    purchase_dist_df[\"transaction_before_churn_window\"] = purchase_dist_df[cols.transaction_date] &lt; (\n        purchase_dist_df[cols.transaction_date].max() - pd.Timedelta(days=churn_period)\n    )\n    purchase_dist_df[\"churned\"] = (\n        purchase_dist_df[\"last_transaction\"] &amp; purchase_dist_df[\"transaction_before_churn_window\"]\n    )\n\n    purchase_dist_df = (\n        purchase_dist_df[purchase_dist_df[\"transaction_before_churn_window\"]]\n        .groupby([\"transaction_number\"])[\"churned\"]\n        .value_counts()\n        .unstack()\n    )\n    purchase_dist_df.columns = [\"retained\", \"churned\"]\n    purchase_dist_df[\"churned_pct\"] = purchase_dist_df[\"churned\"].div(purchase_dist_df.sum(axis=1))\n    self.purchase_dist_df = purchase_dist_df\n\n    self.n_unique_customers = df[cols.customer_id].nunique()\n</code></pre>"},{"location":"api/analysis/customer/#pyretailscience.analysis.customer.TransactionChurn.plot","title":"<code>plot(cumulative=False, ax=None, title=None, x_label=None, y_label=None, source_text=None, **kwargs)</code>","text":"<p>Plot the churn rate by number of purchases.</p> <p>Parameters:</p> Name Type Description Default <code>cumulative</code> <code>bool</code> <p>Whether to plot the cumulative distribution. Defaults to False.</p> <code>False</code> <code>ax</code> <code>Axes</code> <p>The Matplotlib axes to plot the graph on. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>x_label</code> <code>str</code> <p>The x-axis label. Defaults to None.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label. Defaults to None.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the plot function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The Matplotlib axes of the plot</p> Source code in <code>pyretailscience/analysis/customer.py</code> <pre><code>def plot(\n    self,\n    cumulative: bool = False,\n    ax: Axes | None = None,\n    title: str | None = None,\n    x_label: str | None = None,\n    y_label: str | None = None,\n    source_text: str | None = None,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plot the churn rate by number of purchases.\n\n    Args:\n        cumulative (bool, optional): Whether to plot the cumulative distribution. Defaults to False.\n        ax (Axes, optional): The Matplotlib axes to plot the graph on. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to None.\n        x_label (str, optional): The x-axis label. Defaults to None.\n        y_label (str, optional): The y-axis label. Defaults to None.\n        source_text (str, optional): The source text to add to the plot. Defaults to None.\n        kwargs (dict[str, any]): Additional keyword arguments to pass to the plot function.\n\n    Returns:\n        SubplotBase: The Matplotlib axes of the plot\n    \"\"\"\n    if cumulative:\n        cumulative_churn_rate_s = self.purchase_dist_df[\"churned\"].cumsum().div(self.n_unique_customers)\n        ax = cumulative_churn_rate_s.plot.area(\n            color=COLORS[\"green\"][500],\n            **kwargs,\n        )\n        ax.set_xlim(self.purchase_dist_df.index.min(), self.purchase_dist_df.index.max())\n    else:\n        ax = self.purchase_dist_df[\"churned_pct\"].plot.bar(\n            rot=0,\n            color=COLORS[\"green\"][500],\n            width=0.8,\n            **kwargs,\n        )\n\n    standard_graph_styles(\n        ax,\n        title=gu.not_none(title, \"Churn Rate by Number of Purchases\"),\n        x_label=gu.not_none(x_label, \"Number of Purchases\"),\n        y_label=gu.not_none(y_label, \"% Churned\"),\n    )\n\n    ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n\n    if source_text:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    return ax\n</code></pre>"},{"location":"api/analysis/customer_decision_hierarchy/","title":"Customer Decision Hierarchy","text":"<p>This module contains the RangePlanning class for performing customer decision hierarchy analysis.</p>"},{"location":"api/analysis/customer_decision_hierarchy/#pyretailscience.analysis.customer_decision_hierarchy.CustomerDecisionHierarchy","title":"<code>CustomerDecisionHierarchy</code>","text":"<p>A class to perform customer decision hierarchy analysis using the Customer Decision Hierarchy method.</p> Source code in <code>pyretailscience/analysis/customer_decision_hierarchy.py</code> <pre><code>class CustomerDecisionHierarchy:\n    \"\"\"A class to perform customer decision hierarchy analysis using the Customer Decision Hierarchy method.\"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        product_col: str,\n        exclude_same_transaction_products: bool = True,\n        method: Literal[\"truncated_svd\", \"yules_q\"] = \"truncated_svd\",\n        min_var_explained: float = 0.8,\n        random_state: int = 42,\n    ) -&gt; None:\n        \"\"\"Initializes the RangePlanning object.\n\n        Args:\n            df (pd.DataFrame): The input dataframe containing transaction data. The dataframe must have the columns\n                customer_id, transaction_id, product_name.\n            product_col (str): The name of the column containing the product or category names.\n            exclude_same_transaction_products (bool, optional): Flag indicating whether to exclude products found in\n                the same transaction from a customer's distinct list of products bought. The idea is that if a\n                customer bought two products in the same transaction they can't be substitutes for that customer.\n                Thus they should be excluded from the analysis. Defaults to True.\n            method (Literal[\"truncated_svd\", \"yules_q\"], optional): The method to use for calculating distances.\n                Defaults to \"truncated_svd\".\n            min_var_explained (float, optional): The minimum variance explained required for truncated SVD method.\n                Only applicable if method is \"truncated_svd\". Defaults to 0.8.\n            random_state (int, optional): Random seed for reproducibility. Defaults to 42.\n\n        Raises:\n            ValueError: If the dataframe does not have the require columns.\n\n        \"\"\"\n        cols = ColumnHelper()\n        required_cols = [cols.customer_id, cols.transaction_id, product_col]\n        missing_cols = set(required_cols) - set(df.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        self.random_state = random_state\n        self.product_col = product_col\n        self.pairs_df = self._get_pairs(df, exclude_same_transaction_products, product_col)\n        self.distances = self._calculate_distances(method=method, min_var_explained=min_var_explained)\n\n    @staticmethod\n    def _get_pairs(df: pd.DataFrame, exclude_same_transaction_products: bool, product_col: str) -&gt; pd.DataFrame:\n        cols = ColumnHelper()\n        if exclude_same_transaction_products:\n            pairs_df = df[[cols.customer_id, cols.transaction_id, product_col]].drop_duplicates()\n            pairs_to_exclude_df = (\n                pairs_df.groupby(cols.transaction_id)\n                .filter(lambda x: len(x) &gt; 1)[[cols.customer_id, product_col]]\n                .drop_duplicates()\n            )\n            # Drop all rows from pairs_df where customer_id and product_name are in pairs_to_exclude_df\n            pairs_df = pairs_df.merge(\n                pairs_to_exclude_df,\n                on=[cols.customer_id, product_col],\n                how=\"left\",\n                indicator=True,\n            )\n            pairs_df = pairs_df[pairs_df[\"_merge\"] == \"left_only\"][[cols.customer_id, product_col]].drop_duplicates()\n        else:\n            pairs_df = df[[cols.customer_id, product_col]].drop_duplicates()\n\n        return pairs_df.reset_index(drop=True).astype(\"category\")\n\n    def _get_truncated_svd_distances(self, min_var_explained: float = 0.8) -&gt; np.array:\n        \"\"\"Calculate the truncated SVD distances for the given pairs dataframe.\n\n        Args:\n            min_var_explained (float): The minimum variance explained required.\n\n        Returns:\n            np.array: The normalized matrix of truncated SVD distances.\n        \"\"\"\n        from scipy.sparse import csr_matrix\n        from sklearn.decomposition import TruncatedSVD\n\n        sparse_matrix = csr_matrix(\n            (\n                [1] * len(self.pairs_df),\n                (\n                    self.pairs_df[self.product_col].cat.codes,\n                    self.pairs_df[get_option(\"column.customer_id\")].cat.codes,\n                ),\n            ),\n        )\n\n        n_products = sparse_matrix.shape[0]\n        svd = TruncatedSVD(n_components=n_products, random_state=self.random_state)\n        svd.fit(sparse_matrix)\n        cuml_var = np.cumsum(svd.explained_variance_ratio_)\n\n        req_n_components = np.argmax(cuml_var &gt;= min_var_explained) + 1\n\n        reduced_matrix = TruncatedSVD(n_components=req_n_components, random_state=self.random_state).fit_transform(\n            sparse_matrix,\n        )\n        norm_matrix = reduced_matrix / np.linalg.norm(reduced_matrix, axis=1, keepdims=True)\n\n        return norm_matrix  # noqa: RET504\n\n    @staticmethod\n    def _calculate_yules_q(bought_product_1: np.array, bought_product_2: np.array) -&gt; float:\n        \"\"\"Calculates the Yule's Q coefficient between two binary arrays.\n\n        Args:\n            bought_product_1 (np.array): Binary array representing the first bought product. Each element is 1 if the\n                customer bought the product and 0 if they didn't.\n            bought_product_2 (np.array): Binary array representing the second bought product. Each element is 1 if the\n                customer bought the product and 0 if they didn't.\n\n        Returns:\n            float: The Yule's Q coefficient.\n\n        Raises:\n            ValueError: If the lengths of `bought_product_1` and `bought_product_2` are not the same.\n            ValueError: If `bought_product_1` or `bought_product_2` is not a boolean array.\n\n        \"\"\"\n        if len(bought_product_1) != len(bought_product_2):\n            raise ValueError(\"The bought_product_1 and bought_product_2 must be the same length\")\n        if len(bought_product_1) == 0:\n            return 0.0\n        if bought_product_1.dtype != bool or bought_product_2.dtype != bool:\n            raise ValueError(\"The bought_product_1 and bought_product_2 must be boolean arrays\")\n\n        a = np.count_nonzero(bought_product_1 &amp; bought_product_2)\n        b = np.count_nonzero(bought_product_1 &amp; ~bought_product_2)\n        c = np.count_nonzero(~bought_product_1 &amp; bought_product_2)\n        d = np.count_nonzero(~bought_product_1 &amp; ~bought_product_2)\n\n        # Calculate Yule's Q coefficient\n        q = (a * d - b * c) / (a * d + b * c)\n\n        return q  # noqa: RET504\n\n    def _get_yules_q_distances(self) -&gt; float:\n        \"\"\"Calculate the Yules Q distances between pairs of products.\n\n        Returns:\n            float: The Yules Q distances between pairs of products.\n        \"\"\"\n        from scipy.sparse import csr_matrix\n\n        # Create a sparse matrix where the rows are the customers and the columns are the products\n        # The values are True if the customer bought the product and False if they didn't\n        product_matrix = csr_matrix(\n            (\n                [True] * len(self.pairs_df),\n                (\n                    self.pairs_df[self.product_col].cat.codes,\n                    self.pairs_df[get_option(\"column.customer_id\")].cat.codes,\n                ),\n            ),\n            dtype=bool,\n        )\n\n        # Calculate the number of customers and products\n        n_products = product_matrix.shape[0]\n\n        # Create an empty matrix to store the yules q values\n        yules_q_matrix = np.zeros((n_products, n_products), dtype=float)\n\n        # Loop through each pair of products\n        for i in range(n_products):\n            arr_i = product_matrix[i].toarray()\n            for j in range(i + 1, n_products):\n                # Calculate the yules q value for the pair of products\n                arr_j = product_matrix[j].toarray()\n                yules_q_dist = 1 - self._calculate_yules_q(arr_i, arr_j)\n\n                # Store the yules q value in the matrix\n                yules_q_matrix[i, j] = yules_q_dist\n                yules_q_matrix[j, i] = yules_q_dist\n\n        # Normalize the yules q values to be between 0 and 1 and return\n        return (yules_q_matrix + 1) / 2\n\n    def _calculate_distances(\n        self,\n        method: Literal[\"truncated_svd\", \"yules_q\"],\n        min_var_explained: float,\n    ) -&gt; None:\n        \"\"\"Calculates distances between items using the specified method.\n\n        Args:\n            method (Literal[\"truncated_svd\", \"yules_q\"], optional): The method to use for calculating distances.\n            min_var_explained (float, optional): The minimum variance explained required for truncated SVD method.\n                Only applicable if method is \"truncated_svd\".\n\n        Raises:\n            ValueError: If the method is not valid.\n\n        Returns:\n            None\n        \"\"\"\n        # Check method is valid\n        if method == \"truncated_svd\":\n            distances = self._get_truncated_svd_distances(min_var_explained=min_var_explained)\n        elif method == \"yules_q\":\n            distances = self._get_yules_q_distances()\n        else:\n            raise ValueError(\"Method must be 'truncated_svd' or 'yules_q'\")\n\n        return distances\n\n    def plot(\n        self,\n        title: str = \"Customer Decision Hierarchy\",\n        x_label: str | None = None,\n        y_label: str | None = None,\n        ax: Axes | None = None,\n        figsize: tuple[int, int] | None = None,\n        source_text: str | None = None,\n        **kwargs: dict[str, any],\n    ) -&gt; SubplotBase:\n        \"\"\"Plots the customer decision hierarchy dendrogram.\n\n        Args:\n            title (str, optional): The title of the plot. Defaults to None.\n            x_label (str, optional): The label for the x-axis. Defaults to None.\n            y_label (str, optional): The label for the y-axis. Defaults to None.\n            ax (Axes, optional): The matplotlib Axes object to plot on. Defaults to None.\n            figsize (tuple[int, int], optional): The figure size. Defaults to None.\n            source_text (str, optional): The source text to annotate on the plot. Defaults to None.\n            **kwargs (dict[str, any]): Additional keyword arguments to pass to the dendrogram function.\n\n        Returns:\n            SubplotBase: The matplotlib SubplotBase object.\n        \"\"\"\n        linkage_matrix = linkage(self.distances, method=\"ward\")\n        labels = self.pairs_df[\"product_name\"].cat.categories\n\n        if ax is None:\n            _, ax = plt.subplots(figsize=figsize)\n\n        orientation = kwargs.get(\"orientation\", \"top\")\n        default_x_label, default_y_label = (\n            (\"Products\", \"Distance\") if orientation in [\"top\", \"bottom\"] else (\"Distance\", \"Products\")\n        )\n\n        gu.standard_graph_styles(\n            ax=ax,\n            title=title,\n            x_label=gu.not_none(x_label, default_x_label),\n            y_label=gu.not_none(y_label, default_y_label),\n        )\n\n        # Set the y label to be on the right side of the plot\n        if orientation == \"left\":\n            ax.yaxis.tick_right()\n            ax.yaxis.set_label_position(\"right\")\n        elif orientation == \"bottom\":\n            ax.xaxis.tick_top()\n            ax.xaxis.set_label_position(\"top\")\n\n        dendrogram(linkage_matrix, labels=labels, ax=ax, **kwargs)\n\n        ax.xaxis.set_tick_params(labelsize=GraphStyles.DEFAULT_TICK_LABEL_FONT_SIZE)\n        ax.yaxis.set_tick_params(labelsize=GraphStyles.DEFAULT_TICK_LABEL_FONT_SIZE)\n\n        # Rotate the x-axis labels if they are too long\n        if orientation in [\"top\", \"bottom\"]:\n            plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n\n        # Set the font properties for the tick labels\n        gu.standard_tick_styles(ax)\n\n        if source_text is not None:\n            gu.add_source_text(ax=ax, source_text=source_text)\n\n        return ax\n</code></pre>"},{"location":"api/analysis/customer_decision_hierarchy/#pyretailscience.analysis.customer_decision_hierarchy.CustomerDecisionHierarchy.__init__","title":"<code>__init__(df, product_col, exclude_same_transaction_products=True, method='truncated_svd', min_var_explained=0.8, random_state=42)</code>","text":"<p>Initializes the RangePlanning object.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataframe containing transaction data. The dataframe must have the columns customer_id, transaction_id, product_name.</p> required <code>product_col</code> <code>str</code> <p>The name of the column containing the product or category names.</p> required <code>exclude_same_transaction_products</code> <code>bool</code> <p>Flag indicating whether to exclude products found in the same transaction from a customer's distinct list of products bought. The idea is that if a customer bought two products in the same transaction they can't be substitutes for that customer. Thus they should be excluded from the analysis. Defaults to True.</p> <code>True</code> <code>method</code> <code>Literal['truncated_svd', 'yules_q']</code> <p>The method to use for calculating distances. Defaults to \"truncated_svd\".</p> <code>'truncated_svd'</code> <code>min_var_explained</code> <code>float</code> <p>The minimum variance explained required for truncated SVD method. Only applicable if method is \"truncated_svd\". Defaults to 0.8.</p> <code>0.8</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataframe does not have the require columns.</p> Source code in <code>pyretailscience/analysis/customer_decision_hierarchy.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame,\n    product_col: str,\n    exclude_same_transaction_products: bool = True,\n    method: Literal[\"truncated_svd\", \"yules_q\"] = \"truncated_svd\",\n    min_var_explained: float = 0.8,\n    random_state: int = 42,\n) -&gt; None:\n    \"\"\"Initializes the RangePlanning object.\n\n    Args:\n        df (pd.DataFrame): The input dataframe containing transaction data. The dataframe must have the columns\n            customer_id, transaction_id, product_name.\n        product_col (str): The name of the column containing the product or category names.\n        exclude_same_transaction_products (bool, optional): Flag indicating whether to exclude products found in\n            the same transaction from a customer's distinct list of products bought. The idea is that if a\n            customer bought two products in the same transaction they can't be substitutes for that customer.\n            Thus they should be excluded from the analysis. Defaults to True.\n        method (Literal[\"truncated_svd\", \"yules_q\"], optional): The method to use for calculating distances.\n            Defaults to \"truncated_svd\".\n        min_var_explained (float, optional): The minimum variance explained required for truncated SVD method.\n            Only applicable if method is \"truncated_svd\". Defaults to 0.8.\n        random_state (int, optional): Random seed for reproducibility. Defaults to 42.\n\n    Raises:\n        ValueError: If the dataframe does not have the require columns.\n\n    \"\"\"\n    cols = ColumnHelper()\n    required_cols = [cols.customer_id, cols.transaction_id, product_col]\n    missing_cols = set(required_cols) - set(df.columns)\n    if len(missing_cols) &gt; 0:\n        msg = f\"The following columns are required but missing: {missing_cols}\"\n        raise ValueError(msg)\n\n    self.random_state = random_state\n    self.product_col = product_col\n    self.pairs_df = self._get_pairs(df, exclude_same_transaction_products, product_col)\n    self.distances = self._calculate_distances(method=method, min_var_explained=min_var_explained)\n</code></pre>"},{"location":"api/analysis/customer_decision_hierarchy/#pyretailscience.analysis.customer_decision_hierarchy.CustomerDecisionHierarchy.plot","title":"<code>plot(title='Customer Decision Hierarchy', x_label=None, y_label=None, ax=None, figsize=None, source_text=None, **kwargs)</code>","text":"<p>Plots the customer decision hierarchy dendrogram.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The title of the plot. Defaults to None.</p> <code>'Customer Decision Hierarchy'</code> <code>x_label</code> <code>str</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>The matplotlib Axes object to plot on. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>tuple[int, int]</code> <p>The figure size. Defaults to None.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to annotate on the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the dendrogram function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The matplotlib SubplotBase object.</p> Source code in <code>pyretailscience/analysis/customer_decision_hierarchy.py</code> <pre><code>def plot(\n    self,\n    title: str = \"Customer Decision Hierarchy\",\n    x_label: str | None = None,\n    y_label: str | None = None,\n    ax: Axes | None = None,\n    figsize: tuple[int, int] | None = None,\n    source_text: str | None = None,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plots the customer decision hierarchy dendrogram.\n\n    Args:\n        title (str, optional): The title of the plot. Defaults to None.\n        x_label (str, optional): The label for the x-axis. Defaults to None.\n        y_label (str, optional): The label for the y-axis. Defaults to None.\n        ax (Axes, optional): The matplotlib Axes object to plot on. Defaults to None.\n        figsize (tuple[int, int], optional): The figure size. Defaults to None.\n        source_text (str, optional): The source text to annotate on the plot. Defaults to None.\n        **kwargs (dict[str, any]): Additional keyword arguments to pass to the dendrogram function.\n\n    Returns:\n        SubplotBase: The matplotlib SubplotBase object.\n    \"\"\"\n    linkage_matrix = linkage(self.distances, method=\"ward\")\n    labels = self.pairs_df[\"product_name\"].cat.categories\n\n    if ax is None:\n        _, ax = plt.subplots(figsize=figsize)\n\n    orientation = kwargs.get(\"orientation\", \"top\")\n    default_x_label, default_y_label = (\n        (\"Products\", \"Distance\") if orientation in [\"top\", \"bottom\"] else (\"Distance\", \"Products\")\n    )\n\n    gu.standard_graph_styles(\n        ax=ax,\n        title=title,\n        x_label=gu.not_none(x_label, default_x_label),\n        y_label=gu.not_none(y_label, default_y_label),\n    )\n\n    # Set the y label to be on the right side of the plot\n    if orientation == \"left\":\n        ax.yaxis.tick_right()\n        ax.yaxis.set_label_position(\"right\")\n    elif orientation == \"bottom\":\n        ax.xaxis.tick_top()\n        ax.xaxis.set_label_position(\"top\")\n\n    dendrogram(linkage_matrix, labels=labels, ax=ax, **kwargs)\n\n    ax.xaxis.set_tick_params(labelsize=GraphStyles.DEFAULT_TICK_LABEL_FONT_SIZE)\n    ax.yaxis.set_tick_params(labelsize=GraphStyles.DEFAULT_TICK_LABEL_FONT_SIZE)\n\n    # Rotate the x-axis labels if they are too long\n    if orientation in [\"top\", \"bottom\"]:\n        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n\n    # Set the font properties for the tick labels\n    gu.standard_tick_styles(ax)\n\n    if source_text is not None:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    return ax\n</code></pre>"},{"location":"api/analysis/gain_loss/","title":"Gain Loss Analysis","text":"<p>This module performs gain loss analysis (switching analysis) on a DataFrame to assess customer movement between brands or products over time.</p> <p>Gain loss analysis, also known as switching analysis, is a marketing analytics technique used to assess customer movement between brands or products over time. It helps businesses understand the dynamics of customer acquisition and churn. Here's a concise definition: Gain loss analysis examines the flow of customers to and from a brand or product, quantifying:</p> <ol> <li>Gains: New customers acquired from competitors</li> <li>Losses: Existing customers lost to competitors</li> <li>Net change: The overall impact on market share</li> </ol> <p>This analysis helps marketers:</p> <ul> <li>Identify trends in customer behavior</li> <li>Evaluate the effectiveness of marketing strategies</li> <li>Understand competitive dynamics in the market</li> </ul>"},{"location":"api/analysis/gain_loss/#pyretailscience.analysis.gain_loss.GainLoss","title":"<code>GainLoss</code>","text":"<p>A class to perform gain loss analysis on a DataFrame to assess customer movement between brands or products over time.</p> Source code in <code>pyretailscience/analysis/gain_loss.py</code> <pre><code>class GainLoss:\n    \"\"\"A class to perform gain loss analysis on a DataFrame to assess customer movement between brands or products over time.\"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        p1_index: list[bool] | pd.Series,\n        p2_index: list[bool] | pd.Series,\n        focus_group_index: list[bool] | pd.Series,\n        focus_group_name: str,\n        comparison_group_index: list[bool] | pd.Series,\n        comparison_group_name: str,\n        group_col: str | None = None,\n        value_col: str = get_option(\"column.unit_spend\"),\n        agg_func: str = \"sum\",\n    ) -&gt; None:\n        \"\"\"Calculate the gain loss table for a given DataFrame at the customer level.\n\n        Args:\n            df (pd.DataFrame): The DataFrame to calculate the gain loss table from.\n            p1_index (list[bool]): The index for the first time period.\n            p2_index (list[bool]): The index for the second time period.\n            focus_group_index (list[bool]): The index for the focus group.\n            focus_group_name (str): The name of the focus group.\n            comparison_group_index (list[bool]): The index for the comparison group.\n            comparison_group_name (str): The name of the comparison group.\n            group_col (str | None, optional): The column to group by. Defaults to None.\n            value_col (str, optional): The column to calculate the gain loss from. Defaults to option column.unit_spend.\n            agg_func (str, optional): The aggregation function to use. Defaults to \"sum\".\n        \"\"\"\n        # # Ensure no overlap between p1 and p2\n        if not df[p1_index].index.intersection(df[p2_index].index).empty:\n            raise ValueError(\"p1_index and p2_index should not overlap\")\n\n        if not df[focus_group_index].index.intersection(df[comparison_group_index].index).empty:\n            raise ValueError(\"focus_group_index and comparison_group_index should not overlap\")\n\n        if not len(p1_index) == len(p2_index) == len(focus_group_index) == len(comparison_group_index):\n            raise ValueError(\n                \"p1_index, p2_index, focus_group_index, and comparison_group_index should have the same length\",\n            )\n\n        required_cols = [get_option(\"column.customer_id\"), value_col] + ([group_col] if group_col is not None else [])\n        missing_cols = set(required_cols) - set(df.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        self.focus_group_name = focus_group_name\n        self.comparison_group_name = comparison_group_name\n        self.group_col = group_col\n        self.value_col = value_col\n\n        self.gain_loss_df = self._calc_gain_loss(\n            df=df,\n            p1_index=p1_index,\n            p2_index=p2_index,\n            focus_group_index=focus_group_index,\n            comparison_group_index=comparison_group_index,\n            group_col=group_col,\n            value_col=value_col,\n            agg_func=agg_func,\n        )\n        self.gain_loss_table_df = self._calc_gains_loss_table(\n            gain_loss_df=self.gain_loss_df,\n            group_col=group_col,\n        )\n\n    @staticmethod\n    def process_customer_group(\n        focus_p1: float,\n        comparison_p1: float,\n        focus_p2: float,\n        comparison_p2: float,\n        focus_diff: float,\n        comparison_diff: float,\n    ) -&gt; tuple[float, float, float, float, float, float]:\n        \"\"\"Process the gain loss for a customer group.\n\n        Args:\n            focus_p1 (float | int): The focus group total in the first time period.\n            comparison_p1 (float | int): The comparison group total in the first time period.\n            focus_p2 (float | int): The focus group total in the second time period.\n            comparison_p2 (float | int): The comparison group total in the second time period.\n            focus_diff (float | int): The difference in the focus group totals.\n            comparison_diff (float | int): The difference in the comparison group totals.\n\n        Returns:\n            tuple[float, float, float, float, float, float]: The gain loss for the customer group.\n        \"\"\"\n        if focus_p1 == 0 and comparison_p1 == 0:\n            return focus_p2, 0, 0, 0, 0, 0\n        if focus_p2 == 0 and comparison_p2 == 0:\n            return 0, -1 * focus_p1, 0, 0, 0, 0\n\n        if focus_diff &gt; 0:\n            focus_inc_dec = focus_diff if comparison_diff &gt; 0 else max(0, comparison_diff + focus_diff)\n        elif comparison_diff &lt; 0:\n            focus_inc_dec = focus_diff\n        else:\n            focus_inc_dec = min(0, comparison_diff + focus_diff)\n\n        increased_focus = max(0, focus_inc_dec)\n        decreased_focus = min(0, focus_inc_dec)\n\n        transfer = focus_diff - focus_inc_dec\n        switch_from_comparison = max(0, transfer)\n        switch_to_comparison = min(0, transfer)\n\n        return 0, 0, increased_focus, decreased_focus, switch_from_comparison, switch_to_comparison\n\n    @staticmethod\n    def _calc_gain_loss(\n        df: pd.DataFrame,\n        p1_index: list[bool],\n        p2_index: list[bool],\n        focus_group_index: list[bool],\n        comparison_group_index: list,\n        group_col: str | None = None,\n        value_col: str = get_option(\"column.unit_spend\"),\n        agg_func: str = \"sum\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate the gain loss table for a given DataFrame at the customer level.\n\n        Args:\n            df (pd.DataFrame): The DataFrame to calculate the gain loss table from.\n            p1_index (list[bool]): The index for the first time period.\n            p2_index (list[bool]): The index for the second time period.\n            focus_group_index (list[bool]): The index for the focus group.\n            comparison_group_index (list[bool]): The index for the comparison group.\n            group_col (str | None, optional): The column to group by. Defaults to None.\n            value_col (str, optional): The column to calculate the gain loss from. Defaults to option column.unit_spend.\n            agg_func (str, optional): The aggregation function to use. Defaults to \"sum\".\n\n        Returns:\n            pd.DataFrame: The gain loss table.\n        \"\"\"\n        cols = ColumnHelper()\n        df = df[p1_index | p2_index].copy()\n        df[cols.customer_id] = df[cols.customer_id].astype(\"category\")\n\n        grp_cols = [cols.customer_id] if group_col is None else [group_col, cols.customer_id]\n\n        p1_df = pd.concat(\n            [\n                df[focus_group_index &amp; p1_index].groupby(grp_cols, observed=False)[value_col].agg(agg_func),\n                df[comparison_group_index &amp; p1_index].groupby(grp_cols, observed=False)[value_col].agg(agg_func),\n                df[(focus_group_index | comparison_group_index) &amp; p1_index]\n                .groupby(grp_cols, observed=False)[value_col]\n                .agg(agg_func),\n            ],\n            axis=1,\n        )\n        p1_df.columns = [\"focus\", \"comparison\", \"total\"]\n\n        p2_df = pd.concat(\n            [\n                df[focus_group_index &amp; p2_index].groupby(grp_cols, observed=False)[value_col].agg(agg_func),\n                df[comparison_group_index &amp; p2_index].groupby(grp_cols, observed=False)[value_col].agg(agg_func),\n                df[(focus_group_index | comparison_group_index) &amp; p2_index]\n                .groupby(grp_cols, observed=False)[value_col]\n                .agg(agg_func),\n            ],\n            axis=1,\n        )\n        p2_df.columns = [\"focus\", \"comparison\", \"total\"]\n\n        gl_df = p1_df.merge(p2_df, on=grp_cols, how=\"outer\", suffixes=(\"_p1\", \"_p2\")).fillna(0)\n\n        # Remove rows that are all 0 due to grouping by customer_id as a categorical with observed=False\n        gl_df = gl_df[~(gl_df == 0).all(axis=1)]\n\n        gl_df[\"focus_diff\"] = gl_df[\"focus_p2\"] - gl_df[\"focus_p1\"]\n        gl_df[\"comparison_diff\"] = gl_df[\"comparison_p2\"] - gl_df[\"comparison_p1\"]\n        gl_df[\"total_diff\"] = gl_df[\"total_p2\"] - gl_df[\"total_p1\"]\n\n        (\n            gl_df[\"new\"],\n            gl_df[\"lost\"],\n            gl_df[\"increased_focus\"],\n            gl_df[\"decreased_focus\"],\n            gl_df[\"switch_from_comparison\"],\n            gl_df[\"switch_to_comparison\"],\n        ) = zip(\n            *gl_df.apply(\n                lambda x: GainLoss.process_customer_group(\n                    focus_p1=x[\"focus_p1\"],\n                    comparison_p1=x[\"comparison_p1\"],\n                    focus_p2=x[\"focus_p2\"],\n                    comparison_p2=x[\"comparison_p2\"],\n                    focus_diff=x[\"focus_diff\"],\n                    comparison_diff=x[\"comparison_diff\"],\n                ),\n                axis=1,\n            ),\n            strict=False,\n        )\n\n        return gl_df\n\n    @staticmethod\n    def _calc_gains_loss_table(\n        gain_loss_df: pd.DataFrame,\n        group_col: str | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregates the gain loss table to show the total gains and losses across customers.\n\n        Args:\n            gain_loss_df (pd.DataFrame): The gain loss table at customer level to aggregate.\n            group_col (str | None, optional): The column to group by. Defaults to None.\n\n        Returns:\n            pd.DataFrame: The aggregated gain loss table\n        \"\"\"\n        if group_col is None:\n            return gain_loss_df.sum().to_frame(\"\").T\n\n        return gain_loss_df.groupby(level=0).sum()\n\n    def plot(\n        self,\n        title: str | None = None,\n        x_label: str | None = None,\n        y_label: str | None = None,\n        ax: Axes | None = None,\n        source_text: str | None = None,\n        move_legend_outside: bool = False,\n        **kwargs: dict[str, any],\n    ) -&gt; SubplotBase:\n        \"\"\"Plot the gain loss table using the bar.plot wrapper.\n\n        Args:\n            title (str | None, optional): The title of the plot. Defaults to None.\n            x_label (str | None, optional): The x-axis label. Defaults to None.\n            y_label (str | None, optional): The y-axis label. Defaults to None.\n            ax (Axes | None, optional): The axes to plot on. Defaults to None.\n            source_text (str | None, optional): The source text to add to the plot. Defaults to None.\n            move_legend_outside (bool, optional): Whether to move the legend outside the plot. Defaults to False.\n            kwargs (dict[str, any]): Additional keyword arguments to pass to the plot.\n\n        Returns:\n            SubplotBase: The plot\n        \"\"\"\n        green_colors = [COLORS[\"green\"][700], COLORS[\"green\"][500], COLORS[\"green\"][300]]\n        red_colors = [COLORS[\"red\"][700], COLORS[\"red\"][500], COLORS[\"red\"][300]]\n\n        increase_cols = [\"new\", \"increased_focus\", \"switch_from_comparison\"]\n        decrease_cols = [\"lost\", \"decreased_focus\", \"switch_to_comparison\"]\n        all_cols = increase_cols + decrease_cols\n\n        plot_df = self.gain_loss_table_df.copy()\n        default_y_label = self.focus_group_name if self.group_col is None else self.group_col\n        plot_data = plot_df.copy()\n\n        color_dict = {col: green_colors[i] for i, col in enumerate(increase_cols)}\n        color_dict.update({col: red_colors[i] for i, col in enumerate(decrease_cols)})\n\n        kwargs.pop(\"stacked\", None)\n\n        ax = bar.plot(\n            df=plot_data,\n            value_col=all_cols,\n            title=gu.not_none(title, f\"Gain Loss from {self.focus_group_name} to {self.comparison_group_name}\"),\n            y_label=gu.not_none(y_label, default_y_label),\n            x_label=gu.not_none(x_label, self.value_col),\n            orientation=\"horizontal\",\n            ax=ax,\n            source_text=source_text,\n            move_legend_outside=move_legend_outside,\n            stacked=True,\n            **kwargs,\n        )\n\n        for i, container in enumerate(ax.containers):\n            col_name = all_cols[i]\n            for patch in container:\n                patch.set_color(color_dict[col_name])\n\n        legend_labels = [\n            \"New\",\n            f\"Increased {self.focus_group_name}\",\n            f\"Switch From {self.comparison_group_name}\",\n            \"Lost\",\n            f\"Decreased {self.focus_group_name}\",\n            f\"Switch To {self.comparison_group_name}\",\n        ]\n\n        if ax.get_legend():\n            ax.get_legend().remove()\n\n        legend = ax.legend(\n            legend_labels,\n            frameon=True,\n            bbox_to_anchor=(1.05, 1) if move_legend_outside else None,\n            loc=\"upper left\" if move_legend_outside else \"best\",\n        )\n        legend.get_frame().set_facecolor(\"white\")\n        legend.get_frame().set_edgecolor(\"white\")\n\n        ax.axvline(0, color=\"black\", linewidth=0.5)\n\n        decimals = gu.get_decimals(ax.get_xlim(), ax.get_xticks())\n        ax.xaxis.set_major_formatter(lambda x, pos: gu.human_format(x, pos, decimals=decimals))\n        ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n\n        gu.standard_tick_styles(ax)\n\n        return ax\n</code></pre>"},{"location":"api/analysis/gain_loss/#pyretailscience.analysis.gain_loss.GainLoss.__init__","title":"<code>__init__(df, p1_index, p2_index, focus_group_index, focus_group_name, comparison_group_index, comparison_group_name, group_col=None, value_col=get_option('column.unit_spend'), agg_func='sum')</code>","text":"<p>Calculate the gain loss table for a given DataFrame at the customer level.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to calculate the gain loss table from.</p> required <code>p1_index</code> <code>list[bool]</code> <p>The index for the first time period.</p> required <code>p2_index</code> <code>list[bool]</code> <p>The index for the second time period.</p> required <code>focus_group_index</code> <code>list[bool]</code> <p>The index for the focus group.</p> required <code>focus_group_name</code> <code>str</code> <p>The name of the focus group.</p> required <code>comparison_group_index</code> <code>list[bool]</code> <p>The index for the comparison group.</p> required <code>comparison_group_name</code> <code>str</code> <p>The name of the comparison group.</p> required <code>group_col</code> <code>str | None</code> <p>The column to group by. Defaults to None.</p> <code>None</code> <code>value_col</code> <code>str</code> <p>The column to calculate the gain loss from. Defaults to option column.unit_spend.</p> <code>get_option('column.unit_spend')</code> <code>agg_func</code> <code>str</code> <p>The aggregation function to use. Defaults to \"sum\".</p> <code>'sum'</code> Source code in <code>pyretailscience/analysis/gain_loss.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame,\n    p1_index: list[bool] | pd.Series,\n    p2_index: list[bool] | pd.Series,\n    focus_group_index: list[bool] | pd.Series,\n    focus_group_name: str,\n    comparison_group_index: list[bool] | pd.Series,\n    comparison_group_name: str,\n    group_col: str | None = None,\n    value_col: str = get_option(\"column.unit_spend\"),\n    agg_func: str = \"sum\",\n) -&gt; None:\n    \"\"\"Calculate the gain loss table for a given DataFrame at the customer level.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to calculate the gain loss table from.\n        p1_index (list[bool]): The index for the first time period.\n        p2_index (list[bool]): The index for the second time period.\n        focus_group_index (list[bool]): The index for the focus group.\n        focus_group_name (str): The name of the focus group.\n        comparison_group_index (list[bool]): The index for the comparison group.\n        comparison_group_name (str): The name of the comparison group.\n        group_col (str | None, optional): The column to group by. Defaults to None.\n        value_col (str, optional): The column to calculate the gain loss from. Defaults to option column.unit_spend.\n        agg_func (str, optional): The aggregation function to use. Defaults to \"sum\".\n    \"\"\"\n    # # Ensure no overlap between p1 and p2\n    if not df[p1_index].index.intersection(df[p2_index].index).empty:\n        raise ValueError(\"p1_index and p2_index should not overlap\")\n\n    if not df[focus_group_index].index.intersection(df[comparison_group_index].index).empty:\n        raise ValueError(\"focus_group_index and comparison_group_index should not overlap\")\n\n    if not len(p1_index) == len(p2_index) == len(focus_group_index) == len(comparison_group_index):\n        raise ValueError(\n            \"p1_index, p2_index, focus_group_index, and comparison_group_index should have the same length\",\n        )\n\n    required_cols = [get_option(\"column.customer_id\"), value_col] + ([group_col] if group_col is not None else [])\n    missing_cols = set(required_cols) - set(df.columns)\n    if len(missing_cols) &gt; 0:\n        msg = f\"The following columns are required but missing: {missing_cols}\"\n        raise ValueError(msg)\n\n    self.focus_group_name = focus_group_name\n    self.comparison_group_name = comparison_group_name\n    self.group_col = group_col\n    self.value_col = value_col\n\n    self.gain_loss_df = self._calc_gain_loss(\n        df=df,\n        p1_index=p1_index,\n        p2_index=p2_index,\n        focus_group_index=focus_group_index,\n        comparison_group_index=comparison_group_index,\n        group_col=group_col,\n        value_col=value_col,\n        agg_func=agg_func,\n    )\n    self.gain_loss_table_df = self._calc_gains_loss_table(\n        gain_loss_df=self.gain_loss_df,\n        group_col=group_col,\n    )\n</code></pre>"},{"location":"api/analysis/gain_loss/#pyretailscience.analysis.gain_loss.GainLoss.plot","title":"<code>plot(title=None, x_label=None, y_label=None, ax=None, source_text=None, move_legend_outside=False, **kwargs)</code>","text":"<p>Plot the gain loss table using the bar.plot wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str | None</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>x_label</code> <code>str | None</code> <p>The x-axis label. Defaults to None.</p> <code>None</code> <code>y_label</code> <code>str | None</code> <p>The y-axis label. Defaults to None.</p> <code>None</code> <code>ax</code> <code>Axes | None</code> <p>The axes to plot on. Defaults to None.</p> <code>None</code> <code>source_text</code> <code>str | None</code> <p>The source text to add to the plot. Defaults to None.</p> <code>None</code> <code>move_legend_outside</code> <code>bool</code> <p>Whether to move the legend outside the plot. Defaults to False.</p> <code>False</code> <code>kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the plot.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The plot</p> Source code in <code>pyretailscience/analysis/gain_loss.py</code> <pre><code>def plot(\n    self,\n    title: str | None = None,\n    x_label: str | None = None,\n    y_label: str | None = None,\n    ax: Axes | None = None,\n    source_text: str | None = None,\n    move_legend_outside: bool = False,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plot the gain loss table using the bar.plot wrapper.\n\n    Args:\n        title (str | None, optional): The title of the plot. Defaults to None.\n        x_label (str | None, optional): The x-axis label. Defaults to None.\n        y_label (str | None, optional): The y-axis label. Defaults to None.\n        ax (Axes | None, optional): The axes to plot on. Defaults to None.\n        source_text (str | None, optional): The source text to add to the plot. Defaults to None.\n        move_legend_outside (bool, optional): Whether to move the legend outside the plot. Defaults to False.\n        kwargs (dict[str, any]): Additional keyword arguments to pass to the plot.\n\n    Returns:\n        SubplotBase: The plot\n    \"\"\"\n    green_colors = [COLORS[\"green\"][700], COLORS[\"green\"][500], COLORS[\"green\"][300]]\n    red_colors = [COLORS[\"red\"][700], COLORS[\"red\"][500], COLORS[\"red\"][300]]\n\n    increase_cols = [\"new\", \"increased_focus\", \"switch_from_comparison\"]\n    decrease_cols = [\"lost\", \"decreased_focus\", \"switch_to_comparison\"]\n    all_cols = increase_cols + decrease_cols\n\n    plot_df = self.gain_loss_table_df.copy()\n    default_y_label = self.focus_group_name if self.group_col is None else self.group_col\n    plot_data = plot_df.copy()\n\n    color_dict = {col: green_colors[i] for i, col in enumerate(increase_cols)}\n    color_dict.update({col: red_colors[i] for i, col in enumerate(decrease_cols)})\n\n    kwargs.pop(\"stacked\", None)\n\n    ax = bar.plot(\n        df=plot_data,\n        value_col=all_cols,\n        title=gu.not_none(title, f\"Gain Loss from {self.focus_group_name} to {self.comparison_group_name}\"),\n        y_label=gu.not_none(y_label, default_y_label),\n        x_label=gu.not_none(x_label, self.value_col),\n        orientation=\"horizontal\",\n        ax=ax,\n        source_text=source_text,\n        move_legend_outside=move_legend_outside,\n        stacked=True,\n        **kwargs,\n    )\n\n    for i, container in enumerate(ax.containers):\n        col_name = all_cols[i]\n        for patch in container:\n            patch.set_color(color_dict[col_name])\n\n    legend_labels = [\n        \"New\",\n        f\"Increased {self.focus_group_name}\",\n        f\"Switch From {self.comparison_group_name}\",\n        \"Lost\",\n        f\"Decreased {self.focus_group_name}\",\n        f\"Switch To {self.comparison_group_name}\",\n    ]\n\n    if ax.get_legend():\n        ax.get_legend().remove()\n\n    legend = ax.legend(\n        legend_labels,\n        frameon=True,\n        bbox_to_anchor=(1.05, 1) if move_legend_outside else None,\n        loc=\"upper left\" if move_legend_outside else \"best\",\n    )\n    legend.get_frame().set_facecolor(\"white\")\n    legend.get_frame().set_edgecolor(\"white\")\n\n    ax.axvline(0, color=\"black\", linewidth=0.5)\n\n    decimals = gu.get_decimals(ax.get_xlim(), ax.get_xticks())\n    ax.xaxis.set_major_formatter(lambda x, pos: gu.human_format(x, pos, decimals=decimals))\n    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n\n    gu.standard_tick_styles(ax)\n\n    return ax\n</code></pre>"},{"location":"api/analysis/gain_loss/#pyretailscience.analysis.gain_loss.GainLoss.process_customer_group","title":"<code>process_customer_group(focus_p1, comparison_p1, focus_p2, comparison_p2, focus_diff, comparison_diff)</code>  <code>staticmethod</code>","text":"<p>Process the gain loss for a customer group.</p> <p>Parameters:</p> Name Type Description Default <code>focus_p1</code> <code>float | int</code> <p>The focus group total in the first time period.</p> required <code>comparison_p1</code> <code>float | int</code> <p>The comparison group total in the first time period.</p> required <code>focus_p2</code> <code>float | int</code> <p>The focus group total in the second time period.</p> required <code>comparison_p2</code> <code>float | int</code> <p>The comparison group total in the second time period.</p> required <code>focus_diff</code> <code>float | int</code> <p>The difference in the focus group totals.</p> required <code>comparison_diff</code> <code>float | int</code> <p>The difference in the comparison group totals.</p> required <p>Returns:</p> Type Description <code>tuple[float, float, float, float, float, float]</code> <p>tuple[float, float, float, float, float, float]: The gain loss for the customer group.</p> Source code in <code>pyretailscience/analysis/gain_loss.py</code> <pre><code>@staticmethod\ndef process_customer_group(\n    focus_p1: float,\n    comparison_p1: float,\n    focus_p2: float,\n    comparison_p2: float,\n    focus_diff: float,\n    comparison_diff: float,\n) -&gt; tuple[float, float, float, float, float, float]:\n    \"\"\"Process the gain loss for a customer group.\n\n    Args:\n        focus_p1 (float | int): The focus group total in the first time period.\n        comparison_p1 (float | int): The comparison group total in the first time period.\n        focus_p2 (float | int): The focus group total in the second time period.\n        comparison_p2 (float | int): The comparison group total in the second time period.\n        focus_diff (float | int): The difference in the focus group totals.\n        comparison_diff (float | int): The difference in the comparison group totals.\n\n    Returns:\n        tuple[float, float, float, float, float, float]: The gain loss for the customer group.\n    \"\"\"\n    if focus_p1 == 0 and comparison_p1 == 0:\n        return focus_p2, 0, 0, 0, 0, 0\n    if focus_p2 == 0 and comparison_p2 == 0:\n        return 0, -1 * focus_p1, 0, 0, 0, 0\n\n    if focus_diff &gt; 0:\n        focus_inc_dec = focus_diff if comparison_diff &gt; 0 else max(0, comparison_diff + focus_diff)\n    elif comparison_diff &lt; 0:\n        focus_inc_dec = focus_diff\n    else:\n        focus_inc_dec = min(0, comparison_diff + focus_diff)\n\n    increased_focus = max(0, focus_inc_dec)\n    decreased_focus = min(0, focus_inc_dec)\n\n    transfer = focus_diff - focus_inc_dec\n    switch_from_comparison = max(0, transfer)\n    switch_to_comparison = min(0, transfer)\n\n    return 0, 0, increased_focus, decreased_focus, switch_from_comparison, switch_to_comparison\n</code></pre>"},{"location":"api/analysis/haversine/","title":"Haversine Distance","text":"<p>This module provides functionality for computing geospatial distances using Ibis expressions.</p> <p>It defines functions for efficient geospatial analysis on structured data tables, leveraging Ibis for optimized query execution.</p>"},{"location":"api/analysis/haversine/#pyretailscience.analysis.haversine--core-features","title":"Core Features","text":"<ul> <li>Ibis-Based Computation: Uses Ibis expressions for scalable processing.</li> <li>Haversine Distance Calculation: Computes great-circle distances dynamically as an Ibis expression.</li> <li>Backend Agnostic: Works with multiple Ibis-supported backends, including SQL-based databases.</li> <li>Efficient Query Optimization: Defers computation to the database or processing engine.</li> </ul>"},{"location":"api/analysis/haversine/#pyretailscience.analysis.haversine--use-cases","title":"Use Cases","text":"<ul> <li>Geospatial Filtering: Identify locations within a certain radius using database queries.</li> <li>Spatial Analysis: Analyze movement patterns and distances between geographic points.</li> <li>Logistics &amp; Routing: Optimize route planning by calculating distances dynamically.</li> </ul>"},{"location":"api/analysis/haversine/#pyretailscience.analysis.haversine--limitations-and-warnings","title":"Limitations and Warnings","text":"<ul> <li>Requires Ibis-Compatible Backend: Ensure your Ibis backend supports trigonometric functions.</li> <li>Assumes Spherical Earth: Uses the Haversine formula, which introduces slight inaccuracies due to Earth's oblate shape.</li> </ul>"},{"location":"api/analysis/haversine/#pyretailscience.analysis.haversine.haversine_distance","title":"<code>haversine_distance(lat_col, lon_col, target_lat_col, target_lon_col, radius=6371.0)</code>","text":"<p>Computes the Haversine distance between two sets of latitude and longitude columns.</p> <p>Parameters:</p> Name Type Description Default <code>lat_col</code> <code>Column</code> <p>Column containing source latitudes.</p> required <code>lon_col</code> <code>Column</code> <p>Column containing source longitudes.</p> required <code>target_lat_col</code> <code>Column</code> <p>Column containing target latitudes.</p> required <code>target_lon_col</code> <code>Column</code> <p>Column containing target longitudes.</p> required <code>radius</code> <code>float</code> <p>Earth's radius in kilometers (default: 6371 km).</p> <code>6371.0</code> <p>Returns:</p> Type Description <code>Column</code> <p>ibis.expr.types.Column: An Ibis expression representing the computed distances.</p> Source code in <code>pyretailscience/analysis/haversine.py</code> <pre><code>def haversine_distance(\n    lat_col: ibis.expr.types.Column,\n    lon_col: ibis.expr.types.Column,\n    target_lat_col: ibis.expr.types.Column,\n    target_lon_col: ibis.expr.types.Column,\n    radius: float = 6371.0,\n) -&gt; ibis.expr.types.Column:\n    \"\"\"Computes the Haversine distance between two sets of latitude and longitude columns.\n\n    Parameters:\n        lat_col (ibis.expr.types.Column): Column containing source latitudes.\n        lon_col (ibis.expr.types.Column): Column containing source longitudes.\n        target_lat_col (ibis.expr.types.Column): Column containing target latitudes.\n        target_lon_col (ibis.expr.types.Column): Column containing target longitudes.\n        radius (float, optional): Earth's radius in kilometers (default: 6371 km).\n\n    Returns:\n        ibis.expr.types.Column: An Ibis expression representing the computed distances.\n    \"\"\"\n    lat1_rad = lat_col.radians()\n    lat2_rad = target_lat_col.radians()\n    delta_lat = (target_lat_col - lat_col).radians()\n    delta_lon = (target_lon_col - lon_col).radians()\n\n    a = (delta_lat / 2).sin().pow(2) + lat1_rad.cos() * lat2_rad.cos() * (delta_lon / 2).sin().pow(2)\n    c = 2 * a.sqrt().asin()\n\n    return radius * c\n</code></pre>"},{"location":"api/analysis/product_association/","title":"Product Associations","text":"<p>Product Association Rules Generation.</p> <p>This module implements functionality for generating product association rules, a powerful technique in retail analytics and market basket analysis.</p> <p>Product association rules are used to uncover relationships between different products that customers tend to purchase together. These rules provide valuable insights into consumer behavior and purchasing patterns, which can be leveraged by retail businesses in various ways:</p> <ol> <li> <p>Cross-selling and upselling: By identifying products frequently bought together, retailers can make targeted product    recommendations to increase sales and average order value.</p> </li> <li> <p>Store layout optimization: Understanding product associations helps in strategic product placement within stores,    potentially increasing impulse purchases and overall sales.</p> </li> <li> <p>Inventory management: Knowing which products are often bought together aids in maintaining appropriate stock levels    and predicting demand.</p> </li> <li> <p>Marketing and promotions: Association rules can guide the creation ofeffective bundle offers and promotional    campaigns.</p> </li> <li> <p>Customer segmentation: Patterns in product associations can reveal distinct customer segments with specific    preferences.</p> </li> <li> <p>New product development: Insights from association rules can inform decisions about new product lines or features.</p> </li> </ol> <p>The module uses metrics such as support, confidence, and uplift to quantifythe strength and significance of product associations:</p> <ul> <li>Support: The frequency of items appearing together in transactions.</li> <li>Confidence: The likelihood of buying one product given the purchase of another.</li> <li>Uplift: The increase in purchase probability of one product when another is bought.</li> </ul> <p>By leveraging these association rules, retailers can make data-driven decisions to enhance customer experience, optimize operations, and drive business growth.</p>"},{"location":"api/analysis/product_association/#pyretailscience.analysis.product_association.ProductAssociation","title":"<code>ProductAssociation</code>","text":"<p>A class for generating and analyzing product association rules.</p> <p>This class calculates association rules between products based on transaction data, helping to identify patterns in customer purchasing behavior.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame containing transaction data.</p> required <code>value_col</code> <code>str</code> <p>The name of the column in the input DataFrame that contains the product identifiers.</p> required <code>group_col</code> <code>str</code> <p>The name of the column that identifies unique transactions or customers. Defaults to option column.column_id.</p> <code>get_option('column.customer_id')</code> <code>target_item</code> <code>str or None</code> <p>A specific product to focus the association analysis on. If None, associations for all products are calculated. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>A DataFrame containing the calculated association rules and their metrics.</p> Example <p>import pandas as pd transaction_df = pd.DataFrame({ ...     'customer_id': [1, 1, 2, 2, 3], ...     'product_id': ['A', 'B', 'B', 'C', 'A'] ... }) pa = ProductAssociation(df=transaction_df, value_col='product_id', group_col='customer_id') print(pa.df)  # View the calculated association rules</p> Note <p>The resulting DataFrame (pa.df) contains the following columns: - product_1, product_2: The pair of products for which the association is calculated. - occurrences_1, occurrences_2: The number of transactions containing each product. - cooccurrences: The number of transactions containing both products. - support: The proportion of transactions containing both products. - confidence: The probability of buying product_2 given that product_1 was bought. - uplift: The ratio of the observed support to the expected support if the products were independent.</p> Source code in <code>pyretailscience/analysis/product_association.py</code> <pre><code>class ProductAssociation:\n    \"\"\"A class for generating and analyzing product association rules.\n\n    This class calculates association rules between products based on transaction data,\n    helping to identify patterns in customer purchasing behavior.\n\n    Args:\n        df (pandas.DataFrame): The input DataFrame containing transaction data.\n        value_col (str): The name of the column in the input DataFrame that contains\n            the product identifiers.\n        group_col (str, optional): The name of the column that identifies unique\n            transactions or customers. Defaults to option column.column_id.\n        target_item (str or None, optional): A specific product to focus the\n            association analysis on. If None, associations for all products are\n            calculated. Defaults to None.\n\n    Attributes:\n        df (pandas.DataFrame): A DataFrame containing the calculated association\n            rules and their metrics.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; transaction_df = pd.DataFrame({\n        ...     'customer_id': [1, 1, 2, 2, 3],\n        ...     'product_id': ['A', 'B', 'B', 'C', 'A']\n        ... })\n        &gt;&gt;&gt; pa = ProductAssociation(df=transaction_df, value_col='product_id', group_col='customer_id')\n        &gt;&gt;&gt; print(pa.df)  # View the calculated association rules\n\n    Note:\n        The resulting DataFrame (pa.df) contains the following columns:\n        - product_1, product_2: The pair of products for which the association is calculated.\n        - occurrences_1, occurrences_2: The number of transactions containing each product.\n        - cooccurrences: The number of transactions containing both products.\n        - support: The proportion of transactions containing both products.\n        - confidence: The probability of buying product_2 given that product_1 was bought.\n        - uplift: The ratio of the observed support to the expected support if the products were independent.\n    \"\"\"\n\n    _df: pd.DataFrame | None = None\n\n    def __init__(\n        self,\n        df: pd.DataFrame | ibis.Table,\n        value_col: str,\n        group_col: str = get_option(\"column.customer_id\"),\n        target_item: str | None = None,\n        min_occurrences: int = 1,\n        min_cooccurrences: int = 1,\n        min_support: float = 0.0,\n        min_confidence: float = 0.0,\n        min_uplift: float = 0.0,\n    ) -&gt; None:\n        \"\"\"Initialize the ProductAssociation object.\n\n        Args:\n            df (pd.DataFrame | ibis.Table) : The input DataFrame or ibis Table containing transaction data.\n            value_col (str): The name of the column in the input DataFrame that contains the product identifiers.\n            group_col (str, optional): The name of the column that identifies unique transactions or customers. Defaults\n                to option column.unit_spend.\n            target_item (str or None, optional): A specific product to focus the association analysis on. If None,\n                associations for all products are calculated. Defaults to None.\n            min_occurrences (int, optional): The minimum number of occurrences required for each product in the\n                association analysis. Defaults to 1. Must be at least 1.\n            min_cooccurrences (int, optional): The minimum number of co-occurrences required for the product pairs in\n                the association analysis. Defaults to 1. Must be at least 1.\n            min_support (float, optional): The minimum support value required for the association rules. Defaults to\n                0.0. Must be between 0 and 1.\n            min_confidence (float, optional): The minimum confidence value required for the association rules. Defaults\n                to 0.0. Must be between 0 and 1.\n            min_uplift (float, optional): The minimum uplift value required for the association rules. Defaults to 0.0.\n                Must be greater or equal to 0.\n\n        Raises:\n            ValueError: If the number of combinations is not 2 or 3, or if any of the minimum values are invalid.\n            ValueError: If the minimum support, confidence, or uplift values are outside the valid range.\n            ValueError: If the minimum occurrences or cooccurrences are less than 1.\n            ValueError: If the input DataFrame does not contain the required columns or if they have null values.\n        \"\"\"\n        required_cols = [group_col, value_col]\n        missing_cols = set(required_cols) - set(df.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        self.table = self._calc_association(\n            df=df,\n            value_col=value_col,\n            group_col=group_col,\n            target_item=target_item,\n            min_occurrences=min_occurrences,\n            min_cooccurrences=min_cooccurrences,\n            min_support=min_support,\n            min_confidence=min_confidence,\n            min_uplift=min_uplift,\n        )\n\n    @staticmethod\n    def _calc_association(\n        df: pd.DataFrame | ibis.Table,\n        value_col: str,\n        group_col: str = get_option(\"column.customer_id\"),\n        target_item: str | None = None,\n        min_occurrences: int = 1,\n        min_cooccurrences: int = 1,\n        min_support: float = 0.0,\n        min_confidence: float = 0.0,\n        min_uplift: float = 0.0,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate product association rules based on transaction data.\n\n        This method calculates association rules between products based on transaction data,\n        helping to identify patterns in customer purchasing behavior.\n\n        Args:\n            df (pd.DataFrame | ibis.Table) : The input DataFrame or ibis Table containing transaction data.\n            value_col (str): The name of the column in the input DataFrame that contains the product identifiers.\n            group_col (str, optional): The name of the column that identifies unique transactions or customers. Defaults\n                to option column.unit_spend.\n            target_item (str or None, optional): A specific product to focus the association analysis on. If None,\n                associations for all products are calculated. Defaults to None.\n            min_occurrences (int, optional): The minimum number of occurrences required for each product in the\n                association analysis. Defaults to 1. Must be at least 1.\n            min_cooccurrences (int, optional): The minimum number of co-occurrences required for the product pairs in\n                the association analysis. Defaults to 1. Must be at least 1.\n            min_support (float, optional): The minimum support value required for the association rules. Defaults to\n                0.0. Must be between 0 and 1.\n            min_confidence (float, optional): The minimum confidence value required for the association rules. Defaults\n                to 0.0. Must be between 0 and 1.\n            min_uplift (float, optional): The minimum uplift value required for the association rules. Defaults to 0.0.\n                Must be greater or equal to 0.\n\n        Returns:\n            pandas.DataFrame: A DataFrame containing the calculated association rules and their metrics.\n\n        Raises:\n            ValueError: If the number of combinations is not 2 or 3, or if any of the minimum values are invalid.\n            ValueError: If the minimum support, confidence, or uplift values are outside the valid range.\n            ValueError: If the minimum occurrences or cooccurrences are less than 1.\n\n        Note:\n            The resulting DataFrame contains the following columns:\n            - product_1, product_2: The pair of products for which the association is calculated.\n            - occurrences_1, occurrences_2: The number of transactions containing each product.\n            - cooccurrences: The number of transactions containing both products.\n            - support: The proportion of transactions containing both products.\n            - confidence: The probability of buying product_2 given that product_1 was bought.\n            - uplift: The ratio of the observed support to the expected support if the products were independent.\n        \"\"\"\n        if min_occurrences &lt; 1:\n            raise ValueError(\"Minimum occurrences must be at least 1.\")\n        if min_cooccurrences &lt; 1:\n            raise ValueError(\"Minimum cooccurrences must be at least 1.\")\n        if min_support &lt; 0.0 or min_support &gt; 1.0:\n            raise ValueError(\"Minimum support must be between 0 and 1.\")\n        if min_confidence &lt; 0.0 or min_confidence &gt; 1.0:\n            raise ValueError(\"Minimum confidence must be between 0 and 1.\")\n        if min_uplift &lt; 0.0:\n            raise ValueError(\"Minimum uplift must be greater or equal to 0.\")\n\n        if isinstance(df, pd.DataFrame):\n            df = ibis.memtable(df)\n\n        unique_transactions = df.select(df[group_col], df[value_col]).distinct()\n        total_transactions = unique_transactions.alias(\"t\")[group_col].nunique().name(\"total_count\")\n\n        product_occurrences = (\n            unique_transactions.group_by(value_col)\n            .aggregate(\n                occurrences=lambda t: t[group_col].nunique(),\n            )\n            .mutate(occurrence_probability=lambda t: t.occurrences / total_transactions)\n            .filter(lambda t: t.occurrences &gt;= min_occurrences)\n        )\n\n        left_table = unique_transactions.rename({\"item_1\": value_col})\n        right_table = unique_transactions.rename({\"item_2\": value_col})\n\n        join_logic = [left_table[group_col] == right_table[group_col]]\n        if target_item is None:\n            join_logic.append(left_table[\"item_1\"] &lt; right_table[\"item_2\"])\n        else:\n            join_logic.extend(\n                [\n                    left_table[\"item_1\"] != right_table[\"item_2\"],\n                    left_table[\"item_1\"] == target_item,\n                ],\n            )\n        merged_df = left_table.join(\n            right_table,\n            predicates=join_logic,\n            lname=\"\",\n            rname=\"{name}_right\",\n        )\n\n        product_occurrences_1 = product_occurrences.rename(\n            {\"item_1\": value_col, \"occurrences_1\": \"occurrences\", \"occurrence_probability_1\": \"occurrence_probability\"},\n        )\n        product_occurrences_2 = product_occurrences.rename(\n            {\"item_2\": value_col, \"occurrences_2\": \"occurrences\", \"occurrence_probability_2\": \"occurrence_probability\"},\n        )\n\n        merged_df = merged_df.join(\n            product_occurrences_1,\n            predicates=[merged_df[\"item_1\"] == product_occurrences_1[\"item_1\"]],\n        )\n\n        merged_df = merged_df.join(\n            product_occurrences_2,\n            predicates=[merged_df[\"item_2\"] == product_occurrences_2[\"item_2\"]],\n        )\n\n        cooccurrences = merged_df.group_by([\"item_1\", \"item_2\"]).aggregate(cooccurrences=merged_df[group_col].nunique())\n        cooccurrences = cooccurrences.mutate(\n            support=cooccurrences.cooccurrences / total_transactions,\n        )\n        cooccurrences = cooccurrences.filter(\n            (cooccurrences.cooccurrences &gt;= min_cooccurrences) &amp; (cooccurrences.support &gt;= min_support),\n        )\n\n        product_occurrences_1_rename = product_occurrences.rename(\n            {\"item_1\": value_col, \"occurrences_1\": \"occurrences\", \"prob_1\": \"occurrence_probability\"},\n        )\n        product_occurrences_2_rename = product_occurrences.rename(\n            {\"item_2\": value_col, \"occurrences_2\": \"occurrences\", \"prob_2\": \"occurrence_probability\"},\n        )\n\n        product_pairs = cooccurrences.join(\n            product_occurrences_1_rename,\n            predicates=[cooccurrences[\"item_1\"] == product_occurrences_1_rename[\"item_1\"]],\n        )\n        product_pairs = product_pairs.join(\n            product_occurrences_2_rename,\n            predicates=[product_pairs[\"item_2\"] == product_occurrences_2_rename[\"item_2\"]],\n        )\n\n        product_pairs = product_pairs.mutate(\n            confidence=product_pairs[\"cooccurrences\"] / product_pairs[\"occurrences_1\"],\n            uplift=product_pairs[\"support\"] / (product_pairs[\"prob_1\"] * product_pairs[\"prob_2\"]),\n        )\n\n        result = product_pairs.filter(product_pairs.uplift &gt;= min_uplift)\n\n        if target_item is None:\n            col_order = [\n                \"item_1\",\n                \"item_2\",\n                \"occurrences_1\",\n                \"occurrences_2\",\n                \"cooccurrences\",\n                \"support\",\n                \"confidence\",\n                \"uplift\",\n            ]\n            inverse_pairs = result.mutate(\n                item_1=result[\"item_2\"],\n                item_2=result[\"item_1\"],\n                occurrences_1=result[\"occurrences_2\"],\n                occurrences_2=result[\"occurrences_1\"],\n                prob_1=result[\"prob_2\"],\n                prob_2=result[\"prob_1\"],\n                confidence=result[\"cooccurrences\"] / result[\"occurrences_2\"],\n            )\n            result = result[col_order].union(inverse_pairs[col_order])\n\n        result = result.filter(result.confidence &gt;= min_confidence)\n        final_result = result.order_by([\"item_1\", \"item_2\"])\n        final_result = final_result.rename(\n            {\n                f\"{value_col}_1\": \"item_1\",\n                f\"{value_col}_2\": \"item_2\",\n            },\n        )\n        return final_result[\n            [\n                f\"{value_col}_1\",\n                f\"{value_col}_2\",\n                \"occurrences_1\",\n                \"occurrences_2\",\n                \"cooccurrences\",\n                \"support\",\n                \"confidence\",\n                \"uplift\",\n            ]\n        ]\n\n    @property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"Returns the executed DataFrame.\"\"\"\n        if self._df is None:\n            self._df = self.table.execute().reset_index(drop=True)\n        return self._df\n</code></pre>"},{"location":"api/analysis/product_association/#pyretailscience.analysis.product_association.ProductAssociation.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Returns the executed DataFrame.</p>"},{"location":"api/analysis/product_association/#pyretailscience.analysis.product_association.ProductAssociation.__init__","title":"<code>__init__(df, value_col, group_col=get_option('column.customer_id'), target_item=None, min_occurrences=1, min_cooccurrences=1, min_support=0.0, min_confidence=0.0, min_uplift=0.0)</code>","text":"<p>Initialize the ProductAssociation object.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame | ibis.Table) </code> <p>The input DataFrame or ibis Table containing transaction data.</p> required <code>value_col</code> <code>str</code> <p>The name of the column in the input DataFrame that contains the product identifiers.</p> required <code>group_col</code> <code>str</code> <p>The name of the column that identifies unique transactions or customers. Defaults to option column.unit_spend.</p> <code>get_option('column.customer_id')</code> <code>target_item</code> <code>str or None</code> <p>A specific product to focus the association analysis on. If None, associations for all products are calculated. Defaults to None.</p> <code>None</code> <code>min_occurrences</code> <code>int</code> <p>The minimum number of occurrences required for each product in the association analysis. Defaults to 1. Must be at least 1.</p> <code>1</code> <code>min_cooccurrences</code> <code>int</code> <p>The minimum number of co-occurrences required for the product pairs in the association analysis. Defaults to 1. Must be at least 1.</p> <code>1</code> <code>min_support</code> <code>float</code> <p>The minimum support value required for the association rules. Defaults to 0.0. Must be between 0 and 1.</p> <code>0.0</code> <code>min_confidence</code> <code>float</code> <p>The minimum confidence value required for the association rules. Defaults to 0.0. Must be between 0 and 1.</p> <code>0.0</code> <code>min_uplift</code> <code>float</code> <p>The minimum uplift value required for the association rules. Defaults to 0.0. Must be greater or equal to 0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of combinations is not 2 or 3, or if any of the minimum values are invalid.</p> <code>ValueError</code> <p>If the minimum support, confidence, or uplift values are outside the valid range.</p> <code>ValueError</code> <p>If the minimum occurrences or cooccurrences are less than 1.</p> <code>ValueError</code> <p>If the input DataFrame does not contain the required columns or if they have null values.</p> Source code in <code>pyretailscience/analysis/product_association.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame | ibis.Table,\n    value_col: str,\n    group_col: str = get_option(\"column.customer_id\"),\n    target_item: str | None = None,\n    min_occurrences: int = 1,\n    min_cooccurrences: int = 1,\n    min_support: float = 0.0,\n    min_confidence: float = 0.0,\n    min_uplift: float = 0.0,\n) -&gt; None:\n    \"\"\"Initialize the ProductAssociation object.\n\n    Args:\n        df (pd.DataFrame | ibis.Table) : The input DataFrame or ibis Table containing transaction data.\n        value_col (str): The name of the column in the input DataFrame that contains the product identifiers.\n        group_col (str, optional): The name of the column that identifies unique transactions or customers. Defaults\n            to option column.unit_spend.\n        target_item (str or None, optional): A specific product to focus the association analysis on. If None,\n            associations for all products are calculated. Defaults to None.\n        min_occurrences (int, optional): The minimum number of occurrences required for each product in the\n            association analysis. Defaults to 1. Must be at least 1.\n        min_cooccurrences (int, optional): The minimum number of co-occurrences required for the product pairs in\n            the association analysis. Defaults to 1. Must be at least 1.\n        min_support (float, optional): The minimum support value required for the association rules. Defaults to\n            0.0. Must be between 0 and 1.\n        min_confidence (float, optional): The minimum confidence value required for the association rules. Defaults\n            to 0.0. Must be between 0 and 1.\n        min_uplift (float, optional): The minimum uplift value required for the association rules. Defaults to 0.0.\n            Must be greater or equal to 0.\n\n    Raises:\n        ValueError: If the number of combinations is not 2 or 3, or if any of the minimum values are invalid.\n        ValueError: If the minimum support, confidence, or uplift values are outside the valid range.\n        ValueError: If the minimum occurrences or cooccurrences are less than 1.\n        ValueError: If the input DataFrame does not contain the required columns or if they have null values.\n    \"\"\"\n    required_cols = [group_col, value_col]\n    missing_cols = set(required_cols) - set(df.columns)\n    if len(missing_cols) &gt; 0:\n        msg = f\"The following columns are required but missing: {missing_cols}\"\n        raise ValueError(msg)\n\n    self.table = self._calc_association(\n        df=df,\n        value_col=value_col,\n        group_col=group_col,\n        target_item=target_item,\n        min_occurrences=min_occurrences,\n        min_cooccurrences=min_cooccurrences,\n        min_support=min_support,\n        min_confidence=min_confidence,\n        min_uplift=min_uplift,\n    )\n</code></pre>"},{"location":"api/analysis/revenue_tree/","title":"Customer Analysis","text":"<p>Revenue Tree Analysis Module.</p> <p>This module implements a Revenue Tree analysis for retail businesses. The Revenue Tree is a hierarchical breakdown of factors contributing to overall revenue, allowing for detailed analysis of sales performance and identification of areas for improvement.</p> <p>Key Components of the Revenue Tree:</p> <ol> <li> <p>Revenue: The top-level metric, calculated as Customers * Revenue per Customer.</p> </li> <li> <p>Revenue per Customer: Average revenue generated per customer, calculated as:    Orders per Customer * Average Order Value.</p> </li> <li> <p>Orders per Customer: Average number of orders placed by each customer.</p> </li> <li> <p>Average Order Value: Average monetary value of each order, calculated as:    Items per Order * Price per Item.</p> </li> <li> <p>Items per Order: Average number of items in each order.</p> </li> <li> <p>Price per Item: Average price of each item sold.</p> </li> </ol> <p>This module can be used to create, update, and analyze Revenue Tree data structures for retail businesses, helping to identify key drivers of revenue changes and inform strategic decision-making.</p>"},{"location":"api/analysis/revenue_tree/#pyretailscience.analysis.revenue_tree.RevenueTree","title":"<code>RevenueTree</code>","text":"<p>Revenue Tree Analysis Class.</p> Source code in <code>pyretailscience/analysis/revenue_tree.py</code> <pre><code>@plugin_manager.extensible\nclass RevenueTree:\n    \"\"\"Revenue Tree Analysis Class.\"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame | ibis.Table,\n        period_col: str,\n        p1_value: str,\n        p2_value: str,\n        group_col: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Revenue Tree Analysis Class.\n\n        Args:\n            df (pd.DataFrame | ibis.Table): The input DataFrame or ibis Table containing transaction data.\n            period_col (str): The column representing the period.\n            p1_value (str): The value representing the first period.\n            p2_value (str): The value representing the second period.\n            group_col (str, optional): The column to group the data by. Defaults to None.\n\n        Raises:\n            ValueError: If the required columns are not present in the DataFrame.\n        \"\"\"\n        cols = ColumnHelper()\n\n        required_cols = [\n            cols.customer_id,\n            cols.transaction_id,\n            cols.unit_spend,\n        ]\n        if cols.unit_qty in df.columns:\n            required_cols.append(cols.unit_qty)\n\n        if group_col is not None:\n            required_cols.append(group_col)\n\n        missing_cols = set(required_cols) - set(df.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        df, p1_index, p2_index = self._agg_data(df, period_col, p1_value, p2_value, group_col)\n\n        self.df = calc_tree_kpis(\n            df=df,\n            p1_index=p1_index,\n            p2_index=p2_index,\n        )\n\n    @staticmethod\n    def _agg_data(\n        df: pd.DataFrame | ibis.Table,\n        period_col: str,\n        p1_value: str,\n        p2_value: str,\n        group_col: str | None = None,\n    ) -&gt; tuple[pd.DataFrame, list[bool], list[bool]]:\n        cols = ColumnHelper()\n\n        if isinstance(df, pd.DataFrame):\n            df: ibis.Table = ibis.memtable(df)\n\n        aggs = {\n            cols.agg_customer_id: df[cols.customer_id].nunique(),\n            cols.agg_transaction_id: df[cols.transaction_id].nunique(),\n            cols.agg_unit_spend: df[cols.unit_spend].sum(),\n        }\n        if cols.unit_qty in df.columns:\n            aggs[cols.agg_unit_qty] = df[cols.unit_qty].sum()\n\n        group_by_cols = [group_col, period_col] if group_col else [period_col]\n        df = pd.DataFrame(df.group_by(group_by_cols).aggregate(**aggs).execute())\n        p1_df = df[df[period_col] == p1_value].drop(columns=[period_col])\n        p2_df = df[df[period_col] == p2_value].drop(columns=[period_col])\n\n        if group_col is not None:\n            p1_df = p1_df.sort_values(by=group_col)\n            p2_df = p2_df.sort_values(by=group_col)\n\n        new_p1_index = [True] * len(p1_df) + [False] * len(p2_df)\n        new_p2_index = [not i for i in new_p1_index]\n\n        result_df = pd.concat([p1_df, p2_df], ignore_index=True)\n\n        if group_col is None:\n            result_df.index = [\"p1\", \"p2\"]\n        else:\n            result_df.set_index(group_col, inplace=True)\n            result_df.index = pd.CategoricalIndex(result_df.index)\n        return result_df, new_p1_index, new_p2_index\n\n    @staticmethod\n    def _get_final_col_order(include_quantity: bool) -&gt; str:\n        cols = ColumnHelper()\n        col_order = [\n            # Customers\n            cols.agg_customer_id_p1,\n            cols.agg_customer_id_p2,\n            cols.agg_customer_id_diff,\n            cols.agg_customer_id_pct_diff,\n            cols.agg_customer_id_contrib,\n            # Transactions\n            cols.agg_transaction_id_p1,\n            cols.agg_transaction_id_p2,\n            cols.agg_transaction_id_diff,\n            cols.agg_transaction_id_pct_diff,\n            # Unit Spend\n            cols.agg_unit_spend_p1,\n            cols.agg_unit_spend_p2,\n            cols.agg_unit_spend_diff,\n            cols.agg_unit_spend_pct_diff,\n            # Spend / Customer\n            cols.calc_spend_per_cust_p1,\n            cols.calc_spend_per_cust_p2,\n            cols.calc_spend_per_cust_diff,\n            cols.calc_spend_per_cust_pct_diff,\n            cols.calc_spend_per_cust_contrib,\n            # Transactions / Customer\n            cols.calc_trans_per_cust_p1,\n            cols.calc_trans_per_cust_p2,\n            cols.calc_trans_per_cust_diff,\n            cols.calc_trans_per_cust_pct_diff,\n            cols.calc_trans_per_cust_contrib,\n            # Spend / Transaction\n            cols.calc_spend_per_trans_p1,\n            cols.calc_spend_per_trans_p2,\n            cols.calc_spend_per_trans_diff,\n            cols.calc_spend_per_trans_pct_diff,\n            cols.calc_spend_per_trans_contrib,\n            # Elasticity\n            cols.calc_frequency_elasticity,\n        ]\n\n        if include_quantity:\n            col_order.extend(\n                [\n                    # Unit Quantity\n                    cols.agg_unit_qty_p1,\n                    cols.agg_unit_qty_p2,\n                    cols.agg_unit_qty_diff,\n                    cols.agg_unit_qty_pct_diff,\n                    # Quantity / Transaction\n                    cols.calc_units_per_trans_p1,\n                    cols.calc_units_per_trans_p2,\n                    cols.calc_units_per_trans_diff,\n                    cols.calc_units_per_trans_pct_diff,\n                    cols.calc_units_per_trans_contrib,\n                    # Price / Unit\n                    cols.calc_price_per_unit_p1,\n                    cols.calc_price_per_unit_p2,\n                    cols.calc_price_per_unit_diff,\n                    cols.calc_price_per_unit_pct_diff,\n                    cols.calc_price_per_unit_contrib,\n                    # Price Elasticity\n                    cols.calc_price_elasticity,\n                ],\n            )\n\n        return col_order\n\n    @staticmethod\n    def _check_graphviz_installation() -&gt; bool:\n        \"\"\"Check if Graphviz is installed on the system.\n\n        Returns:\n            bool: True if Graphviz is installed, False otherwise.\n        \"\"\"\n        system = platform.system().lower()\n        try:\n            subprocess.run([\"dot\", \"-V\"], check=True, stderr=subprocess.DEVNULL, shell=(system == \"windows\"))  # noqa: S603 S607\n        except FileNotFoundError:\n            return False\n        except subprocess.CalledProcessError:\n            return False\n\n        return True\n\n    def draw_tree(\n        self,\n        tree_index: int = 0,\n        value_labels: tuple[str, str] | None = None,\n        unit_spend_label: str = \"Revenue\",\n        customer_id_label: str = \"Customers\",\n        spend_per_customer_label: str = \"Spend / Customer\",\n        transactions_per_customer_label: str = \"Visits / Customer\",\n        spend_per_transaction_label: str = \"Spend / Visit\",\n        units_per_transaction_label: str = \"Units / Visit\",\n        price_per_unit_label: str = \"Price / Unit\",\n        humman_format: bool = False,\n    ) -&gt; graphviz.Digraph:\n        \"\"\"Draw the Revenue Tree graph as a Graphviz visualization.\n\n        Args:\n            tree_index (int, optional): The index of the tree to draw. Defaults to 0. Used when the group_col is\n                specified and multiple trees are generated.\n            value_labels (tuple[str, str], optional): Labels for the value columns. Defaults to None. When None, the\n                default labels of Current Period and Previous Period are used for P1 and P2.\n            unit_spend_label (str, optional): Label for the Revenue column. Defaults to \"Revenue\".\n            customer_id_label (str, optional): Label for the Customers column. Defaults to \"Customers\".\n            spend_per_customer_label (str, optional): Label for the Spend / Customer column. Defaults to\n                \"Spend / Customer\".\n            transactions_per_customer_label (str, optional): Label for the Visits / Customer column. Defaults to\n                \"Visits / Customer\".\n            spend_per_transaction_label (str, optional): Label for the Spend / Visit column. Defaults to\n                \"Spend / Visit\".\n            units_per_transaction_label (str, optional): Label for the Units / Visit column. Defaults to\n                \"Units / Visit\".\n            price_per_unit_label (str, optional): Label for the Price / Unit column. Defaults to\n                \"Price / Unit\".\n            humman_format (bool, optional): Whether to use human-readable formatting. Defaults to False.\n\n        Returns:\n            graphviz.Digraph: The Graphviz visualization of the Revenue Tree.\n        \"\"\"\n        cols = ColumnHelper()\n\n        if not self._check_graphviz_installation():\n            raise ImportError(\n                \"Graphviz is required to draw the Revenue Tree graph. See here for installation instructions: \"\n                \"https://github.com/xflr6/graphviz?tab=readme-ov-file#installation\",\n            )\n        graph = graphviz.Digraph()\n        graph.attr(\"graph\", bgcolor=\"transparent\")\n\n        graph_data = self.df.to_dict(orient=\"records\")[tree_index]\n\n        self.build_node(\n            graph,\n            title=unit_spend_label,\n            name=\"agg_unit_spend\",\n            p2_value=graph_data[cols.agg_unit_spend_p2],\n            p1_value=graph_data[cols.agg_unit_spend_p1],\n            value_labels=value_labels,\n            humman_format=humman_format,\n        )\n\n        self.build_node(\n            graph,\n            title=customer_id_label,\n            name=\"agg_customer_id\",\n            p2_value=graph_data[cols.agg_customer_id_p2],\n            p1_value=graph_data[cols.agg_customer_id_p1],\n            contrib_value=graph_data[cols.agg_customer_id_contrib],\n            value_labels=value_labels,\n            humman_format=humman_format,\n        )\n\n        # Spend / Cust\n        self.build_node(\n            graph,\n            title=spend_per_customer_label,\n            name=\"calc_spend_per_customer\",\n            p2_value=graph_data[cols.calc_spend_per_cust_p2],\n            p1_value=graph_data[cols.calc_spend_per_cust_p1],\n            contrib_value=graph_data[cols.calc_spend_per_cust_contrib],\n            value_labels=value_labels,\n            humman_format=humman_format,\n        )\n\n        # Visits / Customer\n        self.build_node(\n            graph,\n            title=transactions_per_customer_label,\n            name=\"calc_transactions_per_customer\",\n            p2_value=graph_data[cols.calc_trans_per_cust_p2],\n            p1_value=graph_data[cols.calc_trans_per_cust_p1],\n            contrib_value=graph_data[cols.calc_trans_per_cust_contrib],\n            value_labels=value_labels,\n            humman_format=humman_format,\n        )\n        # Spend / Visit\n        self.build_node(\n            graph,\n            title=spend_per_transaction_label,\n            name=\"calc_spend_per_transaction\",\n            p2_value=graph_data[cols.calc_spend_per_trans_p2],\n            p1_value=graph_data[cols.calc_spend_per_trans_p1],\n            contrib_value=graph_data[cols.calc_spend_per_trans_contrib],\n            value_labels=value_labels,\n            humman_format=humman_format,\n        )\n\n        graph.edge(\"agg_unit_spend\", \"calc_spend_per_customer\")\n        graph.edge(\"agg_unit_spend\", \"agg_customer_id\")\n\n        graph.edge(\"calc_spend_per_customer\", \"calc_transactions_per_customer\")\n        graph.edge(\"calc_spend_per_customer\", \"calc_spend_per_transaction\")\n\n        if cols.agg_unit_qty_p1 in graph_data:\n            # Units / Visit\n            self.build_node(\n                graph,\n                title=units_per_transaction_label,\n                name=\"calc_units_per_transaction\",\n                p2_value=graph_data[cols.calc_units_per_trans_p2],\n                p1_value=graph_data[cols.calc_units_per_trans_p1],\n                contrib_value=graph_data[cols.calc_units_per_trans_contrib],\n                value_labels=value_labels,\n                humman_format=humman_format,\n            )\n\n            # Price / Unit\n            self.build_node(\n                graph,\n                title=price_per_unit_label,\n                name=\"calc_price_per_unit\",\n                p2_value=graph_data[cols.calc_price_per_unit_p2],\n                p1_value=graph_data[cols.calc_price_per_unit_p1],\n                contrib_value=graph_data[cols.calc_price_per_unit_contrib],\n                value_labels=value_labels,\n                humman_format=humman_format,\n            )\n\n            graph.edge(\"calc_spend_per_transaction\", \"calc_units_per_transaction\")\n            graph.edge(\"calc_spend_per_transaction\", \"calc_price_per_unit\")\n\n        return graph\n\n    def build_node(\n        self,\n        graph: graphviz.Digraph,\n        title: str,\n        p2_value: float,\n        p1_value: float,\n        contrib_value: float | None = None,\n        name: str | None = None,\n        value_decimal_places: int = 2,\n        diff_decimal_places: int = 2,\n        pct_decimal_places: int = 1,\n        value_labels: tuple[str, str] | None = None,\n        show_diff: bool = True,\n        value_suffix: str = \"\",\n        humman_format: bool = False,\n    ) -&gt; None:\n        \"\"\"Build a node for the Revenue Tree graph.\"\"\"\n        if name is None:\n            name = title\n        if value_labels is None:\n            value_labels = (\"Current Period\", \"Previous Period\")\n\n        diff = p2_value - p1_value\n\n        if humman_format:\n            p2_value_str = (gu.human_format(p2_value, 0, decimals=value_decimal_places) + \" \" + value_suffix).strip()\n            p1_value_str = (gu.human_format(p1_value, 0, decimals=value_decimal_places) + \" \" + value_suffix).strip()\n            diff_str = (gu.human_format(diff, 0, decimals=diff_decimal_places) + \" \" + value_suffix).strip()\n        else:\n            style = \",\" if isinstance(p2_value, int) else f\",.{value_decimal_places}f\"\n            p2_value_str = f\"{p2_value:{style}} {value_suffix}\".strip()\n            p1_value_str = f\"{p1_value:{style}} {value_suffix}\".strip()\n            diff_str = f\"{diff:{style}} {value_suffix}\".strip()\n\n        pct_diff_str = \"N/A - Divide By 0\" if p1_value == 0 else f\"{diff / p1_value * 100:,.{pct_decimal_places}f}%\"\n\n        diff_color = \"darkgreen\" if diff &gt;= 0 else \"red\"\n\n        height = 1.5\n        diff_html = \"\"\n        if show_diff:\n            diff_html = dedent(\n                f\"\"\"\n            &lt;tr&gt;\n                &lt;td align=\"right\"&gt;&lt;font color=\"white\" face=\"arial\"&gt;&lt;b&gt;Diff&amp;nbsp;&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;\n                &lt;td bgcolor=\"white\"&gt;&lt;font color=\"{diff_color}\" face=\"arial\"&gt;{diff_str}&lt;/font&gt;&lt;/td&gt;\n            &lt;/tr&gt;\n            \"\"\",\n            )\n            height += 0.25\n\n        contrib_html = \"\"\n        if contrib_value is not None:\n            contrib_str = gu.human_format(contrib_value, 0, decimals=value_decimal_places)\n            contrib_color = \"darkgreen\" if diff &gt;= 0 else \"red\"\n            contrib_html = dedent(\n                f\"\"\"\n            &lt;tr&gt;\n                &lt;td align=\"right\"&gt;&lt;font color=\"white\" face=\"arial\"&gt;&lt;b&gt;Contribution&amp;nbsp;&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;\n                &lt;td bgcolor=\"white\"&gt;&lt;font color=\"{contrib_color}\" face=\"arial\"&gt;{contrib_str}&lt;/font&gt;&lt;/td&gt;\n            &lt;/tr&gt;\n            \"\"\",\n            )\n            height += 0.25\n\n        graph.node(\n            name=name,\n            shape=\"box\",\n            style=\"filled, rounded\",\n            color=COLORS[\"green\"][500],\n            width=\"4\",\n            height=str(height),\n            align=\"center\",\n            label=dedent(\n                f\"\"\"&lt;\n                &lt;table border=\"0\" align=\"center\" width=\"100%\"&gt;\n                    &lt;tr&gt;&lt;td colspan=\"2\"&gt;&lt;font point-size=\"18\" color=\"white\" face=\"arial\"&gt;&lt;b&gt;{title}&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;\n                    &lt;tr&gt;\n                        &lt;td width=\"150%\"&gt;&lt;font color=\"white\" face=\"arial\"&gt;&lt;b&gt;{value_labels[0]}&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;\n                        &lt;td width=\"150%\"&gt;&lt;font color=\"white\" face=\"arial\"&gt;&lt;b&gt;{value_labels[1]}&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr&gt;\n                        &lt;td bgcolor=\"white\"&gt;&lt;font face=\"arial\"&gt;{p2_value_str}&lt;/font&gt;&lt;/td&gt;\n                        &lt;td bgcolor=\"white\"&gt;&lt;font face=\"arial\"&gt;{p1_value_str}&lt;/font&gt;&lt;/td&gt;\n                    &lt;/tr&gt;\n                    {diff_html}\n                    &lt;tr&gt;\n                        &lt;td align=\"right\"&gt;&lt;font color=\"white\" face=\"arial\"&gt;&lt;b&gt;Pct Diff&amp;nbsp;&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;\n                        &lt;td bgcolor=\"white\"&gt;&lt;font color=\"{diff_color}\" face=\"arial\"&gt;{pct_diff_str}&lt;/font&gt;&lt;/td&gt;\n                    &lt;/tr&gt;\n                    {contrib_html}\n                &lt;/table&gt;\n                &gt;\"\"\",\n            ),\n        )\n</code></pre>"},{"location":"api/analysis/revenue_tree/#pyretailscience.analysis.revenue_tree.RevenueTree.__init__","title":"<code>__init__(df, period_col, p1_value, p2_value, group_col=None)</code>","text":"<p>Initialize the Revenue Tree Analysis Class.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame | Table</code> <p>The input DataFrame or ibis Table containing transaction data.</p> required <code>period_col</code> <code>str</code> <p>The column representing the period.</p> required <code>p1_value</code> <code>str</code> <p>The value representing the first period.</p> required <code>p2_value</code> <code>str</code> <p>The value representing the second period.</p> required <code>group_col</code> <code>str</code> <p>The column to group the data by. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the required columns are not present in the DataFrame.</p> Source code in <code>pyretailscience/analysis/revenue_tree.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame | ibis.Table,\n    period_col: str,\n    p1_value: str,\n    p2_value: str,\n    group_col: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Revenue Tree Analysis Class.\n\n    Args:\n        df (pd.DataFrame | ibis.Table): The input DataFrame or ibis Table containing transaction data.\n        period_col (str): The column representing the period.\n        p1_value (str): The value representing the first period.\n        p2_value (str): The value representing the second period.\n        group_col (str, optional): The column to group the data by. Defaults to None.\n\n    Raises:\n        ValueError: If the required columns are not present in the DataFrame.\n    \"\"\"\n    cols = ColumnHelper()\n\n    required_cols = [\n        cols.customer_id,\n        cols.transaction_id,\n        cols.unit_spend,\n    ]\n    if cols.unit_qty in df.columns:\n        required_cols.append(cols.unit_qty)\n\n    if group_col is not None:\n        required_cols.append(group_col)\n\n    missing_cols = set(required_cols) - set(df.columns)\n    if len(missing_cols) &gt; 0:\n        msg = f\"The following columns are required but missing: {missing_cols}\"\n        raise ValueError(msg)\n\n    df, p1_index, p2_index = self._agg_data(df, period_col, p1_value, p2_value, group_col)\n\n    self.df = calc_tree_kpis(\n        df=df,\n        p1_index=p1_index,\n        p2_index=p2_index,\n    )\n</code></pre>"},{"location":"api/analysis/revenue_tree/#pyretailscience.analysis.revenue_tree.RevenueTree.build_node","title":"<code>build_node(graph, title, p2_value, p1_value, contrib_value=None, name=None, value_decimal_places=2, diff_decimal_places=2, pct_decimal_places=1, value_labels=None, show_diff=True, value_suffix='', humman_format=False)</code>","text":"<p>Build a node for the Revenue Tree graph.</p> Source code in <code>pyretailscience/analysis/revenue_tree.py</code> <pre><code>def build_node(\n    self,\n    graph: graphviz.Digraph,\n    title: str,\n    p2_value: float,\n    p1_value: float,\n    contrib_value: float | None = None,\n    name: str | None = None,\n    value_decimal_places: int = 2,\n    diff_decimal_places: int = 2,\n    pct_decimal_places: int = 1,\n    value_labels: tuple[str, str] | None = None,\n    show_diff: bool = True,\n    value_suffix: str = \"\",\n    humman_format: bool = False,\n) -&gt; None:\n    \"\"\"Build a node for the Revenue Tree graph.\"\"\"\n    if name is None:\n        name = title\n    if value_labels is None:\n        value_labels = (\"Current Period\", \"Previous Period\")\n\n    diff = p2_value - p1_value\n\n    if humman_format:\n        p2_value_str = (gu.human_format(p2_value, 0, decimals=value_decimal_places) + \" \" + value_suffix).strip()\n        p1_value_str = (gu.human_format(p1_value, 0, decimals=value_decimal_places) + \" \" + value_suffix).strip()\n        diff_str = (gu.human_format(diff, 0, decimals=diff_decimal_places) + \" \" + value_suffix).strip()\n    else:\n        style = \",\" if isinstance(p2_value, int) else f\",.{value_decimal_places}f\"\n        p2_value_str = f\"{p2_value:{style}} {value_suffix}\".strip()\n        p1_value_str = f\"{p1_value:{style}} {value_suffix}\".strip()\n        diff_str = f\"{diff:{style}} {value_suffix}\".strip()\n\n    pct_diff_str = \"N/A - Divide By 0\" if p1_value == 0 else f\"{diff / p1_value * 100:,.{pct_decimal_places}f}%\"\n\n    diff_color = \"darkgreen\" if diff &gt;= 0 else \"red\"\n\n    height = 1.5\n    diff_html = \"\"\n    if show_diff:\n        diff_html = dedent(\n            f\"\"\"\n        &lt;tr&gt;\n            &lt;td align=\"right\"&gt;&lt;font color=\"white\" face=\"arial\"&gt;&lt;b&gt;Diff&amp;nbsp;&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;\n            &lt;td bgcolor=\"white\"&gt;&lt;font color=\"{diff_color}\" face=\"arial\"&gt;{diff_str}&lt;/font&gt;&lt;/td&gt;\n        &lt;/tr&gt;\n        \"\"\",\n        )\n        height += 0.25\n\n    contrib_html = \"\"\n    if contrib_value is not None:\n        contrib_str = gu.human_format(contrib_value, 0, decimals=value_decimal_places)\n        contrib_color = \"darkgreen\" if diff &gt;= 0 else \"red\"\n        contrib_html = dedent(\n            f\"\"\"\n        &lt;tr&gt;\n            &lt;td align=\"right\"&gt;&lt;font color=\"white\" face=\"arial\"&gt;&lt;b&gt;Contribution&amp;nbsp;&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;\n            &lt;td bgcolor=\"white\"&gt;&lt;font color=\"{contrib_color}\" face=\"arial\"&gt;{contrib_str}&lt;/font&gt;&lt;/td&gt;\n        &lt;/tr&gt;\n        \"\"\",\n        )\n        height += 0.25\n\n    graph.node(\n        name=name,\n        shape=\"box\",\n        style=\"filled, rounded\",\n        color=COLORS[\"green\"][500],\n        width=\"4\",\n        height=str(height),\n        align=\"center\",\n        label=dedent(\n            f\"\"\"&lt;\n            &lt;table border=\"0\" align=\"center\" width=\"100%\"&gt;\n                &lt;tr&gt;&lt;td colspan=\"2\"&gt;&lt;font point-size=\"18\" color=\"white\" face=\"arial\"&gt;&lt;b&gt;{title}&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;\n                &lt;tr&gt;\n                    &lt;td width=\"150%\"&gt;&lt;font color=\"white\" face=\"arial\"&gt;&lt;b&gt;{value_labels[0]}&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;\n                    &lt;td width=\"150%\"&gt;&lt;font color=\"white\" face=\"arial\"&gt;&lt;b&gt;{value_labels[1]}&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;\n                &lt;/tr&gt;\n                &lt;tr&gt;\n                    &lt;td bgcolor=\"white\"&gt;&lt;font face=\"arial\"&gt;{p2_value_str}&lt;/font&gt;&lt;/td&gt;\n                    &lt;td bgcolor=\"white\"&gt;&lt;font face=\"arial\"&gt;{p1_value_str}&lt;/font&gt;&lt;/td&gt;\n                &lt;/tr&gt;\n                {diff_html}\n                &lt;tr&gt;\n                    &lt;td align=\"right\"&gt;&lt;font color=\"white\" face=\"arial\"&gt;&lt;b&gt;Pct Diff&amp;nbsp;&lt;/b&gt;&lt;/font&gt;&lt;/td&gt;\n                    &lt;td bgcolor=\"white\"&gt;&lt;font color=\"{diff_color}\" face=\"arial\"&gt;{pct_diff_str}&lt;/font&gt;&lt;/td&gt;\n                &lt;/tr&gt;\n                {contrib_html}\n            &lt;/table&gt;\n            &gt;\"\"\",\n        ),\n    )\n</code></pre>"},{"location":"api/analysis/revenue_tree/#pyretailscience.analysis.revenue_tree.RevenueTree.draw_tree","title":"<code>draw_tree(tree_index=0, value_labels=None, unit_spend_label='Revenue', customer_id_label='Customers', spend_per_customer_label='Spend / Customer', transactions_per_customer_label='Visits / Customer', spend_per_transaction_label='Spend / Visit', units_per_transaction_label='Units / Visit', price_per_unit_label='Price / Unit', humman_format=False)</code>","text":"<p>Draw the Revenue Tree graph as a Graphviz visualization.</p> <p>Parameters:</p> Name Type Description Default <code>tree_index</code> <code>int</code> <p>The index of the tree to draw. Defaults to 0. Used when the group_col is specified and multiple trees are generated.</p> <code>0</code> <code>value_labels</code> <code>tuple[str, str]</code> <p>Labels for the value columns. Defaults to None. When None, the default labels of Current Period and Previous Period are used for P1 and P2.</p> <code>None</code> <code>unit_spend_label</code> <code>str</code> <p>Label for the Revenue column. Defaults to \"Revenue\".</p> <code>'Revenue'</code> <code>customer_id_label</code> <code>str</code> <p>Label for the Customers column. Defaults to \"Customers\".</p> <code>'Customers'</code> <code>spend_per_customer_label</code> <code>str</code> <p>Label for the Spend / Customer column. Defaults to \"Spend / Customer\".</p> <code>'Spend / Customer'</code> <code>transactions_per_customer_label</code> <code>str</code> <p>Label for the Visits / Customer column. Defaults to \"Visits / Customer\".</p> <code>'Visits / Customer'</code> <code>spend_per_transaction_label</code> <code>str</code> <p>Label for the Spend / Visit column. Defaults to \"Spend / Visit\".</p> <code>'Spend / Visit'</code> <code>units_per_transaction_label</code> <code>str</code> <p>Label for the Units / Visit column. Defaults to \"Units / Visit\".</p> <code>'Units / Visit'</code> <code>price_per_unit_label</code> <code>str</code> <p>Label for the Price / Unit column. Defaults to \"Price / Unit\".</p> <code>'Price / Unit'</code> <code>humman_format</code> <code>bool</code> <p>Whether to use human-readable formatting. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Digraph</code> <p>graphviz.Digraph: The Graphviz visualization of the Revenue Tree.</p> Source code in <code>pyretailscience/analysis/revenue_tree.py</code> <pre><code>def draw_tree(\n    self,\n    tree_index: int = 0,\n    value_labels: tuple[str, str] | None = None,\n    unit_spend_label: str = \"Revenue\",\n    customer_id_label: str = \"Customers\",\n    spend_per_customer_label: str = \"Spend / Customer\",\n    transactions_per_customer_label: str = \"Visits / Customer\",\n    spend_per_transaction_label: str = \"Spend / Visit\",\n    units_per_transaction_label: str = \"Units / Visit\",\n    price_per_unit_label: str = \"Price / Unit\",\n    humman_format: bool = False,\n) -&gt; graphviz.Digraph:\n    \"\"\"Draw the Revenue Tree graph as a Graphviz visualization.\n\n    Args:\n        tree_index (int, optional): The index of the tree to draw. Defaults to 0. Used when the group_col is\n            specified and multiple trees are generated.\n        value_labels (tuple[str, str], optional): Labels for the value columns. Defaults to None. When None, the\n            default labels of Current Period and Previous Period are used for P1 and P2.\n        unit_spend_label (str, optional): Label for the Revenue column. Defaults to \"Revenue\".\n        customer_id_label (str, optional): Label for the Customers column. Defaults to \"Customers\".\n        spend_per_customer_label (str, optional): Label for the Spend / Customer column. Defaults to\n            \"Spend / Customer\".\n        transactions_per_customer_label (str, optional): Label for the Visits / Customer column. Defaults to\n            \"Visits / Customer\".\n        spend_per_transaction_label (str, optional): Label for the Spend / Visit column. Defaults to\n            \"Spend / Visit\".\n        units_per_transaction_label (str, optional): Label for the Units / Visit column. Defaults to\n            \"Units / Visit\".\n        price_per_unit_label (str, optional): Label for the Price / Unit column. Defaults to\n            \"Price / Unit\".\n        humman_format (bool, optional): Whether to use human-readable formatting. Defaults to False.\n\n    Returns:\n        graphviz.Digraph: The Graphviz visualization of the Revenue Tree.\n    \"\"\"\n    cols = ColumnHelper()\n\n    if not self._check_graphviz_installation():\n        raise ImportError(\n            \"Graphviz is required to draw the Revenue Tree graph. See here for installation instructions: \"\n            \"https://github.com/xflr6/graphviz?tab=readme-ov-file#installation\",\n        )\n    graph = graphviz.Digraph()\n    graph.attr(\"graph\", bgcolor=\"transparent\")\n\n    graph_data = self.df.to_dict(orient=\"records\")[tree_index]\n\n    self.build_node(\n        graph,\n        title=unit_spend_label,\n        name=\"agg_unit_spend\",\n        p2_value=graph_data[cols.agg_unit_spend_p2],\n        p1_value=graph_data[cols.agg_unit_spend_p1],\n        value_labels=value_labels,\n        humman_format=humman_format,\n    )\n\n    self.build_node(\n        graph,\n        title=customer_id_label,\n        name=\"agg_customer_id\",\n        p2_value=graph_data[cols.agg_customer_id_p2],\n        p1_value=graph_data[cols.agg_customer_id_p1],\n        contrib_value=graph_data[cols.agg_customer_id_contrib],\n        value_labels=value_labels,\n        humman_format=humman_format,\n    )\n\n    # Spend / Cust\n    self.build_node(\n        graph,\n        title=spend_per_customer_label,\n        name=\"calc_spend_per_customer\",\n        p2_value=graph_data[cols.calc_spend_per_cust_p2],\n        p1_value=graph_data[cols.calc_spend_per_cust_p1],\n        contrib_value=graph_data[cols.calc_spend_per_cust_contrib],\n        value_labels=value_labels,\n        humman_format=humman_format,\n    )\n\n    # Visits / Customer\n    self.build_node(\n        graph,\n        title=transactions_per_customer_label,\n        name=\"calc_transactions_per_customer\",\n        p2_value=graph_data[cols.calc_trans_per_cust_p2],\n        p1_value=graph_data[cols.calc_trans_per_cust_p1],\n        contrib_value=graph_data[cols.calc_trans_per_cust_contrib],\n        value_labels=value_labels,\n        humman_format=humman_format,\n    )\n    # Spend / Visit\n    self.build_node(\n        graph,\n        title=spend_per_transaction_label,\n        name=\"calc_spend_per_transaction\",\n        p2_value=graph_data[cols.calc_spend_per_trans_p2],\n        p1_value=graph_data[cols.calc_spend_per_trans_p1],\n        contrib_value=graph_data[cols.calc_spend_per_trans_contrib],\n        value_labels=value_labels,\n        humman_format=humman_format,\n    )\n\n    graph.edge(\"agg_unit_spend\", \"calc_spend_per_customer\")\n    graph.edge(\"agg_unit_spend\", \"agg_customer_id\")\n\n    graph.edge(\"calc_spend_per_customer\", \"calc_transactions_per_customer\")\n    graph.edge(\"calc_spend_per_customer\", \"calc_spend_per_transaction\")\n\n    if cols.agg_unit_qty_p1 in graph_data:\n        # Units / Visit\n        self.build_node(\n            graph,\n            title=units_per_transaction_label,\n            name=\"calc_units_per_transaction\",\n            p2_value=graph_data[cols.calc_units_per_trans_p2],\n            p1_value=graph_data[cols.calc_units_per_trans_p1],\n            contrib_value=graph_data[cols.calc_units_per_trans_contrib],\n            value_labels=value_labels,\n            humman_format=humman_format,\n        )\n\n        # Price / Unit\n        self.build_node(\n            graph,\n            title=price_per_unit_label,\n            name=\"calc_price_per_unit\",\n            p2_value=graph_data[cols.calc_price_per_unit_p2],\n            p1_value=graph_data[cols.calc_price_per_unit_p1],\n            contrib_value=graph_data[cols.calc_price_per_unit_contrib],\n            value_labels=value_labels,\n            humman_format=humman_format,\n        )\n\n        graph.edge(\"calc_spend_per_transaction\", \"calc_units_per_transaction\")\n        graph.edge(\"calc_spend_per_transaction\", \"calc_price_per_unit\")\n\n    return graph\n</code></pre>"},{"location":"api/analysis/revenue_tree/#pyretailscience.analysis.revenue_tree.calc_tree_kpis","title":"<code>calc_tree_kpis(df, p1_index, p2_index)</code>","text":"<p>Calculate various key performance indicators (KPIs) for tree analysis.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing relevant data.</p> required <code>p1_index</code> <code>list[bool] | Series</code> <p>Boolean index for period 1.</p> required <code>p2_index</code> <code>list[bool] | Series</code> <p>Boolean index for period 2.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with calculated KPI values, including differences</p> <code>DataFrame</code> <p>and percentage differences between periods.</p> Source code in <code>pyretailscience/analysis/revenue_tree.py</code> <pre><code>@plugin_manager.extensible\ndef calc_tree_kpis(\n    df: pd.DataFrame,\n    p1_index: list[bool] | pd.Series,\n    p2_index: list[bool] | pd.Series,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate various key performance indicators (KPIs) for tree analysis.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing relevant data.\n        p1_index (list[bool] | pd.Series): Boolean index for period 1.\n        p2_index (list[bool] | pd.Series): Boolean index for period 2.\n\n    Returns:\n        pd.DataFrame: A DataFrame with calculated KPI values, including differences\n        and percentage differences between periods.\n    \"\"\"\n    cols = ColumnHelper()\n    required_cols = [cols.agg_customer_id, cols.agg_transaction_id, cols.agg_unit_spend]\n\n    if cols.agg_unit_qty in df.columns:\n        required_cols.append(cols.agg_unit_qty)\n\n    df = df[required_cols].copy()\n    df_cols = df.columns\n\n    if cols.agg_unit_qty in df_cols:\n        df[cols.calc_units_per_trans] = df[cols.agg_unit_qty] / df[cols.agg_transaction_id]\n        df[cols.calc_price_per_unit] = df[cols.agg_unit_spend] / df[cols.agg_unit_qty]\n\n    df[cols.calc_spend_per_cust] = df[cols.agg_unit_spend] / df[cols.agg_customer_id]\n    df[cols.calc_spend_per_trans] = df[cols.agg_unit_spend] / df[cols.agg_transaction_id]\n    df[cols.calc_trans_per_cust] = df[cols.agg_transaction_id] / df[cols.agg_customer_id]\n\n    p1_df = df[p1_index]\n    p1_df.columns = [col + \"_\" + get_option(\"column.suffix.period_1\") for col in p1_df.columns]\n    p2_df = df[p2_index]\n    p2_df.columns = [col + \"_\" + get_option(\"column.suffix.period_2\") for col in p2_df.columns]\n\n    # When df only contains two periods than the indexes should be dropped for proper concatenation\n    if len(df.index) == 2:  # noqa: PLR2004\n        p1_df = p1_df.reset_index(drop=True)\n        p2_df = p2_df.reset_index(drop=True)\n\n    # fillna with 0 to handle cases when one time period isn't present\n    df = pd.concat([p1_df, p2_df], axis=1).fillna(0)\n\n    for col in [\n        cols.agg_customer_id,\n        cols.agg_transaction_id,\n        cols.agg_unit_spend,\n        cols.calc_spend_per_trans,\n        cols.calc_trans_per_cust,\n        cols.calc_spend_per_cust,\n    ]:\n        # Difference calculations\n        df[col + \"_\" + get_option(\"column.suffix.difference\")] = (\n            df[col + \"_\" + get_option(\"column.suffix.period_2\")] - df[col + \"_\" + get_option(\"column.suffix.period_1\")]\n        )\n\n        # Percentage change calculations\n        df[col + \"_\" + get_option(\"column.suffix.percent_difference\")] = (\n            df[col + \"_\" + get_option(\"column.suffix.difference\")]\n            / df[col + \"_\" + get_option(\"column.suffix.period_1\")]\n        )\n\n    # Calculate price elasticity\n    if cols.agg_unit_qty in df_cols:\n        df[cols.calc_price_elasticity] = (\n            (df[cols.agg_unit_qty_p2] - df[cols.agg_unit_qty_p1])\n            / ((df[cols.agg_unit_qty_p2] + df[cols.agg_unit_qty_p1]) / 2)\n        ) / (\n            (df[cols.calc_price_per_unit_p2] - df[cols.calc_price_per_unit_p1])\n            / ((df[cols.calc_price_per_unit_p2] + df[cols.calc_price_per_unit_p1]) / 2)\n        )\n\n    # Calculate frequency elasticity\n    df[cols.calc_frequency_elasticity] = (\n        (df[cols.calc_trans_per_cust_p2] - df[cols.calc_trans_per_cust_p1])\n        / ((df[cols.calc_trans_per_cust_p2] + df[cols.calc_trans_per_cust_p1]) / 2)\n    ) / (\n        (df[cols.calc_spend_per_cust_p2] - df[cols.calc_spend_per_cust_p1])\n        / ((df[cols.calc_spend_per_cust_p2] + df[cols.calc_spend_per_cust_p1]) / 2)\n    )\n\n    # Contribution calculations\n    df[cols.agg_customer_id_contrib] = (\n        df[cols.agg_unit_spend_p2]\n        - (df[cols.agg_customer_id_p1] * df[cols.calc_spend_per_cust_p2])\n        - ((df[cols.agg_customer_id_diff] * df[cols.calc_spend_per_cust_diff]) / 2)\n    )\n    df[cols.calc_spend_per_cust_contrib] = (\n        df[cols.agg_unit_spend_p2]\n        - (df[cols.calc_spend_per_cust_p1] * df[cols.agg_customer_id_p2])\n        - ((df[cols.agg_customer_id_diff] * df[cols.calc_spend_per_cust_diff]) / 2)\n    )\n\n    df[cols.calc_trans_per_cust_contrib] = (\n        (\n            df[cols.calc_spend_per_cust_p2]\n            - (df[cols.calc_trans_per_cust_p1] * df[cols.calc_spend_per_trans_p2])\n            - ((df[cols.calc_trans_per_cust_diff] * df[cols.calc_spend_per_trans_diff]) / 2)\n        )\n        * df[cols.agg_customer_id_p2]\n    ) - ((df[cols.agg_customer_id_diff] * df[cols.calc_spend_per_cust_diff]) / 4)\n\n    df[cols.calc_spend_per_trans_contrib] = (\n        (\n            df[cols.calc_spend_per_cust_p2]\n            - (df[cols.calc_spend_per_trans_p1] * df[cols.calc_trans_per_cust_p2])\n            - ((df[cols.calc_trans_per_cust_diff] * df[cols.calc_spend_per_trans_diff]) / 2)\n        )\n        * df[cols.agg_customer_id_p2]\n    ) - ((df[cols.agg_customer_id_diff] * df[cols.calc_spend_per_cust_diff]) / 4)\n\n    if cols.agg_unit_qty in df_cols:\n        # Difference calculations\n        for col in [\n            cols.agg_unit_qty,\n            cols.calc_units_per_trans,\n            cols.calc_price_per_unit,\n        ]:\n            df[col + \"_\" + get_option(\"column.suffix.difference\")] = (\n                df[col + \"_\" + get_option(\"column.suffix.period_2\")]\n                - df[col + \"_\" + get_option(\"column.suffix.period_1\")]\n            )\n\n        for col in [\n            cols.agg_unit_qty,\n            cols.calc_units_per_trans,\n            cols.calc_price_per_unit,\n        ]:\n            df[col + \"_\" + get_option(\"column.suffix.percent_difference\")] = (\n                df[col + \"_\" + get_option(\"column.suffix.difference\")]\n                / df[col + \"_\" + get_option(\"column.suffix.period_1\")]\n            )\n\n        df[cols.calc_price_per_unit_contrib] = (\n            (\n                (\n                    df[cols.calc_spend_per_trans_p2]\n                    - (df[cols.calc_price_per_unit_p1] * df[cols.calc_units_per_trans_p2])\n                    - ((df[cols.calc_units_per_trans_diff] * df[cols.calc_price_per_unit_diff]) / 2)\n                )\n                * df[cols.calc_trans_per_cust_p2]\n            )\n            - ((df[cols.calc_trans_per_cust_diff] * df[cols.calc_spend_per_trans_diff]) / 4)\n        ) * df[cols.agg_customer_id_p2] - ((df[cols.agg_customer_id_diff] * df[cols.calc_spend_per_cust_diff]) / 8)\n\n        df[cols.calc_units_per_trans_contrib] = (\n            (\n                (\n                    df[cols.calc_spend_per_trans_p2]\n                    - (df[cols.calc_units_per_trans_p1] * df[cols.calc_price_per_unit_p2])\n                    - ((df[cols.calc_units_per_trans_diff] * df[cols.calc_price_per_unit_diff]) / 2)\n                )\n                * df[cols.calc_trans_per_cust_p2]\n            )\n            - ((df[cols.calc_trans_per_cust_diff] * df[cols.calc_spend_per_trans_diff]) / 4)\n        ) * df[cols.agg_customer_id_p2] - ((df[cols.agg_customer_id_diff] * df[cols.calc_spend_per_cust_diff]) / 8)\n\n    cols = RevenueTree._get_final_col_order(include_quantity=cols.agg_unit_qty in df_cols)\n\n    return df[cols]\n</code></pre>"},{"location":"api/plots/","title":"Index Plot","text":"<p>This module provides functionality for creating index plots in retail analytics.</p> <p>Index plots are useful for comparing the performance of different categories or segments against a baseline or average, typically set at 100. The module supports customization of the plot's appearance, sorting of data, and filtering by specific groups, offering valuable insights into retail operations.</p>"},{"location":"api/plots/#pyretailscience.plots.index--features","title":"Features","text":"<ul> <li>Index Plot Creation: Visualize how categories or segments perform relative to a baseline value, typically set at 100.   Useful for comparing performance across products, regions, or customer segments.</li> <li>Flexible Sorting: Sort data by either group or value to highlight specific trends in the data.</li> <li>Data Filtering: Filter data based on specified groups to focus on specific categories or exclude unwanted data.</li> <li>Highlighting Range: Highlight specific ranges of values (e.g., performance range between 80-120) to focus on performance.</li> <li>Series Support: Optionally include a <code>series_col</code> for plotting multiple series (e.g., time periods) within the same plot.</li> <li>Graph Customization: Adjust titles, axis labels, legend titles, and styling to match the specific context of the analysis.</li> </ul>"},{"location":"api/plots/#pyretailscience.plots.index--use-cases","title":"Use Cases","text":"<ul> <li>Retail Performance Comparison: Compare product or regional performance to the company average or baseline using an index plot.</li> <li>Customer Segment Analysis: Evaluate customer segment behavior against overall performance, helping identify high-performing segments.</li> <li>Operational Insights: Identify areas of concern or opportunity by comparing store, region, or product performance against the baseline.</li> <li>Visualizing Retail Strategy: Support decision-making by visualizing which categories or products overperform or underperform relative to a baseline.</li> </ul>"},{"location":"api/plots/#pyretailscience.plots.index--limitations-and-handling-of-data","title":"Limitations and Handling of Data","text":"<ul> <li>Data Grouping and Aggregation: Supports aggregation functions such as sum, average, etc., for calculating the index.</li> <li>Sorting: Sorting can be applied by group or value, allowing analysts to focus on specific trends. If <code>series_col</code> is provided, sorting by <code>group</code> is applied.</li> <li>Group Filtering: Users can exclude or include specific groups for focused analysis, with error handling to ensure conflicting options are not used simultaneously.</li> </ul>"},{"location":"api/plots/#pyretailscience.plots.index--functionality-details","title":"Functionality Details","text":"<ul> <li>plot(): Generates the index plot, which can be customized with multiple options such as sorting, filtering, and styling.</li> <li>get_indexes(): Helper function for calculating the index of the value column for a given subset of the dataframe based on filters and aggregation.</li> </ul>"},{"location":"api/plots/#pyretailscience.plots.index.filter_by_groups","title":"<code>filter_by_groups(df, group_col, exclude_groups=None, include_only_groups=None)</code>","text":"<p>Filter dataframe by groups.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to filter.</p> required <code>group_col</code> <code>str</code> <p>The column name for grouping.</p> required <code>exclude_groups</code> <code>list[any]</code> <p>Groups to exclude. Defaults to None.</p> <code>None</code> <code>include_only_groups</code> <code>list[any]</code> <p>Groups to include. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>pyretailscience/plots/index.py</code> <pre><code>def filter_by_groups(\n    df: pd.DataFrame,\n    group_col: str,\n    exclude_groups: list[any] | None = None,\n    include_only_groups: list[any] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter dataframe by groups.\n\n    Args:\n        df (pd.DataFrame): The dataframe to filter.\n        group_col (str): The column name for grouping.\n        exclude_groups (list[any], optional): Groups to exclude. Defaults to None.\n        include_only_groups (list[any], optional): Groups to include. Defaults to None.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    result_df = df.copy()\n    if exclude_groups is not None:\n        result_df = result_df[~result_df[group_col].isin(exclude_groups)]\n    if include_only_groups is not None:\n        result_df = result_df[result_df[group_col].isin(include_only_groups)]\n    return result_df\n</code></pre>"},{"location":"api/plots/#pyretailscience.plots.index.filter_by_value_thresholds","title":"<code>filter_by_value_thresholds(df, filter_above=None, filter_below=None)</code>","text":"<p>Filter dataframe by index value thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to filter.</p> required <code>filter_above</code> <code>float</code> <p>Only keep indices above this value. Defaults to None.</p> <code>None</code> <code>filter_below</code> <code>float</code> <p>Only keep indices below this value. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If filtering results in an empty dataset.</p> Source code in <code>pyretailscience/plots/index.py</code> <pre><code>def filter_by_value_thresholds(\n    df: pd.DataFrame,\n    filter_above: float | None = None,\n    filter_below: float | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter dataframe by index value thresholds.\n\n    Args:\n        df (pd.DataFrame): The dataframe to filter.\n        filter_above (float, optional): Only keep indices above this value. Defaults to None.\n        filter_below (float, optional): Only keep indices below this value. Defaults to None.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n\n    Raises:\n        ValueError: If filtering results in an empty dataset.\n    \"\"\"\n    result_df = df.copy()\n    if filter_above is not None:\n        result_df = result_df[result_df[\"index\"] &gt; filter_above]\n    if filter_below is not None:\n        result_df = result_df[result_df[\"index\"] &lt; filter_below]\n\n    # Check if filtering resulted in an empty dataframe\n    if len(result_df) == 0:\n        raise ValueError(\n            \"Filtering resulted in an empty dataset. Consider adjusting filter parameters.\",\n        )\n\n    return result_df\n</code></pre>"},{"location":"api/plots/#pyretailscience.plots.index.filter_top_bottom_n","title":"<code>filter_top_bottom_n(df, top_n=None, bottom_n=None)</code>","text":"<p>Filter dataframe to include only top N and/or bottom N rows by index value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to filter.</p> required <code>top_n</code> <code>int</code> <p>Number of top items to include. Defaults to None.</p> <code>None</code> <code>bottom_n</code> <code>int</code> <p>Number of bottom items to include. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If top_n or bottom_n exceed the available groups.</p> <code>ValueError</code> <p>If the sum of top_n and bottom_n exceeds the total number of groups.</p> <code>ValueError</code> <p>If filtering results in an empty dataset.</p> Source code in <code>pyretailscience/plots/index.py</code> <pre><code>def filter_top_bottom_n(df: pd.DataFrame, top_n: int | None = None, bottom_n: int | None = None) -&gt; pd.DataFrame:\n    \"\"\"Filter dataframe to include only top N and/or bottom N rows by index value.\n\n    Args:\n        df (pd.DataFrame): The dataframe to filter.\n        top_n (int, optional): Number of top items to include. Defaults to None.\n        bottom_n (int, optional): Number of bottom items to include. Defaults to None.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n\n    Raises:\n        ValueError: If top_n or bottom_n exceed the available groups.\n        ValueError: If the sum of top_n and bottom_n exceeds the total number of groups.\n        ValueError: If filtering results in an empty dataset.\n    \"\"\"\n    top_n = None if top_n == 0 else top_n\n    bottom_n = None if bottom_n == 0 else bottom_n\n\n    if (top_n is None and bottom_n is None) or len(df) == 0:\n        return df\n\n    # Check if top_n or bottom_n exceed the dataframe length\n    df_length = len(df)\n    if top_n is not None and top_n &gt; df_length:\n        error_msg = f\"top_n ({top_n}) cannot exceed the number of available groups ({df_length}).\"\n        raise ValueError(error_msg)\n    if bottom_n is not None and bottom_n &gt; df_length:\n        error_msg = f\"bottom_n ({bottom_n}) cannot exceed the number of available groups ({df_length}).\"\n        raise ValueError(error_msg)\n\n    # Check if top_n + bottom_n exceeds total groups\n    if top_n is not None and bottom_n is not None and top_n + bottom_n &gt; df_length:\n        error_msg = f\"The sum of top_n ({top_n}) and bottom_n ({bottom_n}) cannot exceed the total number of groups ({df_length}).\"\n        raise ValueError(error_msg)\n\n    # Create a temporary dataframe sorted by index value\n    temp_df = df.copy().sort_values(by=\"index\", ascending=False)\n\n    selected_rows = pd.DataFrame()\n    if top_n is not None:\n        selected_rows = pd.concat([selected_rows, temp_df.head(top_n)])\n    if bottom_n is not None:\n        selected_rows = pd.concat([selected_rows, temp_df.tail(bottom_n)])\n\n    return selected_rows\n</code></pre>"},{"location":"api/plots/#pyretailscience.plots.index.get_indexes","title":"<code>get_indexes(df, value_to_index, index_col, value_col, group_col, index_subgroup_col=None, agg_func='sum', offset=0)</code>","text":"<p>Calculates the index of the value_col using Ibis for efficient computation at scale.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame | Table</code> <p>The dataframe or Ibis table to calculate the index on. Can be a pandas dataframe or an Ibis table.</p> required <code>value_to_index</code> <code>str</code> <p>The baseline category or value to index against (e.g., \"A\").</p> required <code>index_col</code> <code>str</code> <p>The column to calculate the index on (e.g., \"category\").</p> required <code>value_col</code> <code>str</code> <p>The column to calculate the index on (e.g., \"sales\").</p> required <code>group_col</code> <code>str</code> <p>The column to group the data by (e.g., \"region\").</p> required <code>index_subgroup_col</code> <code>str</code> <p>The column to subgroup the index by (e.g., \"store_type\"). Defaults to None.</p> <code>None</code> <code>agg_func</code> <code>str</code> <p>The aggregation function to apply to the <code>value_col</code>. Valid options are \"sum\", \"mean\", \"max\", \"min\", or \"nunique\". Defaults to \"sum\".</p> <code>'sum'</code> <code>offset</code> <code>int</code> <p>The offset value to subtract from the index. This allows for adjustments to the index values. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The calculated index values with grouping columns.</p> Source code in <code>pyretailscience/plots/index.py</code> <pre><code>def get_indexes(\n    df: pd.DataFrame | ibis.Table,\n    value_to_index: str,\n    index_col: str,\n    value_col: str,\n    group_col: str,\n    index_subgroup_col: str | None = None,\n    agg_func: str = \"sum\",\n    offset: int = 0,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculates the index of the value_col using Ibis for efficient computation at scale.\n\n    Args:\n        df (pd.DataFrame | ibis.Table): The dataframe or Ibis table to calculate the index on. Can be a pandas\n            dataframe or an Ibis table.\n        value_to_index (str): The baseline category or value to index against (e.g., \"A\").\n        index_col (str): The column to calculate the index on (e.g., \"category\").\n        value_col (str): The column to calculate the index on (e.g., \"sales\").\n        group_col (str): The column to group the data by (e.g., \"region\").\n        index_subgroup_col (str, optional): The column to subgroup the index by (e.g., \"store_type\"). Defaults to None.\n        agg_func (str, optional): The aggregation function to apply to the `value_col`. Valid options are \"sum\", \"mean\",\n            \"max\", \"min\", or \"nunique\". Defaults to \"sum\".\n        offset (int, optional): The offset value to subtract from the index. This allows for adjustments to the index\n            values. Defaults to 0.\n\n    Returns:\n        pd.DataFrame: The calculated index values with grouping columns.\n    \"\"\"\n    if isinstance(df, pd.DataFrame):\n        df = df.copy()\n        table = ibis.memtable(df)\n    else:\n        table = df\n\n    agg_func = agg_func.lower()\n    if agg_func not in {\"sum\", \"mean\", \"max\", \"min\", \"nunique\"}:\n        raise ValueError(\"Unsupported aggregation function.\")\n\n    agg_fn = lambda x: getattr(x, agg_func)()\n\n    group_cols = [group_col] if index_subgroup_col is None else [index_subgroup_col, group_col]\n\n    overall_agg = table.group_by(group_cols).aggregate(value=agg_fn(table[value_col]))\n\n    if index_subgroup_col is None:\n        overall_total = overall_agg.value.sum().execute()\n        overall_props = overall_agg.mutate(proportion_overall=overall_agg.value / overall_total)\n    else:\n        overall_total = overall_agg.group_by(index_subgroup_col).aggregate(total=lambda t: t.value.sum())\n        overall_props = (\n            overall_agg.join(overall_total, index_subgroup_col)\n            .mutate(proportion_overall=lambda t: t.value / t.total)\n            .drop(\"total\")\n        )\n\n    table = table.filter(table[index_col] == value_to_index)\n    subset_agg = table.group_by(group_cols).aggregate(value=agg_fn(table[value_col]))\n\n    if index_subgroup_col is None:\n        subset_total = subset_agg.value.sum().name(\"total\")\n        subset_props = subset_agg.mutate(proportion=subset_agg.value / subset_total)\n    else:\n        subset_total = subset_agg.group_by(index_subgroup_col).aggregate(total=lambda t: t.value.sum())\n        subset_props = (\n            subset_agg.join(subset_total, index_subgroup_col)\n            .filter(lambda t: t.total != 0)\n            .mutate(proportion=lambda t: t.value / t.total)\n            .drop(\"total\")\n        )\n\n    result = (\n        subset_props.join(overall_props, group_cols)\n        .mutate(\n            index=lambda t: (t.proportion / t.proportion_overall * 100) - offset,\n        )\n        .order_by(group_cols)\n    )\n\n    return result[[*group_cols, \"index\"]].execute()\n</code></pre>"},{"location":"api/plots/#pyretailscience.plots.index.plot","title":"<code>plot(df, value_col, group_col, index_col, value_to_index, agg_func='sum', series_col=None, title=None, x_label='Index', y_label=None, legend_title=None, highlight_range='default', sort_by='group', sort_order='ascending', ax=None, source_text=None, exclude_groups=None, include_only_groups=None, drop_na=False, top_n=None, bottom_n=None, filter_above=None, filter_below=None, **kwargs)</code>","text":"<p>Creates an index plot.</p> <p>Index plots are visual tools used in retail analytics to compare different categories or segments against a baseline or average value, typically set at 100. Index plots allow analysts to:</p> <ul> <li>Quickly identify which categories over- or underperform relative to the average</li> <li>Compare performance across diverse categories on a standardized scale</li> <li>Highlight areas of opportunity or concern in retail operations</li> <li>Easily communicate relative performance to stakeholders without revealing sensitive absolute numbers</li> </ul> <p>In retail contexts, index plots are valuable for:</p> <ul> <li>Comparing sales performance across product categories</li> <li>Analyzing customer segment behavior against the overall average</li> <li>Evaluating store or regional performance relative to company-wide metrics</li> <li>Identifying high-potential areas for growth or investment</li> </ul> <p>By normalizing data to an index, these plots facilitate meaningful comparisons and help focus attention on significant deviations from expected performance, supporting more informed decision-making in retail strategy and operations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to plot.</p> required <code>value_col</code> <code>str</code> <p>The column to plot.</p> required <code>group_col</code> <code>str</code> <p>The column to group the data by.</p> required <code>index_col</code> <code>str</code> <p>The column to calculate the index on (e.g., \"category\").</p> required <code>value_to_index</code> <code>str</code> <p>The baseline category or value to index against (e.g., \"A\").</p> required <code>agg_func</code> <code>str</code> <p>The aggregation function to apply to the value_col. Defaults to \"sum\".</p> <code>'sum'</code> <code>series_col</code> <code>str</code> <p>The column to use as the series. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to None. When None the title is set to <code>f\"{value_col.title()} by {group_col.title()}\"</code></p> <code>None</code> <code>x_label</code> <code>str</code> <p>The x-axis label. Defaults to \"Index\".</p> <code>'Index'</code> <code>y_label</code> <code>str</code> <p>The y-axis label. Defaults to None. When None the y-axis label is set to the title case of <code>group_col</code></p> <code>None</code> <code>legend_title</code> <code>str</code> <p>The title of the legend. Defaults to None. When None the legend title is set to the title case of <code>group_col</code></p> <code>None</code> <code>highlight_range</code> <code>Literal['default'] | tuple[float, float] | None</code> <p>The range to highlight. Defaults to \"default\". When \"default\" the range is set to (80, 120). When None no range is highlighted.</p> <code>'default'</code> <code>sort_by</code> <code>Literal['group', 'value'] | None</code> <p>The column to sort by. Defaults to \"group\". When None the data is not sorted. When \"group\" the data is sorted by group_col. When \"value\" the data is sorted by the value_col. When series_col is not None this option is ignored.</p> <code>'group'</code> <code>sort_order</code> <code>Literal['ascending', 'descending']</code> <p>The order to sort the data. Defaults to \"ascending\".</p> <code>'ascending'</code> <code>ax</code> <code>Axes</code> <p>The matplotlib axes object to plot on. Defaults to None.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot. Defaults to None.</p> <code>None</code> <code>exclude_groups</code> <code>list[any]</code> <p>The groups to exclude from the plot. Defaults to None.</p> <code>None</code> <code>include_only_groups</code> <code>list[any]</code> <p>The groups to include in the plot. Defaults to None. When None all groups are included. When not None only the groups in the list are included. Can not be used with exclude_groups.</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop NA index values. Defaults to False.</p> <code>False</code> <code>top_n</code> <code>int</code> <p>Display only the top N indexes by value. Only applicable when series_col is None. Defaults to None.</p> <code>None</code> <code>bottom_n</code> <code>int</code> <p>Display only the bottom N indexes by value. Only applicable when series_col is None. Defaults to None.</p> <code>None</code> <code>filter_above</code> <code>float</code> <p>Only display indexes above this value. Only applicable when series_col is None. Defaults to None.</p> <code>None</code> <code>filter_below</code> <code>float</code> <p>Only display indexes below this value. Only applicable when series_col is None. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the Pandas plot function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The matplotlib axes object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sort_by is not either \"group\" or \"value\" or None.</p> <code>ValueError</code> <p>If sort_order is not either \"ascending\" or \"descending\".</p> <code>ValueError</code> <p>If exclude_groups and include_only_groups are used together.</p> <code>ValueError</code> <p>If both top_n and bottom_n are provided but their sum exceeds the total number of groups.</p> <code>ValueError</code> <p>If top_n or bottom_n exceed the number of available groups.</p> <code>ValueError</code> <p>If top_n, bottom_n, filter_above, or filter_below are used when series_col is provided.</p> <code>ValueError</code> <p>If filtering results in an empty dataset.</p> Source code in <code>pyretailscience/plots/index.py</code> <pre><code>def plot(  # noqa: C901, PLR0913\n    df: pd.DataFrame,\n    value_col: str,\n    group_col: str,\n    index_col: str,\n    value_to_index: str,\n    agg_func: str = \"sum\",\n    series_col: str | None = None,\n    title: str | None = None,\n    x_label: str = \"Index\",\n    y_label: str | None = None,\n    legend_title: str | None = None,\n    highlight_range: Literal[\"default\"] | tuple[float, float] | None = \"default\",\n    sort_by: Literal[\"group\", \"value\"] | None = \"group\",\n    sort_order: Literal[\"ascending\", \"descending\"] = \"ascending\",\n    ax: Axes | None = None,\n    source_text: str | None = None,\n    exclude_groups: list[any] | None = None,\n    include_only_groups: list[any] | None = None,\n    drop_na: bool = False,\n    top_n: int | None = None,\n    bottom_n: int | None = None,\n    filter_above: float | None = None,\n    filter_below: float | None = None,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Creates an index plot.\n\n    Index plots are visual tools used in retail analytics to compare different categories or segments against a\n    baseline or average value, typically set at 100. Index plots allow analysts to:\n\n    - Quickly identify which categories over- or underperform relative to the average\n    - Compare performance across diverse categories on a standardized scale\n    - Highlight areas of opportunity or concern in retail operations\n    - Easily communicate relative performance to stakeholders without revealing sensitive absolute numbers\n\n    In retail contexts, index plots are valuable for:\n\n    - Comparing sales performance across product categories\n    - Analyzing customer segment behavior against the overall average\n    - Evaluating store or regional performance relative to company-wide metrics\n    - Identifying high-potential areas for growth or investment\n\n    By normalizing data to an index, these plots facilitate meaningful comparisons and help focus attention on\n    significant deviations from expected performance, supporting more informed decision-making in retail strategy and\n    operations.\n\n    Args:\n        df (pd.DataFrame): The dataframe to plot.\n        value_col (str): The column to plot.\n        group_col (str): The column to group the data by.\n        index_col (str): The column to calculate the index on (e.g., \"category\").\n        value_to_index (str): The baseline category or value to index against (e.g., \"A\").\n        agg_func (str, optional): The aggregation function to apply to the value_col. Defaults to \"sum\".\n        series_col (str, optional): The column to use as the series. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to None. When None the title is set to\n            `f\"{value_col.title()} by {group_col.title()}\"`\n        x_label (str, optional): The x-axis label. Defaults to \"Index\".\n        y_label (str, optional): The y-axis label. Defaults to None. When None the y-axis label is set to the title\n            case of `group_col`\n        legend_title (str, optional): The title of the legend. Defaults to None. When None the legend title is set to\n            the title case of `group_col`\n        highlight_range (Literal[\"default\"] | tuple[float, float] | None, optional): The range to highlight. Defaults\n            to \"default\". When \"default\" the range is set to (80, 120). When None no range is highlighted.\n        sort_by (Literal[\"group\", \"value\"] | None, optional): The column to sort by. Defaults to \"group\". When None the\n            data is not sorted. When \"group\" the data is sorted by group_col. When \"value\" the data is sorted by\n            the value_col. When series_col is not None this option is ignored.\n        sort_order (Literal[\"ascending\", \"descending\"], optional): The order to sort the data. Defaults to \"ascending\".\n        ax (Axes, optional): The matplotlib axes object to plot on. Defaults to None.\n        source_text (str, optional): The source text to add to the plot. Defaults to None.\n        exclude_groups (list[any], optional): The groups to exclude from the plot. Defaults to None.\n        include_only_groups (list[any], optional): The groups to include in the plot. Defaults to None. When None all\n            groups are included. When not None only the groups in the list are included. Can not be used with\n            exclude_groups.\n        drop_na (bool, optional): Whether to drop NA index values. Defaults to False.\n        top_n (int, optional): Display only the top N indexes by value. Only applicable when series_col is None. Defaults to None.\n        bottom_n (int, optional): Display only the bottom N indexes by value. Only applicable when series_col is None. Defaults to None.\n        filter_above (float, optional): Only display indexes above this value. Only applicable when series_col is None. Defaults to None.\n        filter_below (float, optional): Only display indexes below this value. Only applicable when series_col is None. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the Pandas plot function.\n\n    Returns:\n        SubplotBase: The matplotlib axes object.\n\n    Raises:\n        ValueError: If sort_by is not either \"group\" or \"value\" or None.\n        ValueError: If sort_order is not either \"ascending\" or \"descending\".\n        ValueError: If exclude_groups and include_only_groups are used together.\n        ValueError: If both top_n and bottom_n are provided but their sum exceeds the total number of groups.\n        ValueError: If top_n or bottom_n exceed the number of available groups.\n        ValueError: If top_n, bottom_n, filter_above, or filter_below are used when series_col is provided.\n        ValueError: If filtering results in an empty dataset.\n    \"\"\"\n    if sort_by not in [\"group\", \"value\", None]:\n        raise ValueError(\"sort_by must be either 'group' or 'value' or None\")\n    if series_col is not None and sort_by == \"value\":\n        raise ValueError(\"sort_by cannot be 'value' when series_col is provided.\")\n    if sort_order not in [\"ascending\", \"descending\"]:\n        raise ValueError(\"sort_order must be either 'ascending' or 'descending'\")\n    if exclude_groups is not None and include_only_groups is not None:\n        raise ValueError(\"exclude_groups and include_only_groups cannot be used together.\")\n    if series_col is not None and (\n        top_n is not None or bottom_n is not None or filter_above is not None or filter_below is not None\n    ):\n        raise ValueError(\n            \"top_n, bottom_n, filter_above, and filter_below cannot be used when series_col is provided.\",\n        )\n\n    index_df = get_indexes(\n        df=df,\n        index_col=index_col,\n        value_to_index=value_to_index,\n        index_subgroup_col=series_col,\n        value_col=value_col,\n        agg_func=agg_func,\n        offset=100,\n        group_col=group_col,\n    )\n\n    if drop_na:\n        index_df = index_df.dropna(subset=[\"index\"])\n\n    index_df = filter_by_groups(\n        df=index_df,\n        group_col=group_col,\n        exclude_groups=exclude_groups,\n        include_only_groups=include_only_groups,\n    )\n\n    if series_col is None:\n        index_df = filter_by_value_thresholds(\n            df=index_df,\n            filter_above=filter_above,\n            filter_below=filter_below,\n        )\n\n        colors = COLORS[\"green\"][500]\n        show_legend = False\n        index_df = index_df[[group_col, \"index\"]].set_index(group_col)\n        if sort_by in [\"group\", \"value\"]:\n            index_df = index_df.sort_values(\n                by=group_col if sort_by == \"group\" else \"index\",\n                ascending=sort_order == \"ascending\",\n            )\n\n        index_df = filter_top_bottom_n(\n            df=index_df,\n            top_n=top_n,\n            bottom_n=bottom_n,\n        )\n\n    else:\n        show_legend = True\n        colors = get_linear_cmap(\"green\")(np.linspace(0, 1, df[series_col].nunique()))\n\n        if sort_by == \"group\":\n            index_df = index_df.sort_values(by=[group_col, series_col], ascending=sort_order == \"ascending\")\n\n        index_df = index_df.pivot_table(\n            index=group_col,\n            columns=series_col,\n            values=\"index\",\n            sort=False,\n        )\n\n    ax = index_df.plot.barh(\n        left=100,\n        legend=show_legend,\n        ax=ax,\n        color=colors,\n        width=GraphStyles.DEFAULT_BAR_WIDTH,\n        zorder=2,\n        **kwargs,\n    )\n\n    ax.axvline(100, color=\"black\", linewidth=1, alpha=0.5)\n    if highlight_range is not None:\n        highlight_range = (80, 120) if highlight_range == \"default\" else highlight_range\n        ax.axvline(highlight_range[0], color=\"black\", linewidth=0.25, alpha=0.1, zorder=-1)\n        ax.axvline(highlight_range[1], color=\"black\", linewidth=0.25, alpha=0.1, zorder=-1)\n        ax.axvspan(highlight_range[0], highlight_range[1], color=\"black\", alpha=0.1, zorder=-1)\n\n    default_title = f\"{value_col.title()} by {group_col.title()}\"\n\n    ax = gu.standard_graph_styles(\n        ax=ax,\n        title=gu.not_none(title, default_title),\n        x_label=gu.not_none(x_label, \"Index\"),\n        y_label=gu.not_none(y_label, group_col.title()),\n        legend_title=legend_title,\n        show_legend=show_legend,\n    )\n\n    if source_text is not None:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    gu.standard_tick_styles(ax)\n\n    return ax\n</code></pre>"},{"location":"api/plots/area/","title":"Area Plot","text":"<p>This module provides functionality for creating area plots from pandas DataFrames.</p> <p>It is designed to visualize data distributions over time or across categories using filled area charts. These plots help highlight trends and comparisons between different groups by stacking or overlaying areas.</p> <p>While this module supports datetime values on the x-axis, the plots.time_area module is better suited for explicitly time-based visualizations, offering features like resampling and time-based aggregation.</p>"},{"location":"api/plots/area/#pyretailscience.plots.area--core-features","title":"Core Features","text":"<ul> <li>Flexible X-Axis Handling: Uses an index or a specified x-axis column (<code>x_col</code>) for plotting.</li> <li>Multiple Area Support: Allows plotting multiple columns (<code>value_col</code>) or groups (<code>group_col</code>).</li> <li>Dynamic Color Mapping: Automatically selects a colormap based on the number of groups.</li> <li>Legend Customization: Supports custom legend titles and the option to move the legend outside the plot.</li> <li>Source Text: Provides an option to add source attribution to the plot.</li> </ul>"},{"location":"api/plots/area/#pyretailscience.plots.area--use-cases","title":"Use Cases","text":"<ul> <li>Time Series Visualization: Show trends in a metric over time (e.g., revenue by month).</li> <li>Stacked Area Charts: Compare contributions of different groups over time.</li> <li>Category-Based Area Plots: Visualize distributions of data across categories.</li> </ul>"},{"location":"api/plots/area/#pyretailscience.plots.area--limitations-and-warnings","title":"Limitations and Warnings","text":"<ul> <li>Handling of Datetime Data: If a datetime column is passed as <code>x_col</code>, a warning suggests using   the plots.time_area module for better handling.</li> <li>Pre-Aggregated Data Required: The module does not perform data aggregation; data should be pre-aggregated   before being passed to the function.</li> </ul>"},{"location":"api/plots/area/#pyretailscience.plots.area.plot","title":"<code>plot(df, value_col, x_label=None, y_label=None, title=None, x_col=None, group_col=None, ax=None, source_text=None, legend_title=None, move_legend_outside=False, **kwargs)</code>","text":"<p>Plots an area chart for the given <code>value_col</code> over <code>x_col</code> or index, with optional grouping by <code>group_col</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to plot.</p> required <code>value_col</code> <code>str or list of str</code> <p>The column(s) to plot.</p> required <code>x_label</code> <code>str</code> <p>The x-axis label.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The column to be used as the x-axis. If None, the index is used.</p> <code>None</code> <code>group_col</code> <code>str</code> <p>The column used to define different areas in the plot.</p> <code>None</code> <code>legend_title</code> <code>str</code> <p>The title of the legend.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Matplotlib axes object to plot on.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot.</p> <code>None</code> <code>move_legend_outside</code> <code>bool</code> <p>Move the legend outside the plot.</p> <code>False</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments for Pandas' <code>plot</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The matplotlib axes object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>value_col</code> is a list and <code>group_col</code> is provided (which causes ambiguity in plotting).</p> Source code in <code>pyretailscience/plots/area.py</code> <pre><code>def plot(\n    df: pd.DataFrame,\n    value_col: str | list[str],\n    x_label: str | None = None,\n    y_label: str | None = None,\n    title: str | None = None,\n    x_col: str | None = None,\n    group_col: str | None = None,\n    ax: Axes | None = None,\n    source_text: str | None = None,\n    legend_title: str | None = None,\n    move_legend_outside: bool = False,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plots an area chart for the given `value_col` over `x_col` or index, with optional grouping by `group_col`.\n\n    Args:\n        df (pd.DataFrame): The dataframe to plot.\n        value_col (str or list of str): The column(s) to plot.\n        x_label (str, optional): The x-axis label.\n        y_label (str, optional): The y-axis label.\n        title (str, optional): The title of the plot.\n        x_col (str, optional): The column to be used as the x-axis. If None, the index is used.\n        group_col (str, optional): The column used to define different areas in the plot.\n        legend_title (str, optional): The title of the legend.\n        ax (Axes, optional): Matplotlib axes object to plot on.\n        source_text (str, optional): The source text to add to the plot.\n        move_legend_outside (bool, optional): Move the legend outside the plot.\n        **kwargs: Additional keyword arguments for Pandas' `plot` function.\n\n    Returns:\n        SubplotBase: The matplotlib axes object.\n\n    Raises:\n        ValueError: If `value_col` is a list and `group_col` is provided (which causes ambiguity in plotting).\n    \"\"\"\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n\n    if isinstance(value_col, list) and group_col:\n        raise ValueError(\"Cannot use both a list for `value_col` and a `group_col`. Choose one.\")\n\n    if group_col is None:\n        pivot_df = df.set_index(x_col if x_col is not None else df.index)[\n            [value_col] if isinstance(value_col, str) else value_col\n        ]\n    else:\n        pivot_df = df.pivot(index=x_col if x_col is not None else None, columns=group_col, values=value_col)\n\n    is_multi_area = (group_col is not None) or (isinstance(value_col, list) and len(value_col) &gt; 1)\n\n    color_gen_threshold = 4\n    num_colors = len(pivot_df.columns) if is_multi_area else 1\n    color_gen = get_single_color_cmap() if num_colors &lt; color_gen_threshold else get_multi_color_cmap()\n    colors = [next(color_gen) for _ in range(num_colors)]\n    alpha = kwargs.pop(\"alpha\", 0.7)\n    ax = pivot_df.plot(\n        ax=ax,\n        kind=\"area\",\n        alpha=alpha,\n        color=colors,\n        legend=is_multi_area,\n        **kwargs,\n    )\n\n    ax = gu.standard_graph_styles(\n        ax=ax,\n        title=title,\n        x_label=x_label,\n        y_label=y_label,\n        legend_title=legend_title,\n        move_legend_outside=move_legend_outside,\n    )\n\n    if source_text is not None:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    return gu.standard_tick_styles(ax)\n</code></pre>"},{"location":"api/plots/bar/","title":"Bar Plot","text":"<p>This module provides flexible functionality for creating bar plots from pandas DataFrames or Series.</p> <p>It allows you to create bar plots with optional grouping, sorting, orientation, and data labels. The module supports both single and grouped bar plots, where grouped bars are created by providing a <code>x_col</code>, which defines the x-axis labels or categories.</p>"},{"location":"api/plots/bar/#pyretailscience.plots.bar--features","title":"Features","text":"<ul> <li>Single or Grouped Bar Plots: Plot one or more value columns (<code>value_col</code>) as bars. The <code>x_col</code> is used to define categories or groups on the x-axis (e.g., products, categories, or regions). Grouped bars can be created by specifying both <code>value_col</code> (list of columns) and <code>x_col</code>.</li> <li>Sorting and Orientation: Customize the sorting of bars (ascending or descending) and choose between vertical (<code>\"v\"</code>, <code>\"vertical\"</code>) or horizontal (<code>\"h\"</code>, <code>\"horizontal\"</code>) bar orientations.</li> <li>Data Labels: Add data labels to bars, with options to show absolute values or percentages.</li> <li>Hatching Patterns: Apply hatch patterns to the bars for enhanced visual differentiation.</li> <li>Legend Customization: Move the legend outside the plot for better visibility, especially when dealing with grouped bars or multiple value columns.</li> </ul>"},{"location":"api/plots/bar/#pyretailscience.plots.bar--use-cases","title":"Use Cases","text":"<ul> <li>Sales and Revenue Analysis: Visualize sales or revenue across different products or categories by creating grouped   bar plots (e.g., revenue across quarters or regions). The <code>x_col</code> will define the products or categories displayed on   the x-axis.</li> <li>Comparative Analysis: Compare multiple metrics simultaneously by plotting grouped bars. For instance, you can   compare product sales for different periods side by side, with <code>x_col</code> defining the x-axis categories.</li> <li>Distribution Analysis: Visualize the distribution of categorical data (e.g., product sales) across different   categories, where <code>x_col</code> defines the x-axis labels.</li> </ul>"},{"location":"api/plots/bar/#pyretailscience.plots.bar--limitations-and-handling-of-data","title":"Limitations and Handling of Data","text":"<ul> <li>Series Support: The module can also handle pandas Series, though <code>x_col</code> cannot be provided when plotting a Series.   In this case, the index of the Series will define the x-axis labels.</li> </ul>"},{"location":"api/plots/bar/#pyretailscience.plots.bar.plot","title":"<code>plot(df, value_col=None, x_col=None, title=None, x_label=None, y_label=None, legend_title=None, ax=None, source_text=None, move_legend_outside=False, orientation='vertical', sort_order=None, data_label_format=None, use_hatch=False, num_digits=3, **kwargs)</code>","text":"<p>Creates a customizable bar plot from a DataFrame or Series with optional features like sorting, orientation, and adding data labels. Grouped bars can be created with the use of a grouping column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame | Series</code> <p>The input DataFrame or Series containing the data to be plotted.</p> required <code>value_col</code> <code>str | list[str]</code> <p>The column(s) containing values to plot as bars. Multiple value columns                                    create grouped bars. Defaults to None.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The column to group data by, used for grouping bars. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>x_label</code> <code>str</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>legend_title</code> <code>str</code> <p>The title for the legend. Defaults to None.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>The Matplotlib Axes object to plot on. Defaults to None.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>Text to be displayed as a source at the bottom of the plot. Defaults to None.</p> <code>None</code> <code>move_legend_outside</code> <code>bool</code> <p>Whether to move the legend outside the plot area. Defaults to False.</p> <code>False</code> <code>orientation</code> <code>Literal['horizontal', 'h', 'vertical', 'v']</code> <p>Orientation of the bars. Can be                                                                  \"horizontal\", \"h\", \"vertical\", or \"v\".                                                                  Defaults to \"vertical\".</p> <code>'vertical'</code> <code>sort_order</code> <code>Literal['ascending', 'descending'] | None</code> <p>Sorting order for the bars. Can be                                                               \"ascending\" or \"descending\". Defaults to None.</p> <code>None</code> <code>data_label_format</code> <code>Literal['absolute', 'percentage'] | None</code> <p>Format for displaying data labels.                                                                     \"absolute\" shows raw values,                                                                     \"percentage\" shows percentage.                                                                     Defaults to None.</p> <code>None</code> <code>use_hatch</code> <code>bool</code> <p>Whether to apply hatch patterns to the bars. Defaults to False.</p> <code>False</code> <code>num_digits</code> <code>int</code> <p>The number of digits to display in the data labels. Defaults to 3.</p> <code>3</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments for the Pandas <code>plot</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The Matplotlib Axes object with the generated plot.</p> Source code in <code>pyretailscience/plots/bar.py</code> <pre><code>def plot(\n    df: pd.DataFrame | pd.Series,\n    value_col: str | list[str] | None = None,\n    x_col: str | None = None,\n    title: str | None = None,\n    x_label: str | None = None,\n    y_label: str | None = None,\n    legend_title: str | None = None,\n    ax: Axes | None = None,\n    source_text: str | None = None,\n    move_legend_outside: bool = False,\n    orientation: Literal[\"horizontal\", \"h\", \"vertical\", \"v\"] = \"vertical\",\n    sort_order: Literal[\"ascending\", \"descending\"] | None = None,\n    data_label_format: Literal[\"absolute\", \"percentage_by_bar_group\", \"percentage_by_series\"] | None = None,\n    use_hatch: bool = False,\n    num_digits: int = 3,\n    **kwargs: dict[str, Any],\n) -&gt; SubplotBase:\n    \"\"\"Creates a customizable bar plot from a DataFrame or Series with optional features like sorting, orientation, and adding data labels. Grouped bars can be created with the use of a grouping column.\n\n    Args:\n        df (pd.DataFrame | pd.Series): The input DataFrame or Series containing the data to be plotted.\n        value_col (str | list[str], optional): The column(s) containing values to plot as bars. Multiple value columns\n                                               create grouped bars. Defaults to None.\n        x_col (str, optional): The column to group data by, used for grouping bars. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to None.\n        x_label (str, optional): The label for the x-axis. Defaults to None.\n        y_label (str, optional): The label for the y-axis. Defaults to None.\n        legend_title (str, optional): The title for the legend. Defaults to None.\n        ax (Axes, optional): The Matplotlib Axes object to plot on. Defaults to None.\n        source_text (str, optional): Text to be displayed as a source at the bottom of the plot. Defaults to None.\n        move_legend_outside (bool, optional): Whether to move the legend outside the plot area. Defaults to False.\n        orientation (Literal[\"horizontal\", \"h\", \"vertical\", \"v\"], optional): Orientation of the bars. Can be\n                                                                             \"horizontal\", \"h\", \"vertical\", or \"v\".\n                                                                             Defaults to \"vertical\".\n        sort_order (Literal[\"ascending\", \"descending\"] | None, optional): Sorting order for the bars. Can be\n                                                                          \"ascending\" or \"descending\". Defaults to None.\n        data_label_format (Literal[\"absolute\", \"percentage\"] | None, optional): Format for displaying data labels.\n                                                                                \"absolute\" shows raw values,\n                                                                                \"percentage\" shows percentage.\n                                                                                Defaults to None.\n        use_hatch (bool, optional): Whether to apply hatch patterns to the bars. Defaults to False.\n        num_digits (int, optional): The number of digits to display in the data labels. Defaults to 3.\n        **kwargs (dict[str, any]): Additional keyword arguments for the Pandas `plot` function.\n\n    Returns:\n        SubplotBase: The Matplotlib Axes object with the generated plot.\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"Cannot plot with empty DataFrame\")\n\n    # Check if x_col exists in the DataFrame, if provided\n    if x_col is not None and x_col not in df.columns:\n        msg = f\"x_col '{x_col}' not found in DataFrame\"\n        raise KeyError(msg)\n\n    valid_orientations = [\"horizontal\", \"h\", \"vertical\", \"v\"]\n    if orientation not in valid_orientations:\n        error_msg = f\"Invalid orientation: {orientation}. Expected one of {valid_orientations}\"\n        raise ValueError(error_msg)\n\n    # Validate the sort_order value\n    valid_sort_orders = [\"ascending\", \"descending\", None]\n    if sort_order not in valid_sort_orders:\n        error_msg = f\"Invalid sort_order: {sort_order}. Expected one of {valid_sort_orders}\"\n        raise ValueError(error_msg)\n\n    # Validate the data_label_format value\n    valid_data_label_formats = [\"absolute\", \"percentage_by_bar_group\", \"percentage_by_series\", None]\n    if data_label_format not in valid_data_label_formats:\n        error_msg = f\"Invalid data_label_format: {data_label_format}. Expected one of {valid_data_label_formats}\"\n        raise ValueError(error_msg)\n\n    width = kwargs.pop(\"width\", 0.8)\n\n    value_col = [value_col] if isinstance(value_col, str) else ([\"Value\"] if value_col is None else value_col)\n\n    df = df.to_frame(name=value_col[0]) if isinstance(df, pd.Series) else df\n\n    if data_label_format in [\"percentage_by_bar_group\", \"percentage_by_series\"] and (df[value_col] &lt; 0).any().any():\n        warnings.warn(\n            f\"Negative values detected in {value_col}. This may lead to unexpected behavior in terms of the data \"\n            f\"label format '{data_label_format}'.\",\n            UserWarning,\n            stacklevel=2,\n        )\n\n    df = df.sort_values(by=value_col[0], ascending=sort_order == \"ascending\") if sort_order is not None else df\n\n    color_gen_threshold = 4\n    cmap = get_single_color_cmap() if len(value_col) &lt; color_gen_threshold else get_multi_color_cmap()\n\n    plot_kind = \"bar\" if orientation in [\"vertical\", \"v\"] else \"barh\"\n\n    ax = df.plot(\n        kind=plot_kind,\n        y=value_col,\n        x=x_col,\n        ax=ax,\n        width=width,\n        color=[next(cmap) for _ in range(len(value_col))],\n        legend=(len(value_col) &gt; 1),\n        **kwargs,\n    )\n\n    ax = gu.standard_graph_styles(\n        ax=ax,\n        title=title,\n        x_label=x_label,\n        y_label=y_label,\n        legend_title=legend_title,\n        move_legend_outside=move_legend_outside,\n    )\n\n    if use_hatch:\n        ax = gu.apply_hatches(ax=ax, num_segments=len(value_col))\n\n    # Add data labels\n    if data_label_format:\n        _generate_bar_labels(\n            ax=ax,\n            plot_kind=plot_kind,\n            value_col=value_col,\n            df=df,\n            data_label_format=data_label_format,\n            x_col=x_col if x_col is not None else df.index,\n            is_stacked=kwargs.get(\"stacked\", False),\n            num_digits=num_digits,\n        )\n\n    if source_text:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    return gu.standard_tick_styles(ax=ax)\n</code></pre>"},{"location":"api/plots/cohort/","title":"Cohort Plot","text":"<p>This module provides functionality for creating cohort plots from pandas DataFrames.</p> <p>It is designed to visualize data distributions using color-coded heatmaps, helping to highlight trends and comparisons between different groups.</p>"},{"location":"api/plots/cohort/#pyretailscience.plots.cohort--core-features","title":"Core Features","text":"<ul> <li>Color Mapping: Uses a predefined colormap for visualizing data.</li> <li>Customizable Labels: Supports custom labels for x-axis, y-axis, title, and colorbar.</li> <li>Source Text: Provides an option to add source attribution to the plot.</li> <li>Grid and Tick Customization: Applies standard styling for better readability.</li> </ul>"},{"location":"api/plots/cohort/#pyretailscience.plots.cohort--use-cases","title":"Use Cases","text":"<ul> <li>Cohort Analysis: Visualize how different groups behave over time.</li> <li>Category-Based Heatmaps: Compare values across different categories.</li> </ul>"},{"location":"api/plots/cohort/#pyretailscience.plots.cohort--limitations-and-warnings","title":"Limitations and Warnings","text":"<ul> <li>Data Aggregation Required: The module does not perform data aggregation; data should be pre-aggregated before being passed to the function.</li> <li>Fixed Color Mapping: The module uses a predefined colormap without dynamic adjustments.</li> </ul>"},{"location":"api/plots/cohort/#pyretailscience.plots.cohort.plot","title":"<code>plot(df, cbar_label, x_label=None, y_label=None, title=None, ax=None, source_text=None, percentage=True, figsize=None, **kwargs)</code>","text":"<p>Plots a cohort plot for the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing cohort analysis data.</p> required <code>cbar_label</code> <code>str</code> <p>Label for the colorbar.</p> required <code>x_label</code> <code>str</code> <p>Label for x-axis.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>Label for y-axis.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Matplotlib axes object to plot on.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>Additional source text annotation.</p> <code>None</code> <code>percentage</code> <code>bool</code> <p>If True, displays cohort values as percentages. Defaults to False.</p> <code>True</code> <code>figsize</code> <code>tuple[int, int]</code> <p>The size of the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments for cohort styling.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The matplotlib axes object.</p> Source code in <code>pyretailscience/plots/cohort.py</code> <pre><code>def plot(\n    df: pd.DataFrame,\n    cbar_label: str,\n    x_label: str | None = None,\n    y_label: str | None = None,\n    title: str | None = None,\n    ax: Axes | None = None,\n    source_text: str | None = None,\n    percentage: bool = True,\n    figsize: tuple[int, int] | None = None,\n    **kwargs: dict,\n) -&gt; SubplotBase:\n    \"\"\"Plots a cohort plot for the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): Dataframe containing cohort analysis data.\n        cbar_label (str): Label for the colorbar.\n        x_label (str, optional): Label for x-axis.\n        y_label (str, optional): Label for y-axis.\n        title (str, optional): Title of the plot.\n        ax (Axes, optional): Matplotlib axes object to plot on.\n        source_text (str, optional): Additional source text annotation.\n        percentage (bool, optional): If True, displays cohort values as percentages. Defaults to False.\n        figsize (tuple[int, int], optional): The size of the plot. Defaults to None.\n        **kwargs: Additional keyword arguments for cohort styling.\n\n    Returns:\n        SubplotBase: The matplotlib axes object.\n    \"\"\"\n    if ax is None:\n        _, ax = plt.subplots(figsize=figsize)\n    cmap = get_listed_cmap(\"green\")\n    im = ax.imshow(df, cmap=cmap, **kwargs)\n    cbar = ax.figure.colorbar(im, ax=ax, format=ticker.StrMethodFormatter(\"{x:.0%}\" if percentage else \"{x:,.0f}\"))\n    cbar.ax.set_ylabel(cbar_label, rotation=-90, va=\"bottom\", fontsize=\"x-large\")\n\n    ax.set_xticks(np.arange(df.shape[1]))\n    ax.set_yticks(np.arange(df.shape[0]))\n    ax.set_xticklabels(df.columns, rotation_mode=\"anchor\")\n    ax.set_yticklabels(df.index.astype(str))\n\n    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n    ax.set_xticks(np.arange(df.shape[1] + 1) - 0.5, minor=True)\n    ax.set_yticks(np.arange(df.shape[0] + 1) - 0.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle=\"-\", linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n    threshold = im.norm(1.0) / 2.0 if percentage else im.norm(df.to_numpy().max()) / 2.0\n    valfmt = ticker.StrMethodFormatter(\"{x:.0%}\" if percentage else \"{x:,.0f}\")\n    textcolors = (\"black\", \"white\")\n    for i in range(df.shape[0]):\n        for j in range(df.shape[1]):\n            color = textcolors[int(im.norm(df.iloc[i, j]) &gt; threshold)]\n            ax.text(j, i, valfmt(df.iloc[i, j], None), ha=\"center\", va=\"center\", color=color, fontsize=7)\n\n    ax = gu.standard_graph_styles(\n        ax=ax,\n        title=title,\n        x_label=x_label,\n        y_label=y_label,\n    )\n    ax.grid(False)\n    ax.hlines(y=3 - 0.5, xmin=-0.5, xmax=df.shape[1] - 0.5, color=\"white\", linewidth=4)\n\n    if source_text:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    return gu.standard_tick_styles(ax)\n</code></pre>"},{"location":"api/plots/histogram/","title":"Histogram Plot","text":"<p>This module provides flexible functionality for creating histograms from pandas DataFrames or Series.</p> <p>It allows you to visualize distributions of one or more value columns and optionally group them by a categorical column. The module is designed to handle both DataFrames and Series, allowing you to create simple histograms or compare distributions across categories by splitting the data into multiple histograms.</p>"},{"location":"api/plots/histogram/#pyretailscience.plots.histogram--core-features","title":"Core Features","text":"<ul> <li>Single or Multiple Histograms: Plot one or more value columns (<code>value_col</code>) as histograms. For example, visualize the distribution of a single metric or compare multiple metrics simultaneously.</li> <li>Grouped Histograms: Create separate histograms for each unique value in <code>group_col</code> (e.g., product categories or regions), allowing for easy comparison of distributions across groups.</li> <li>Range Clipping and Filling: Use <code>range_lower</code> and <code>range_upper</code> to limit the values being plotted by clipping them or filling values outside the range with NaN. This is particularly useful when visualizing specific data ranges.</li> <li>Comprehensive Customization: Customize plot titles, axis labels, and legends, with the option to move the legend outside the plot.</li> </ul>"},{"location":"api/plots/histogram/#pyretailscience.plots.histogram--use-cases","title":"Use Cases","text":"<ul> <li>Distribution Analysis: Visualize the distribution of key metrics like revenue, sales, or user activity using single or multiple histograms.</li> <li>Group Comparisons: Compare distributions across different groups, such as product categories, geographic regions, or customer segments. For instance, plot histograms to show how sales vary across different product categories.</li> <li>Trends and Ranges: Use range_lower and range_upper to visualize data within specific ranges, filtering out outliers or focusing on key metrics for analysis.</li> </ul>"},{"location":"api/plots/histogram/#pyretailscience.plots.histogram--limitations-and-handling-of-data","title":"Limitations and Handling of Data","text":"<ul> <li>Pre-Aggregated Data Required: This module does not perform any data aggregation, so all data must be pre-aggregated before being passed in for plotting.</li> <li>Grouped Histograms: If <code>group_col</code> is provided, the data will be pivoted so that each unique value in <code>group_col</code> becomes a separate histogram. Otherwise, a single histogram is plotted.</li> <li>Series Support: The module can also handle pandas Series, though <code>group_col</code> cannot be provided when plotting a Series.</li> </ul>"},{"location":"api/plots/histogram/#pyretailscience.plots.histogram--additional-features","title":"Additional Features","text":"<ul> <li>Range Clipping or Filling: You can control how the data is visualized by specifying bounds. If data points fall outside the defined range, you can either clip them to the boundary values or fill them with NaN for exclusion.</li> <li>Legend Customization: For multiple histograms, you can add legends, including the option to move the legend outside the plot for clarity.</li> </ul>"},{"location":"api/plots/histogram/#pyretailscience.plots.histogram.plot","title":"<code>plot(df, value_col=None, group_col=None, title=None, x_label=None, y_label=None, legend_title=None, ax=None, source_text=None, move_legend_outside=False, range_lower=None, range_upper=None, range_method='clip', use_hatch=False, **kwargs)</code>","text":"<p>Plots a histogram of <code>value_col</code>, optionally split by <code>group_col</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame | Series</code> <p>The dataframe (or series) to plot.</p> required <code>value_col</code> <code>str or list of str</code> <p>The column(s) to plot. Can be a list of columns for multiple histograms.</p> <code>None</code> <code>group_col</code> <code>str</code> <p>The column used to define different histograms.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>None</code> <code>x_label</code> <code>str</code> <p>The x-axis label.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label.</p> <code>None</code> <code>legend_title</code> <code>str</code> <p>The title of the legend.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Matplotlib axes object to plot on.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot.</p> <code>None</code> <code>move_legend_outside</code> <code>bool</code> <p>Move the legend outside the plot.</p> <code>False</code> <code>range_lower</code> <code>float</code> <p>Lower bound for clipping or filling NA values.</p> <code>None</code> <code>range_upper</code> <code>float</code> <p>Upper bound for clipping or filling NA values.</p> <code>None</code> <code>range_method</code> <code>str</code> <p>Whether to \"clip\" values outside the range or \"fillna\". Defaults to \"clip\".</p> <code>'clip'</code> <code>use_hatch</code> <code>bool</code> <p>Whether to use hatching for the bars.</p> <code>False</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments for Pandas' <code>plot</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The matplotlib axes object.</p> Source code in <code>pyretailscience/plots/histogram.py</code> <pre><code>def plot(\n    df: pd.DataFrame | pd.Series,\n    value_col: str | list[str] | None = None,\n    group_col: str | None = None,\n    title: str | None = None,\n    x_label: str | None = None,\n    y_label: str | None = None,\n    legend_title: str | None = None,\n    ax: Axes | None = None,\n    source_text: str | None = None,\n    move_legend_outside: bool = False,\n    range_lower: float | None = None,\n    range_upper: float | None = None,\n    range_method: Literal[\"clip\", \"fillna\"] = \"clip\",\n    use_hatch: bool = False,\n    **kwargs: dict[str, Any],\n) -&gt; SubplotBase:\n    \"\"\"Plots a histogram of `value_col`, optionally split by `group_col`.\n\n    Args:\n        df (pd.DataFrame | pd.Series): The dataframe (or series) to plot.\n        value_col (str or list of str, optional): The column(s) to plot. Can be a list of columns for multiple histograms.\n        group_col (str, optional): The column used to define different histograms.\n        title (str, optional): The title of the plot.\n        x_label (str, optional): The x-axis label.\n        y_label (str, optional): The y-axis label.\n        legend_title (str, optional): The title of the legend.\n        ax (Axes, optional): Matplotlib axes object to plot on.\n        source_text (str, optional): The source text to add to the plot.\n        move_legend_outside (bool, optional): Move the legend outside the plot.\n        range_lower (float, optional): Lower bound for clipping or filling NA values.\n        range_upper (float, optional): Upper bound for clipping or filling NA values.\n        range_method (str, optional): Whether to \"clip\" values outside the range or \"fillna\". Defaults to \"clip\".\n        use_hatch (bool, optional): Whether to use hatching for the bars.\n        **kwargs: Additional keyword arguments for Pandas' `plot` function.\n\n    Returns:\n        SubplotBase: The matplotlib axes object.\n    \"\"\"\n    if isinstance(value_col, list) and group_col is not None:\n        raise ValueError(\"`value_col` cannot be a list when `group_col` is provided. Please choose one or the other.\")\n\n    value_col = _prepare_value_col(df=df, value_col=value_col)\n\n    if isinstance(df, pd.Series):\n        df = df.to_frame(name=value_col[0])\n\n    if (range_lower is not None) or (range_upper is not None):\n        df = _apply_range_clipping(\n            df=df,\n            value_col=value_col,\n            range_lower=range_lower,\n            range_upper=range_upper,\n            range_method=range_method,\n        )\n\n    num_histograms = _get_num_histograms(df=df, value_col=value_col, group_col=group_col)\n\n    color_gen = get_multi_color_cmap()\n    colors = [next(color_gen) for _ in range(num_histograms)]\n\n    ax = _plot_histogram(\n        df=df,\n        value_col=value_col,\n        group_col=group_col,\n        ax=ax,\n        colors=colors,\n        num_histograms=num_histograms,\n        **kwargs,\n    )\n\n    ax = gu.standard_graph_styles(\n        ax=ax,\n        title=title,\n        x_label=x_label,\n        y_label=y_label,\n        legend_title=legend_title,\n        move_legend_outside=move_legend_outside,\n    )\n\n    if use_hatch:\n        ax = gu.apply_hatches(ax=ax, num_segments=num_histograms)\n\n    if source_text:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    return gu.standard_tick_styles(ax=ax)\n</code></pre>"},{"location":"api/plots/line/","title":"Line Plot","text":"<p>This module provides flexible functionality for creating line plots from pandas DataFrames.</p> <p>It focuses on visualizing sequences that are ordered or sequential but not necessarily categorical, such as \"days since an event\" or \"months since a competitor opened.\" However, while this module can handle datetime values on the x-axis, the plots.time_line module has additional features that make working with datetimes easier, such as easily resampling the data to alternate time frames.</p> <p>The sequences used in this module can include values like \"days since an event\" (e.g., -2, -1, 0, 1, 2) or \"months since a competitor store opened.\" This module is not intended for use with actual datetime values. If a datetime or datetime-like column is passed as <code>x_col</code>, a warning will be triggered, suggesting the use of the <code>plots.time_line</code> module.</p>"},{"location":"api/plots/line/#pyretailscience.plots.line--core-features","title":"Core Features","text":"<ul> <li>Plotting Sequences or Indexes: Plot one or more value columns (<code>value_col</code>) with support for sequences like -2, -1, 0, 1, 2 (e.g., months since an event), using either the index or a specified x-axis column (<code>x_col</code>).</li> <li>Custom X-Axis or Index: Use any column as the x-axis (<code>x_col</code>) or plot based on the index if no x-axis column is specified.</li> <li>Multiple Lines: Create separate lines for each unique value in <code>group_col</code> (e.g., categories or product types).</li> <li>Comprehensive Customization: Easily customize plot titles, axis labels, and legends, with the option to move the legend outside the plot.</li> <li>Pre-Aggregated Data: The data must be pre-aggregated before plotting, as no aggregation occurs within the module.</li> </ul>"},{"location":"api/plots/line/#pyretailscience.plots.line--use-cases","title":"Use Cases","text":"<ul> <li>Daily Trends: Plot trends such as daily revenue or user activity, for example, tracking revenue since the start of the year.</li> <li>Event Impact: Visualize how metrics (e.g., revenue, sales, or traffic) change before and after an important event, such as a competitor store opening or a product launch.</li> <li>Category Comparison: Compare metrics across multiple categories over time, for example, tracking total revenue for the top categories before and after an event like the introduction of a new competitor.</li> </ul>"},{"location":"api/plots/line/#pyretailscience.plots.line--limitations-and-handling-of-temporal-data","title":"Limitations and Handling of Temporal Data","text":"<ul> <li>Limited Handling of Temporal Data: This module can plot simple time-based sequences, such as \"days since an event,\" but it cannot manipulate or directly handle datetime or date-like columns. It is not optimized for actual datetime values. If a datetime column is passed or more complex temporal plotting is needed, a warning will suggest using the <code>plots.time_line</code> module, which is specifically designed for working with temporal data and performing time-based manipulation.</li> <li>Pre-Aggregated Data Required: The module does not perform any data aggregation, so all data must be pre-aggregated before being passed in for plotting.</li> </ul>"},{"location":"api/plots/line/#pyretailscience.plots.line.plot","title":"<code>plot(df, value_col, x_label=None, y_label=None, title=None, x_col=None, group_col=None, ax=None, source_text=None, legend_title=None, move_legend_outside=False, **kwargs)</code>","text":"<p>Plots the <code>value_col</code> over the specified <code>x_col</code> or index, creating a separate line for each unique value in <code>group_col</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to plot.</p> required <code>value_col</code> <code>str or list of str</code> <p>The column(s) to plot.</p> required <code>x_label</code> <code>str</code> <p>The x-axis label.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The column to be used as the x-axis. If None, the index is used.</p> <code>None</code> <code>group_col</code> <code>str</code> <p>The column used to define different lines.</p> <code>None</code> <code>legend_title</code> <code>str</code> <p>The title of the legend.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Matplotlib axes object to plot on.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot.</p> <code>None</code> <code>move_legend_outside</code> <code>bool</code> <p>Move the legend outside the plot.</p> <code>False</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments for Pandas' <code>plot</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The matplotlib axes object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>value_col</code> is a list and <code>group_col</code> is provided (which causes ambiguity in plotting).</p> Source code in <code>pyretailscience/plots/line.py</code> <pre><code>def plot(\n    df: pd.DataFrame,\n    value_col: str | list[str],\n    x_label: str | None = None,\n    y_label: str | None = None,\n    title: str | None = None,\n    x_col: str | None = None,\n    group_col: str | None = None,\n    ax: Axes | None = None,\n    source_text: str | None = None,\n    legend_title: str | None = None,\n    move_legend_outside: bool = False,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plots the `value_col` over the specified `x_col` or index, creating a separate line for each unique value in `group_col`.\n\n    Args:\n        df (pd.DataFrame): The dataframe to plot.\n        value_col (str or list of str): The column(s) to plot.\n        x_label (str, optional): The x-axis label.\n        y_label (str, optional): The y-axis label.\n        title (str, optional): The title of the plot.\n        x_col (str, optional): The column to be used as the x-axis. If None, the index is used.\n        group_col (str, optional): The column used to define different lines.\n        legend_title (str, optional): The title of the legend.\n        ax (Axes, optional): Matplotlib axes object to plot on.\n        source_text (str, optional): The source text to add to the plot.\n        move_legend_outside (bool, optional): Move the legend outside the plot.\n        **kwargs: Additional keyword arguments for Pandas' `plot` function.\n\n    Returns:\n        SubplotBase: The matplotlib axes object.\n\n    Raises:\n        ValueError: If `value_col` is a list and `group_col` is provided (which causes ambiguity in plotting).\n    \"\"\"\n    if x_col is not None and pd.api.types.is_datetime64_any_dtype(df[x_col]):\n        warnings.warn(\n            f\"The column '{x_col}' is datetime-like. Consider using the 'plots.time_line' module for time-based plots.\",\n            UserWarning,\n            stacklevel=2,\n        )\n\n    elif x_col is None and pd.api.types.is_datetime64_any_dtype(df.index):\n        warnings.warn(\n            \"The DataFrame index is datetime-like. Consider using the 'plots.time_line' module for time-based plots.\",\n            UserWarning,\n            stacklevel=2,\n        )\n    if isinstance(value_col, list) and group_col:\n        raise ValueError(\"Cannot use both a list for `value_col` and a `group_col`. Choose one.\")\n    if group_col is None:\n        pivot_df = df.set_index(x_col if x_col is not None else df.index)[\n            [value_col] if isinstance(value_col, str) else value_col\n        ]\n    else:\n        pivot_df = df.pivot(index=x_col if x_col is not None else None, columns=group_col, values=value_col)\n\n    is_multi_line = (group_col is not None) or (isinstance(value_col, list) and len(value_col) &gt; 1)\n\n    color_gen_threshold = 4\n    num_colors = len(pivot_df.columns) if is_multi_line else 1\n    color_gen = get_single_color_cmap() if num_colors &lt; color_gen_threshold else get_multi_color_cmap()\n    colors = [next(color_gen) for _ in range(num_colors)]\n\n    ax = pivot_df.plot(\n        ax=ax,\n        linewidth=3,\n        color=colors,\n        legend=is_multi_line,\n        **kwargs,\n    )\n\n    ax = gu.standard_graph_styles(\n        ax=ax,\n        title=title,\n        x_label=x_label,\n        y_label=y_label,\n        legend_title=legend_title,\n        move_legend_outside=move_legend_outside,\n    )\n\n    if source_text is not None:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    return gu.standard_tick_styles(ax)\n</code></pre>"},{"location":"api/plots/period_on_period/","title":"Period On Period Plot","text":"<p>Period on period module.</p> <p>This module provides functionality for plotting multiple overlapping time periods from the same time series on a single line chart using matplotlib.</p> <p>The <code>plot</code> function is useful for visual comparisons of temporal trends across different time windows, with each time window plotted as a separate line but aligned to a common starting point.</p> <p>Example use case: Comparing sales data across multiple promotional weeks or seasonal periods.</p>"},{"location":"api/plots/period_on_period/#pyretailscience.plots.period_on_period.plot","title":"<code>plot(df, x_col, value_col, periods, x_label=None, y_label=None, title=None, source_text=None, legend_title=None, move_legend_outside=False, ax=None, **kwargs)</code>","text":"<p>Plot multiple overlapping periods from a single time series as individual lines.</p> <p>This function is used to align and overlay several time intervals from the same dataset to facilitate visual comparison. Each period is realigned to the reference start date and plotted as a separate line using a distinct linestyle.</p> Note <p>The <code>periods</code> argument accepts a list of (start_date, end_date) tuples, which define the time windows to overlay. Each element in the tuple can be either a string (e.g., \"2022-01-01\") or a <code>datetime</code> object. You can use <code>find_overlapping_periods</code> from <code>pyretailscience.utils.date</code> to generate the <code>periods</code> input automatically.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the time series data.</p> required <code>x_col</code> <code>str</code> <p>Name of the column representing datetime values.</p> required <code>value_col</code> <code>str</code> <p>Name of the column representing the y-axis values (e.g. sales, counts).</p> required <code>periods</code> <code>List[Tuple[Union[str, datetime], Union[str, datetime]]]</code> <p>A list of (start_date, end_date) tuples representing the periods to plot.</p> required <code>x_label</code> <code>Optional[str]</code> <p>Custom label for the x-axis.</p> <code>None</code> <code>y_label</code> <code>Optional[str]</code> <p>Custom label for the y-axis.</p> <code>None</code> <code>title</code> <code>Optional[str]</code> <p>Title for the plot.</p> <code>None</code> <code>source_text</code> <code>Optional[str]</code> <p>Text to show below the plot as a data source.</p> <code>None</code> <code>legend_title</code> <code>Optional[str]</code> <p>Title for the plot legend.</p> <code>None</code> <code>move_legend_outside</code> <code>bool</code> <p>Whether to place the legend outside the plot area.</p> <code>False</code> <code>ax</code> <code>Optional[Axes]</code> <p>Matplotlib Axes object to draw on. If None, a new one is created.</p> <code>None</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments passed to the base line plot function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>matplotlib.axes.Axes: The matplotlib Axes object with the completed plot.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>The 'periods' list must contain at least two (start, end) tuples for comparison.</p> Source code in <code>pyretailscience/plots/period_on_period.py</code> <pre><code>def plot(\n    df: pd.DataFrame,\n    x_col: str,\n    value_col: str,\n    periods: list[tuple[str | datetime, str | datetime]],\n    x_label: str | None = None,\n    y_label: str | None = None,\n    title: str | None = None,\n    source_text: str | None = None,\n    legend_title: str | None = None,\n    move_legend_outside: bool = False,\n    ax: Axes | None = None,\n    **kwargs: dict[str, any],\n) -&gt; Axes:\n    \"\"\"Plot multiple overlapping periods from a single time series as individual lines.\n\n    This function is used to align and overlay several time intervals from the same\n    dataset to facilitate visual comparison. Each period is realigned to the reference\n    start date and plotted as a separate line using a distinct linestyle.\n\n    Note:\n        The `periods` argument accepts a list of (start_date, end_date) tuples,\n        which define the time windows to overlay. Each element in the tuple can be either\n        a string (e.g., \"2022-01-01\") or a `datetime` object. You can use\n        `find_overlapping_periods` from `pyretailscience.utils.date` to generate\n        the `periods` input automatically.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the time series data.\n        x_col (str): Name of the column representing datetime values.\n        value_col (str): Name of the column representing the y-axis values (e.g. sales, counts).\n        periods (List[Tuple[Union[str, datetime], Union[str, datetime]]]):\n            A list of (start_date, end_date) tuples representing the periods to plot.\n        x_label (Optional[str]): Custom label for the x-axis.\n        y_label (Optional[str]): Custom label for the y-axis.\n        title (Optional[str]): Title for the plot.\n        source_text (Optional[str]): Text to show below the plot as a data source.\n        legend_title (Optional[str]): Title for the plot legend.\n        move_legend_outside (bool): Whether to place the legend outside the plot area.\n        ax (Optional[Axes]): Matplotlib Axes object to draw on. If None, a new one is created.\n        **kwargs: Additional keyword arguments passed to the base line plot function.\n\n    Returns:\n        matplotlib.axes.Axes: The matplotlib Axes object with the completed plot.\n\n    Raises:\n        ValueError: The 'periods' list must contain at least two (start, end) tuples for comparison.\n    \"\"\"\n    min_period_length = 2\n    if len(periods) &lt; min_period_length:\n        raise ValueError(\"The 'periods' list must contain at least two (start, end) tuples for comparison.\")\n\n    periods = [(pd.to_datetime(start), pd.to_datetime(end)) for start, end in periods]\n    start_ref = periods[0][0]\n\n    sorted_periods = sorted(periods, reverse=True, key=lambda x: pd.to_datetime(x[0]))\n\n    ax = ax or plt.gca()\n\n    period_styles = {period: LINE_STYLES[idx % len(LINE_STYLES)] for idx, period in enumerate(sorted_periods)}\n\n    df[x_col] = pd.to_datetime(df[x_col])\n\n    start_ref_year = start_ref.year\n\n    for start_str, end_str in periods:\n        style = period_styles[(start_str, end_str)]\n        start = pd.to_datetime(start_str)\n        end = pd.to_datetime(end_str)\n        period_df = df[(df[x_col] &gt;= start) &amp; (df[x_col] &lt;= end)].copy()\n\n        if period_df.empty:\n            continue\n\n        year_diff = start.year - start_ref_year\n\n        period_df[\"realigned_date\"] = period_df[x_col].apply(\n            lambda d, year_diff=year_diff: d - relativedelta(years=year_diff),\n        )\n\n        label = f\"{start_str.date()} to {end_str.date()}\"\n        line_plot(\n            df=period_df,\n            x_col=\"realigned_date\",\n            value_col=value_col,\n            ax=ax,\n            linestyle=style,\n            x_label=x_label,\n            y_label=y_label,\n            **kwargs,\n        )\n        line = ax.get_lines()[-1]\n        line.set_label(label)\n        line.set_linestyle(style)\n\n    ax = gu.standard_graph_styles(\n        ax=ax,\n        title=title,\n        x_label=x_label or x_col,\n        y_label=y_label or value_col,\n        legend_title=legend_title,\n        move_legend_outside=move_legend_outside,\n    )\n\n    if source_text:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    return ax\n</code></pre>"},{"location":"api/plots/scatter/","title":"Scatter Plot","text":"<p>This module provides functionality for creating scatter plots from pandas DataFrames.</p> <p>It is designed to visualize relationships between variables, highlight distributions, and compare different categories using scatter points.</p>"},{"location":"api/plots/scatter/#pyretailscience.plots.scatter--core-features","title":"Core Features","text":"<ul> <li>Flexible X-Axis Handling: Uses an index or a specified x-axis column (<code>x_col</code>) for plotting.</li> <li>Multiple Scatter Groups: Supports plotting multiple columns (<code>value_col</code>) or groups (<code>group_col</code>).</li> <li>Dynamic Color Mapping: Automatically selects a colormap based on the number of groups.</li> <li>Legend Customization: Supports custom legend titles and the option to move the legend outside the plot.</li> <li>Source Text: Provides an option to add source attribution to the plot.</li> </ul>"},{"location":"api/plots/scatter/#pyretailscience.plots.scatter--use-cases","title":"Use Cases","text":"<ul> <li>Category-Based Scatter Plots: Compare different categories using scatter points.</li> <li>Trend Analysis: Identify patterns and outliers in datasets.</li> <li>Multi-Value Scatter Plots: Show multiple data series in a single scatter chart.</li> </ul>"},{"location":"api/plots/scatter/#pyretailscience.plots.scatter--limitations-and-warnings","title":"Limitations and Warnings","text":"<ul> <li>Pre-Aggregated Data Required: The module does not perform data aggregation; data should be pre-aggregated before being passed to the function.</li> </ul>"},{"location":"api/plots/scatter/#pyretailscience.plots.scatter.plot","title":"<code>plot(df, value_col, x_label=None, y_label=None, title=None, x_col=None, group_col=None, ax=None, source_text=None, legend_title=None, move_legend_outside=False, **kwargs)</code>","text":"<p>Plots a scatter chart for the given <code>value_col</code> over <code>x_col</code> or index, with optional grouping by <code>group_col</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame or Series</code> <p>The dataframe or series to plot.</p> required <code>value_col</code> <code>str or list of str</code> <p>The column(s) to plot.</p> required <code>x_label</code> <code>str</code> <p>The x-axis label.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The column to be used as the x-axis. If None, the index is used.</p> <code>None</code> <code>group_col</code> <code>str</code> <p>The column used to define different scatter groups.</p> <code>None</code> <code>legend_title</code> <code>str</code> <p>The title of the legend.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Matplotlib axes object to plot on.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot.</p> <code>None</code> <code>move_legend_outside</code> <code>bool</code> <p>Move the legend outside the plot.</p> <code>False</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments for Pandas' <code>plot</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The matplotlib axes object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>value_col</code> is a list and <code>group_col</code> is provided (which causes ambiguity in plotting).</p> Source code in <code>pyretailscience/plots/scatter.py</code> <pre><code>def plot(\n    df: pd.DataFrame | pd.Series,\n    value_col: str | list[str],\n    x_label: str | None = None,\n    y_label: str | None = None,\n    title: str | None = None,\n    x_col: str | None = None,\n    group_col: str | None = None,\n    ax: Axes | None = None,\n    source_text: str | None = None,\n    legend_title: str | None = None,\n    move_legend_outside: bool = False,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plots a scatter chart for the given `value_col` over `x_col` or index, with optional grouping by `group_col`.\n\n    Args:\n        df (pd.DataFrame or pd.Series): The dataframe or series to plot.\n        value_col (str or list of str): The column(s) to plot.\n        x_label (str, optional): The x-axis label.\n        y_label (str, optional): The y-axis label.\n        title (str, optional): The title of the plot.\n        x_col (str, optional): The column to be used as the x-axis. If None, the index is used.\n        group_col (str, optional): The column used to define different scatter groups.\n        legend_title (str, optional): The title of the legend.\n        ax (Axes, optional): Matplotlib axes object to plot on.\n        source_text (str, optional): The source text to add to the plot.\n        move_legend_outside (bool, optional): Move the legend outside the plot.\n        **kwargs: Additional keyword arguments for Pandas' `plot` function.\n\n    Returns:\n        SubplotBase: The matplotlib axes object.\n\n    Raises:\n        ValueError: If `value_col` is a list and `group_col` is provided (which causes ambiguity in plotting).\n    \"\"\"\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n\n    if isinstance(value_col, list) and group_col:\n        raise ValueError(\"Cannot use both a list for `value_col` and a `group_col`. Choose one.\")\n\n    if group_col is None:\n        pivot_df = df.set_index(x_col if x_col is not None else df.index)[\n            [value_col] if isinstance(value_col, str) else value_col\n        ]\n    else:\n        pivot_df = df.pivot(index=x_col if x_col is not None else None, columns=group_col, values=value_col)\n\n    is_multi_scatter = (group_col is not None) or (isinstance(value_col, list) and len(value_col) &gt; 1)\n\n    color_gen_threshold = 3\n    num_colors = len(pivot_df.columns) if is_multi_scatter else 1\n    color_gen = get_single_color_cmap() if num_colors &lt; color_gen_threshold else get_multi_color_cmap()\n    colors = [next(color_gen) for _ in range(num_colors)]\n\n    ax = ax or plt.gca()\n    alpha = kwargs.pop(\"alpha\", 0.7)\n    for col, color in zip(pivot_df.columns, colors, strict=False):\n        ax.scatter(\n            pivot_df.index,\n            pivot_df[col],\n            color=color,\n            label=col if is_multi_scatter else None,\n            alpha=alpha,\n            **kwargs,\n        )\n\n    ax = gu.standard_graph_styles(\n        ax=ax,\n        title=title,\n        x_label=x_label,\n        y_label=y_label,\n        legend_title=legend_title,\n        move_legend_outside=move_legend_outside,\n    )\n\n    if source_text is not None:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    return gu.standard_tick_styles(ax)\n</code></pre>"},{"location":"api/plots/time/","title":"Time Plot","text":"<p>This module provides functionality for creating timeline plots.</p> <p>Which are essential for visualizing transactional data over time. By aggregating data by specified periods (e.g., daily, weekly, monthly), timeline plots help to identify trends, seasonal patterns, and performance variations across different timeframes. These plots are valuable tools for retail analysis, sales tracking, and customer behavior insights.</p>"},{"location":"api/plots/time/#pyretailscience.plots.time--features","title":"Features","text":"<ul> <li>Timeline Plot Creation: Plot a value column (e.g., sales, transactions) over time, aggregated by a specific period (e.g., daily, weekly).</li> <li>Customizable Aggregation: Supports different aggregation functions (e.g., sum, average) to compute the value column's metrics.</li> <li>Grouping by Categories: Optionally group data by a specific category (e.g., product, region, store) and compare performance over time.</li> <li>Time Period Handling: The <code>period</code> parameter allows data aggregation by different time periods, such as days, weeks, or months.</li> <li>Graph Styling: Customize the appearance of the plot with options to adjust titles, axis labels, legend placement, and more.</li> <li>Color Mapping: Use linear color gradients for category-based groupings to visually differentiate between groups in the timeline.</li> </ul>"},{"location":"api/plots/time/#pyretailscience.plots.time--use-cases","title":"Use Cases","text":"<ul> <li>Sales and Revenue Analysis: Track sales performance over time, either as a total or by group (e.g., product category or store).</li> <li>Seasonal Trend Analysis: Visualize how sales or transaction values fluctuate across different periods, helping to identify seasonal trends or promotional impacts.</li> <li>Customer Behavior Tracking: Examine changes in customer behavior (e.g., purchase frequency, average transaction value) over time.</li> <li>Comparative Performance: Compare multiple categories (e.g., different products or regions) on the same timeline to evaluate relative performance.</li> </ul>"},{"location":"api/plots/time/#pyretailscience.plots.time--limitations-and-handling-of-data","title":"Limitations and Handling of Data","text":"<ul> <li>Time Period Grouping: Data is aggregated by a time period defined by the <code>period</code> argument, which can be adjusted to daily, weekly, monthly, etc.</li> <li>Grouping by Categories: If <code>group_col</code> is specified, the plot will display performance across different categories, with color differentiation for each group.</li> <li>Flexible Aggregation: The aggregation function (e.g., sum, average) can be customized to calculate the desired value for each period.</li> </ul>"},{"location":"api/plots/time/#pyretailscience.plots.time--functionality-details","title":"Functionality Details","text":"<ul> <li>plot(): Generates a timeline plot of a specified value column over time, with customization options for grouping, aggregation, and styling.</li> <li>Helper functions: Utilizes utility functions from the <code>pyretailscience</code> package to handle styling, formatting, and other plot adjustments.</li> </ul>"},{"location":"api/plots/time/#pyretailscience.plots.time.plot","title":"<code>plot(df, value_col, period='D', agg_func='sum', group_col=None, title=None, x_label=None, y_label=None, legend_title=None, ax=None, source_text=None, move_legend_outside=False, **kwargs)</code>","text":"<p>Plots the value_col over time.</p> <p>Timeline plots are a fundamental tool for interpreting transactional data within a temporal context. By presenting data in a chronological sequence, these visualizations reveal patterns and trends that might otherwise remain hidden in raw numbers, making them essential for both historical analysis and forward-looking insights. They are particularly useful for:</p> <ul> <li>Tracking sales performance across different periods (e.g., daily, weekly, monthly)</li> <li>Identifying seasonal patterns or promotional impacts on sales</li> <li>Comparing the performance of different product categories or store locations over time</li> <li>Visualizing customer behavior trends, such as purchase frequency or average transaction value</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to plot.</p> required <code>value_col</code> <code>str</code> <p>The column to plot.</p> required <code>period</code> <code>str | BaseOffset</code> <p>The period to group the data by.</p> <code>'D'</code> <code>agg_func</code> <code>str</code> <p>The aggregation function to apply to the value_col. Defaults to \"sum\".</p> <code>'sum'</code> <code>group_col</code> <code>str</code> <p>The column to group the data by. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to None. When None the title is set to <code>f\"{value_col.title()} by {group_col.title()}\"</code></p> <code>None</code> <code>x_label</code> <code>str</code> <p>The x-axis label. Defaults to None. When None the x-axis label is set to blank</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label. Defaults to None. When None the y-axis label is set to the title case of <code>value_col</code></p> <code>None</code> <code>legend_title</code> <code>str</code> <p>The title of the legend. Defaults to None. When None the legend title is set to the title case of <code>group_col</code></p> <code>None</code> <code>ax</code> <code>Axes</code> <p>The matplotlib axes object to plot on. Defaults to None.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot. Defaults to None.</p> <code>None</code> <code>move_legend_outside</code> <code>bool</code> <p>Whether to move the legend outside the plot. Defaults to True.</p> <code>False</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the Pandas plot function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The matplotlib axes object.</p> Source code in <code>pyretailscience/plots/time.py</code> <pre><code>def plot(\n    df: pd.DataFrame,\n    value_col: str,\n    period: str | BaseOffset = \"D\",\n    agg_func: str = \"sum\",\n    group_col: str | None = None,\n    title: str | None = None,\n    x_label: str | None = None,\n    y_label: str | None = None,\n    legend_title: str | None = None,\n    ax: Axes | None = None,\n    source_text: str | None = None,\n    move_legend_outside: bool = False,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plots the value_col over time.\n\n    Timeline plots are a fundamental tool for interpreting transactional data within a temporal context. By presenting\n    data in a chronological sequence, these visualizations reveal patterns and trends that might otherwise remain hidden\n    in raw numbers, making them essential for both historical analysis and forward-looking insights. They are\n    particularly useful for:\n\n    - Tracking sales performance across different periods (e.g., daily, weekly, monthly)\n    - Identifying seasonal patterns or promotional impacts on sales\n    - Comparing the performance of different product categories or store locations over time\n    - Visualizing customer behavior trends, such as purchase frequency or average transaction value\n\n    Args:\n        df (pd.DataFrame): The dataframe to plot.\n        value_col (str): The column to plot.\n        period (str | BaseOffset): The period to group the data by.\n        agg_func (str, optional): The aggregation function to apply to the value_col. Defaults to \"sum\".\n        group_col (str, optional): The column to group the data by. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to None. When None the title is set to\n            `f\"{value_col.title()} by {group_col.title()}\"`\n        x_label (str, optional): The x-axis label. Defaults to None. When None the x-axis label is set to blank\n        y_label (str, optional): The y-axis label. Defaults to None. When None the y-axis label is set to the title\n            case of `value_col`\n        legend_title (str, optional): The title of the legend. Defaults to None. When None the legend title is set to\n            the title case of `group_col`\n        ax (Axes, optional): The matplotlib axes object to plot on. Defaults to None.\n        source_text (str, optional): The source text to add to the plot. Defaults to None.\n        move_legend_outside (bool, optional): Whether to move the legend outside the plot. Defaults to True.\n        **kwargs: Additional keyword arguments to pass to the Pandas plot function.\n\n    Returns:\n        SubplotBase: The matplotlib axes object.\n    \"\"\"\n    df[\"transaction_period\"] = df[get_option(\"column.transaction_date\")].dt.to_period(\n        period,\n    )\n\n    if group_col is None:\n        colors = COLORS[\"green\"][500]\n        df = df.groupby(\"transaction_period\")[value_col].agg(agg_func)\n        default_title = \"Total Sales\"\n        show_legend = False\n    else:\n        colors = get_linear_cmap(\"green\")(\n            np.linspace(0, 1, df[group_col].nunique()),\n        )\n        df = (\n            df.groupby([group_col, \"transaction_period\"])[value_col]\n            .agg(agg_func)\n            .reset_index()\n            .pivot(index=\"transaction_period\", columns=group_col, values=value_col)\n        )\n        default_title = f\"{value_col.title()} by {group_col.title()}\"\n        show_legend = True\n\n    ax = df.plot(\n        linewidth=3,\n        color=colors,\n        legend=show_legend,\n        ax=ax,\n        **kwargs,\n    )\n    ax = gu.standard_graph_styles(\n        ax,\n        title=gu.not_none(title, default_title),\n        x_label=gu.not_none(x_label, \"\"),\n        y_label=gu.not_none(y_label, value_col.title()),\n        legend_title=legend_title,\n        move_legend_outside=move_legend_outside,\n        show_legend=show_legend,\n    )\n\n    decimals = gu.get_decimals(ax.get_ylim(), ax.get_yticks())\n    ax.yaxis.set_major_formatter(\n        lambda x, pos: gu.human_format(x, pos, decimals=decimals),\n    )\n\n    if source_text is not None:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    gu.standard_tick_styles(ax)\n\n    return ax\n</code></pre>"},{"location":"api/plots/venn/","title":"Venn Plot","text":"<p>This module provides functionality for creating Venn and Euler diagrams from pandas DataFrames.</p> <p>It is designed to visualize relationships between sets, highlighting intersections and differences between them.</p>"},{"location":"api/plots/venn/#pyretailscience.plots.venn--core-features","title":"Core Features","text":"<ul> <li>Supports 2-set and 3-set Diagrams: Allows visualization of up to three overlapping sets.</li> <li>Venn and Euler Diagrams: Uses Venn diagrams by default; switches to Euler diagrams when <code>vary_size=True</code>.</li> <li>Customizable Colors and Labels: Automatically assigns colors and labels for subset representation.</li> <li>Dynamic Sizing: Adjusts circle sizes for Euler diagrams to reflect proportions.</li> <li>Title and Source Attribution: Optionally adds a title and source text.</li> </ul>"},{"location":"api/plots/venn/#pyretailscience.plots.venn--use-cases","title":"Use Cases","text":"<ul> <li>Set Comparisons: Identify shared and unique elements across two or three sets.</li> <li>Proportional Representation: Euler diagrams ensure area-accurate representation.</li> <li>Data Overlap Visualization: Helps in understanding relationships within categorical data.</li> </ul>"},{"location":"api/plots/venn/#pyretailscience.plots.venn--limitations-and-warnings","title":"Limitations and Warnings","text":"<ul> <li>Only Supports 2 or 3 Sets: Does not extend to Venn diagrams with more than three sets.</li> <li>Pre-Aggregated Data Required: The module does not perform data aggregation; input data should already be structured correctly.</li> </ul>"},{"location":"api/plots/venn/#pyretailscience.plots.venn.plot","title":"<code>plot(df, labels, title=None, source_text=None, vary_size=False, figsize=None, ax=None, subset_label_formatter=None, **kwargs)</code>","text":"<p>Plots a Venn or Euler diagram using subset sizes extracted from a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with 'groups' and 'percent' columns.</p> required <code>labels</code> <code>list[str]</code> <p>Labels for the sets in the diagram.</p> required <code>title</code> <code>str</code> <p>Title of the plot. Defaults to None.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>Source text for attribution. Defaults to None.</p> <code>None</code> <code>vary_size</code> <code>bool</code> <p>Whether to vary circle size based on subset sizes. Defaults to False.</p> <code>False</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Size of the plot. Defaults to None.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Matplotlib axes object to plot on. Defaults to None.</p> <code>None</code> <code>subset_label_formatter</code> <code>callable</code> <p>Function to format subset labels. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The matplotlib axes object with the plotted diagram.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of sets is not 2 or 3.</p> Source code in <code>pyretailscience/plots/venn.py</code> <pre><code>def plot(\n    df: pd.DataFrame,\n    labels: list[str],\n    title: str | None = None,\n    source_text: str | None = None,\n    vary_size: bool = False,\n    figsize: tuple[int, int] | None = None,\n    ax: Axes | None = None,\n    subset_label_formatter: Callable | None = None,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plots a Venn or Euler diagram using subset sizes extracted from a DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame with 'groups' and 'percent' columns.\n        labels (list[str]): Labels for the sets in the diagram.\n        title (str, optional): Title of the plot. Defaults to None.\n        source_text (str, optional): Source text for attribution. Defaults to None.\n        vary_size (bool, optional): Whether to vary circle size based on subset sizes. Defaults to False.\n        figsize (tuple[int, int], optional): Size of the plot. Defaults to None.\n        ax (Axes, optional): Matplotlib axes object to plot on. Defaults to None.\n        subset_label_formatter (callable, optional): Function to format subset labels. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        SubplotBase: The matplotlib axes object with the plotted diagram.\n\n    Raises:\n        ValueError: If the number of sets is not 2 or 3.\n    \"\"\"\n    num_sets = len(labels)\n    if num_sets not in {MIN_SUPPORTED_SETS, MAX_SUPPORTED_SETS}:\n        raise ValueError(\"Only 2-set or 3-set Venn diagrams are supported.\")\n\n    colors = [COLORS[\"green\"][500], COLORS[\"green\"][800]]\n    if num_sets == MAX_SUPPORTED_SETS:\n        colors.append(COLORS[\"green\"][200])\n\n    zero_group = (0, 0) if num_sets == MIN_SUPPORTED_SETS else (0, 0, 0)\n    percent_s = df.loc[df[\"groups\"] != zero_group, [\"groups\", \"percent\"]].set_index(\"groups\")[\"percent\"]\n    subset_sizes = percent_s.to_dict()\n\n    subset_labels = {\n        k: subset_label_formatter(v) if subset_label_formatter else str(v) for k, v in subset_sizes.items()\n    }\n\n    if ax is None:\n        _, ax = plt.subplots(figsize=figsize)\n\n    diagram_class = EulerDiagram if vary_size else VennDiagram\n    diagram = diagram_class(\n        set_labels=labels,\n        subset_sizes=subset_sizes,\n        subset_labels=subset_labels,\n        set_colors=colors,\n        ax=ax,\n        **kwargs,\n    )\n\n    center_x, center_y, displacement = 0.5, 0.5, 0.1\n    for text in diagram.set_label_artists:\n        text.set_fontproperties(GraphStyles.POPPINS_REG)\n        text.set_fontsize(GraphStyles.DEFAULT_AXIS_LABEL_FONT_SIZE)\n        if num_sets == MAX_SUPPORTED_SETS and not vary_size:\n            x, y = text.get_position()\n            direction_x, direction_y = x - center_x, y - center_y\n            scale = displacement / (direction_x**2 + direction_y**2) ** 0.5\n            text.set_position((x + scale * direction_x, y + scale * direction_y))\n\n    for subset_id in subset_sizes:\n        if subset_id not in diagram.subset_label_artists:\n            continue\n        text = diagram.subset_label_artists[subset_id]\n        text.set_fontproperties(GraphStyles.POPPINS_REG)\n\n    if title:\n        ax.set_title(\n            title,\n            fontproperties=GraphStyles.POPPINS_SEMI_BOLD,\n            fontsize=GraphStyles.DEFAULT_TITLE_FONT_SIZE,\n            pad=GraphStyles.DEFAULT_TITLE_PAD + 20,\n        )\n\n    if source_text is not None:\n        ax.set_xticklabels([], visible=False)\n        gu.add_source_text(ax=ax, source_text=source_text, is_venn_diagram=True)\n\n    return ax\n</code></pre>"},{"location":"api/plots/waterfall/","title":"Waterfall Plot","text":"<p>This module provides functionality to generate a waterfall chart.</p> <p>A visualization commonly used to illustrate how different positive and negative values contribute to a cumulative total.Waterfall charts are effective in showing the incremental impact of individual components, making them particularly useful for financial analysis, performance tracking, and visualizing changes over time.</p>"},{"location":"api/plots/waterfall/#pyretailscience.plots.waterfall--features","title":"Features","text":"<ul> <li>Waterfall Chart Creation: Displays how different positive and negative values affect a starting total.</li> <li>Data Label Formatting: Supports custom formatting for data labels, including absolute values, percentages, or both.</li> <li>Net Line and Bar Display: Optionally includes a net line and net bar to show the overall cumulative result.</li> <li>Customizable Plot Style: Options to customize chart titles, axis labels, and remove zero amounts for better clarity.</li> <li>Handling of Zero Amounts: Allows removal of zero amounts from the plot to avoid cluttering the chart.</li> <li>Interactive Elements: Supports custom annotations for the chart with source text.</li> </ul>"},{"location":"api/plots/waterfall/#pyretailscience.plots.waterfall--use-cases","title":"Use Cases","text":"<ul> <li>Financial Analysis: Show the breakdown of profits and losses over multiple periods, or how different cost categories affect overall margin.</li> <li>Revenue Tracking: Track how revenue or other key metrics change over time, and visualize the impact of individual contributing factors.</li> <li>Performance Visualization: Highlight how various business or product categories affect overall performance, such as sales, expenses, or growth metrics.</li> <li>Budget Breakdown: Visualize how different spending categories contribute to a total budget over a period.</li> </ul>"},{"location":"api/plots/waterfall/#pyretailscience.plots.waterfall--functionality-details","title":"Functionality Details","text":"<ul> <li>plot(): Generates a waterfall chart from a list of amounts and labels. It supports additional customization for display settings, labels, and source text.</li> <li>format_data_labels(): A helper function used to format the data labels according to the specified format (absolute, percentage, both).</li> </ul>"},{"location":"api/plots/waterfall/#pyretailscience.plots.waterfall.format_data_labels","title":"<code>format_data_labels(amounts, total_change, label_format, decimals)</code>","text":"<p>Format the data labels based on the specified format.</p> Source code in <code>pyretailscience/plots/waterfall.py</code> <pre><code>def format_data_labels(\n    amounts: pd.Series,\n    total_change: float,\n    label_format: str,\n    decimals: int,\n) -&gt; list[str]:\n    \"\"\"Format the data labels based on the specified format.\"\"\"\n    if label_format == \"absolute\":\n        return amounts.apply(lambda x: gu.human_format(x, decimals=decimals + 1))\n\n    if label_format == \"percentage\":\n        return amounts.apply(lambda x: f\"{x / total_change:.0%}\")\n\n    return [f\"{gu.human_format(x, decimals=decimals + 1)} ({x / total_change:.0%})\" for x in amounts]\n</code></pre>"},{"location":"api/plots/waterfall/#pyretailscience.plots.waterfall.plot","title":"<code>plot(amounts, labels, title=None, y_label=None, x_label='', source_text=None, data_label_format=None, display_net_bar=False, display_net_line=False, remove_zero_amounts=True, ax=None, **kwargs)</code>","text":"<p>Generates a waterfall chart.</p> <p>Waterfall plots are particularly good for showing how different things add or subtract from a starting number. For instance: - Changes in sales figures from one period to another - Breakdown of profit margins - Impact of different product categories on overall revenue</p> <p>They are often used to identify key drivers of financial performance, highlight areas for improvement, and communicate complex data stories to stakeholders in an intuitive manner.</p> <p>Parameters:</p> Name Type Description Default <code>amounts</code> <code>list[float]</code> <p>The amounts to plot.</p> required <code>labels</code> <code>list[str]</code> <p>The labels for the amounts.</p> required <code>title</code> <code>str</code> <p>The title of the chart. Defaults to None.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label. Defaults to None.</p> <code>None</code> <code>x_label</code> <code>str</code> <p>The x-axis label. Defaults to None.</p> <code>''</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot. Defaults to None.</p> <code>None</code> <code>data_label_format</code> <code>Literal['absolute', 'percentage', 'both', 'none']</code> <p>The format of the data labels. Defaults to \"absolute\".</p> <code>None</code> <code>display_net_bar</code> <code>bool</code> <p>Whether to display a net bar. Defaults to False.</p> <code>False</code> <code>display_net_line</code> <code>bool</code> <p>Whether to display a net line. Defaults to False.</p> <code>False</code> <code>remove_zero_amounts</code> <code>bool</code> <p>Whether to remove zero amounts from the plot. Defaults to True</p> <code>True</code> <code>ax</code> <code>Axes</code> <p>The matplotlib axes object to plot on. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the Pandas plot function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The matplotlib axes object.</p> Source code in <code>pyretailscience/plots/waterfall.py</code> <pre><code>def plot(\n    amounts: list[float],\n    labels: list[str],\n    title: str | None = None,\n    y_label: str | None = None,\n    x_label: str = \"\",\n    source_text: str | None = None,\n    data_label_format: Literal[\"absolute\", \"percentage\", \"both\"] | None = None,\n    display_net_bar: bool = False,\n    display_net_line: bool = False,\n    remove_zero_amounts: bool = True,\n    ax: Axes | None = None,\n    **kwargs: dict[str, any],\n) -&gt; Axes:\n    \"\"\"Generates a waterfall chart.\n\n    Waterfall plots are particularly good for showing how different things add or subtract from a starting number. For\n    instance:\n    - Changes in sales figures from one period to another\n    - Breakdown of profit margins\n    - Impact of different product categories on overall revenue\n\n    They are often used to identify key drivers of financial performance, highlight areas for improvement, and communicate\n    complex data stories to stakeholders in an intuitive manner.\n\n    Args:\n        amounts (list[float]): The amounts to plot.\n        labels (list[str]): The labels for the amounts.\n        title (str, optional): The title of the chart. Defaults to None.\n        y_label (str, optional): The y-axis label. Defaults to None.\n        x_label (str, optional): The x-axis label. Defaults to None.\n        source_text (str, optional): The source text to add to the plot. Defaults to None.\n        data_label_format (Literal[\"absolute\", \"percentage\", \"both\", \"none\"], optional): The format of the data labels.\n            Defaults to \"absolute\".\n        display_net_bar (bool, optional): Whether to display a net bar. Defaults to False.\n        display_net_line (bool, optional): Whether to display a net line. Defaults to False.\n        remove_zero_amounts (bool, optional): Whether to remove zero amounts from the plot. Defaults to True\n        ax (Axes, optional): The matplotlib axes object to plot on. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the Pandas plot function.\n\n    Returns:\n        Axes: The matplotlib axes object.\n    \"\"\"\n    if len(amounts) != len(labels):\n        raise ValueError(\"The lengths of amounts and labels must be the same.\")\n\n    data_label_format = data_label_format.lower() if data_label_format else None\n    if data_label_format is not None and data_label_format not in [\n        \"absolute\",\n        \"percentage\",\n        \"both\",\n    ]:\n        raise ValueError(\n            \"data_label_format must be either 'absolute', 'percentage', 'both', or None.\",\n        )\n\n    df = pd.DataFrame({\"labels\": labels, \"amounts\": amounts})\n\n    if remove_zero_amounts:\n        df = df[df[\"amounts\"] != 0]\n\n    amount_total = df[\"amounts\"].sum()\n\n    colors = df[\"amounts\"].apply(lambda x: COLORS[\"green\"][500] if x &gt; 0 else COLORS[\"red\"][500]).to_list()\n    bottom = df[\"amounts\"].cumsum().shift(1).fillna(0).to_list()\n\n    if display_net_bar:\n        # Append a row for the net amount\n        df.loc[len(df)] = [\"Net\", amount_total]\n        colors.append(COLORS[\"blue\"][500])\n        bottom.append(0)\n\n    # Create the plot\n    ax = df.plot.bar(\n        x=\"labels\",\n        y=\"amounts\",\n        legend=None,\n        bottom=bottom,\n        color=colors,\n        width=0.8,\n        ax=ax,\n        **kwargs,\n    )\n\n    extra_title_pad = 25 if data_label_format != \"none\" else 0\n    ax = gu.standard_graph_styles(\n        ax,\n        title=title,\n        y_label=gu.not_none(y_label, \"Amounts\"),\n        x_label=x_label,\n        title_pad=GraphStyles.DEFAULT_TITLE_PAD + extra_title_pad,\n    )\n\n    decimals = gu.get_decimals(ax.get_ylim(), ax.get_yticks())\n    ax.yaxis.set_major_formatter(\n        lambda x, pos: gu.human_format(x, pos, decimals=decimals),\n    )\n\n    # Add a black line at the y=0 position\n    ax.axhline(y=0, color=\"black\", linewidth=1, zorder=-1)\n\n    if data_label_format is not None:\n        labels = format_data_labels(\n            df[\"amounts\"],\n            amount_total,\n            data_label_format,\n            decimals,\n        )\n\n        ax.bar_label(\n            ax.containers[0],\n            label_type=\"edge\",\n            labels=labels,\n            padding=5,\n            fontsize=GraphStyles.DEFAULT_BAR_LABEL_FONT_SIZE,\n            fontproperties=GraphStyles.POPPINS_REG,\n        )\n\n    if display_net_line:\n        ax.axhline(y=amount_total, color=\"black\", linewidth=1, linestyle=\"--\")\n\n    if source_text is not None:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    gu.standard_tick_styles(ax)\n\n    return ax\n</code></pre>"},{"location":"api/plots/style/graph_utils/","title":"Graph Utils","text":"<p>Helper functions for styling graphs.</p>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.GraphStyles","title":"<code>GraphStyles</code>","text":"<p>A class to hold the styles for a graph.</p> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>class GraphStyles:\n    \"\"\"A class to hold the styles for a graph.\"\"\"\n\n    POPPINS_BOLD = fm.FontProperties(fname=f\"{ASSETS_PATH}/fonts/Poppins-Bold.ttf\")\n    POPPINS_SEMI_BOLD = fm.FontProperties(fname=f\"{ASSETS_PATH}/fonts/Poppins-SemiBold.ttf\")\n    POPPINS_REG = fm.FontProperties(fname=f\"{ASSETS_PATH}/fonts/Poppins-Regular.ttf\")\n    POPPINS_MED = fm.FontProperties(fname=f\"{ASSETS_PATH}/fonts/Poppins-Medium.ttf\")\n    POPPINS_LIGHT_ITALIC = fm.FontProperties(fname=f\"{ASSETS_PATH}/fonts/Poppins-LightItalic.ttf\")\n\n    DEFAULT_TITLE_FONT_SIZE = 20\n    DEFAULT_SOURCE_FONT_SIZE = 10\n    DEFAULT_AXIS_LABEL_FONT_SIZE = 12\n    DEFAULT_TICK_LABEL_FONT_SIZE = 10\n    DEFAULT_BAR_LABEL_FONT_SIZE = 11\n\n    DEFAULT_AXIS_LABEL_PAD = 10\n    DEFAULT_TITLE_PAD = 10\n\n    DEFAULT_BAR_WIDTH = 0.8\n</code></pre>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.add_regression_line","title":"<code>add_regression_line(ax, color='red', linestyle='--', text_position=0.6, show_equation=True, show_r2=True, **kwargs)</code>","text":"<p>Add a regression line to a plot.</p> <p>This function examines the data in a matplotlib Axes object and adds a linear regression line to it. It can work with both line plots and scatter plots, and can handle both numeric and datetime x-axis values.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The matplotlib axes object containing the plot (line or scatter).</p> required <code>color</code> <code>str</code> <p>Color of the regression line. Defaults to \"red\".</p> <code>'red'</code> <code>linestyle</code> <code>str</code> <p>Style of the regression line. Defaults to \"--\".</p> <code>'--'</code> <code>alpha</code> <code>float</code> <p>Transparency of the regression line. Defaults to 0.8.</p> required <code>linewidth</code> <code>float</code> <p>Width of the regression line. Defaults to 2.0.</p> required <code>label</code> <code>str</code> <p>Label for the regression line in the legend. Defaults to \"Regression Line\".</p> required <code>text_position</code> <code>float</code> <p>Relative position (0-1) for the equation text. Defaults to 0.6.</p> <code>0.6</code> <code>show_equation</code> <code>bool</code> <p>Whether to display the equation on the plot. Defaults to True.</p> <code>True</code> <code>show_r2</code> <code>bool</code> <p>Whether to display the R\u00b2 value on the plot. Defaults to True.</p> <code>True</code> <code>kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the plot function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The matplotlib axes with the regression line added.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the plot contains no visible lines or scatter points.</p> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>def add_regression_line(\n    ax: Axes,\n    color: str = \"red\",\n    linestyle: str = \"--\",\n    text_position: float = 0.6,\n    show_equation: bool = True,\n    show_r2: bool = True,\n    **kwargs: dict[str, any],\n) -&gt; Axes:\n    \"\"\"Add a regression line to a plot.\n\n    This function examines the data in a matplotlib Axes object and adds a linear\n    regression line to it. It can work with both line plots and scatter plots, and\n    can handle both numeric and datetime x-axis values.\n\n    Args:\n        ax (Axes): The matplotlib axes object containing the plot (line or scatter).\n        color (str, optional): Color of the regression line. Defaults to \"red\".\n        linestyle (str, optional): Style of the regression line. Defaults to \"--\".\n        alpha (float, optional): Transparency of the regression line. Defaults to 0.8.\n        linewidth (float, optional): Width of the regression line. Defaults to 2.0.\n        label (str, optional): Label for the regression line in the legend. Defaults to \"Regression Line\".\n        text_position (float, optional): Relative position (0-1) for the equation text. Defaults to 0.6.\n        show_equation (bool, optional): Whether to display the equation on the plot. Defaults to True.\n        show_r2 (bool, optional): Whether to display the R\u00b2 value on the plot. Defaults to True.\n        kwargs: Additional keyword arguments to pass to the plot function.\n\n    Returns:\n        Axes: The matplotlib axes with the regression line added.\n\n    Raises:\n        ValueError: If the plot contains no visible lines or scatter points.\n    \"\"\"\n    # Extract data from the plot\n    x_data, y_data = _extract_plot_data(ax)\n\n    # Convert to numeric data and validate\n    x_numeric, y_numeric = _prepare_numeric_data(x_data, y_data)\n\n    # Calculate linear regression using scipy.stats.linregress\n    slope, intercept, r_value, _, _ = stats.linregress(x_numeric, y_numeric)\n    r_squared = r_value**2\n\n    # Calculate the regression line endpoints\n    y_min = intercept + slope * min(x_numeric)\n    y_max = intercept + slope * max(x_numeric)\n\n    # Plot the regression line\n    x_min, x_max = ax.get_xlim()\n    ax.plot([x_min, x_max], [y_min, y_max], color=color, linestyle=linestyle, **kwargs)\n\n    # Add equation and R\u00b2 text if requested\n    _add_equation_text(ax, slope, intercept, r_squared, color, text_position, show_equation, show_r2)\n\n    return ax\n</code></pre>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.add_source_text","title":"<code>add_source_text(ax, source_text, font_size=GraphStyles.DEFAULT_AXIS_LABEL_FONT_SIZE, vertical_padding=2, is_venn_diagram=False)</code>","text":"<p>Add source text to the bottom left corner of a graph.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The graph to add the source text to.</p> required <code>source_text</code> <code>str</code> <p>The source text.</p> required <code>font_size</code> <code>float</code> <p>The font size of the source text. Defaults to GraphStyles.DEFAULT_AXIS_LABEL_FONT_SIZE.</p> <code>DEFAULT_AXIS_LABEL_FONT_SIZE</code> <code>vertical_padding</code> <code>float</code> <p>The padding in ems below the x-axis label. Defaults to 2.</p> <code>2</code> <code>is_venn_diagram</code> <code>bool</code> <p>Flag to indicate if the diagram is a Venn diagram. If True, <code>x_norm</code> and <code>y_norm</code> will be set to fixed values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Text</code> <code>Text</code> <p>The source text.</p> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>def add_source_text(\n    ax: Axes,\n    source_text: str,\n    font_size: float = GraphStyles.DEFAULT_AXIS_LABEL_FONT_SIZE,\n    vertical_padding: float = 2,\n    is_venn_diagram: bool = False,\n) -&gt; Text:\n    \"\"\"Add source text to the bottom left corner of a graph.\n\n    Args:\n        ax (Axes): The graph to add the source text to.\n        source_text (str): The source text.\n        font_size (float, optional): The font size of the source text.\n            Defaults to GraphStyles.DEFAULT_AXIS_LABEL_FONT_SIZE.\n        vertical_padding (float, optional): The padding in ems below the x-axis label. Defaults to 2.\n        is_venn_diagram (bool, optional): Flag to indicate if the diagram is a Venn diagram.\n            If True, `x_norm` and `y_norm` will be set to fixed values.\n            Defaults to False.\n\n    Returns:\n        Text: The source text.\n    \"\"\"\n    ax.figure.canvas.draw()\n    if is_venn_diagram:\n        x_norm = 0.01\n        y_norm = 0.02\n    else:\n        # Get y coordinate of the text\n        xlabel_box = ax.xaxis.label.get_window_extent(renderer=ax.figure.canvas.get_renderer())\n\n        top_of_label_px = xlabel_box.y0\n\n        padding_px = vertical_padding * font_size\n\n        y_disp = top_of_label_px - padding_px - (xlabel_box.height)\n\n        # Convert display coordinates to normalized figure coordinates\n        y_norm = y_disp / ax.figure.bbox.height\n\n        # Get x coordinate of the text\n        ylabel_box = ax.yaxis.label.get_window_extent(renderer=ax.figure.canvas.get_renderer())\n        title_box = ax.title.get_window_extent(renderer=ax.figure.canvas.get_renderer())\n        min_x0 = min(ylabel_box.x0, title_box.x0)\n        x_norm = ax.figure.transFigure.inverted().transform((min_x0, 0))[0]\n\n    # Add text to the bottom left corner of the figure\n    return ax.figure.text(\n        x_norm,\n        y_norm,\n        source_text,\n        ha=\"left\",\n        va=\"bottom\",\n        transform=ax.figure.transFigure,\n        fontsize=GraphStyles.DEFAULT_SOURCE_FONT_SIZE,\n        fontproperties=GraphStyles.POPPINS_LIGHT_ITALIC,\n        color=\"dimgray\",\n    )\n</code></pre>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.apply_hatches","title":"<code>apply_hatches(ax, num_segments)</code>","text":"<p>Apply hatch patterns to patches in a plot, such as bars, histograms, or area plots.</p> <p>This function divides the patches in the given Axes object into the specified number of segments and applies a different hatch pattern to each segment.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The matplotlib Axes object containing the plot with patches (bars, histograms, etc.).</p> required <code>num_segments</code> <code>int</code> <p>The number of segments to divide the patches into, with each segment receiving a different hatch pattern.</p> required <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The modified Axes object with hatches applied to the patches.</p> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>def apply_hatches(ax: Axes, num_segments: int) -&gt; Axes:\n    \"\"\"Apply hatch patterns to patches in a plot, such as bars, histograms, or area plots.\n\n    This function divides the patches in the given Axes object into the specified\n    number of segments and applies a different hatch pattern to each segment.\n\n    Args:\n        ax (Axes): The matplotlib Axes object containing the plot with patches (bars, histograms, etc.).\n        num_segments (int): The number of segments to divide the patches into, with each segment receiving a different hatch pattern.\n\n    Returns:\n        Axes: The modified Axes object with hatches applied to the patches.\n    \"\"\"\n    available_hatches = _hatches_gen()\n    patch_groups = np.array_split(ax.patches, num_segments)\n    for patch_group in patch_groups:\n        hatch = next(available_hatches)\n        for patch in patch_group:\n            patch.set_hatch(hatch)\n\n    legend = ax.get_legend()\n    if legend:\n        existing_hatches = [patch.get_hatch() for patch in ax.patches if patch.get_hatch() is not None]\n        unique_hatches = [hatch for idx, hatch in enumerate(existing_hatches) if hatch not in existing_hatches[:idx]]\n        for legend_patch, hatch in zip(legend.get_patches(), cycle(unique_hatches)):\n            legend_patch.set_hatch(hatch)\n\n    return ax\n</code></pre>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.get_decimals","title":"<code>get_decimals(ylim, tick_values, max_decimals=10)</code>","text":"<p>Helper function for the <code>human_format</code> function that determines the number of decimals to use for the y-axis.</p> <p>Parameters:</p> Name Type Description Default <code>ylim</code> <code>tuple[float, float]</code> <p>The y-axis limits.</p> required <code>tick_values</code> <code>list[float]</code> <p>The y-axis tick values.</p> required <code>max_decimals</code> <code>int</code> <p>The maximum number of decimals to use. Defaults to 100.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of decimals to use.</p> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>def get_decimals(ylim: tuple[float, float], tick_values: list[float], max_decimals: int = 10) -&gt; int:\n    \"\"\"Helper function for the `human_format` function that determines the number of decimals to use for the y-axis.\n\n    Args:\n        ylim: The y-axis limits.\n        tick_values: The y-axis tick values.\n        max_decimals: The maximum number of decimals to use. Defaults to 100.\n\n    Returns:\n        int: The number of decimals to use.\n    \"\"\"\n    decimals = 0\n    while decimals &lt; max_decimals:\n        tick_labels = [human_format(t, 0, decimals=decimals) for t in tick_values if t &gt;= ylim[0] and t &lt;= ylim[1]]\n        # Ensure no duplicate labels\n        if len(tick_labels) == len(set(tick_labels)):\n            break\n        decimals += 1\n    return decimals\n</code></pre>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.human_format","title":"<code>human_format(num, pos=None, decimals=0, prefix='')</code>","text":"<p>Format a number in a human-readable format for Matplotlib, discarding trailing zeros.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>float</code> <p>The number to format.</p> required <code>pos</code> <code>int</code> <p>The position. Defaults to None. Only used for Matplotlib compatibility.</p> <code>None</code> <code>decimals</code> <code>int</code> <p>The number of decimals. Defaults to 0.</p> <code>0</code> <code>prefix</code> <code>str</code> <p>The prefix of the returned string, eg '$'. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The formatted number, with trailing zeros removed.</p> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>def human_format(\n    num: float,\n    pos: int | None = None,  # noqa: ARG001 (pos is only used for Matplotlib compatibility)\n    decimals: int = 0,\n    prefix: str = \"\",\n) -&gt; str:\n    \"\"\"Format a number in a human-readable format for Matplotlib, discarding trailing zeros.\n\n    Args:\n        num (float): The number to format.\n        pos (int, optional): The position. Defaults to None. Only used for Matplotlib compatibility.\n        decimals (int, optional): The number of decimals. Defaults to 0.\n        prefix (str, optional): The prefix of the returned string, eg '$'. Defaults to \"\".\n\n    Returns:\n        str: The formatted number, with trailing zeros removed.\n    \"\"\"\n    # The minimum difference between two numbers to receive a different suffix\n    minimum_magnitude_difference = 1000.0\n    magnitude = 0\n\n    # Keep dividing by 1000 until the number is small enough\n    while abs(num) &gt;= minimum_magnitude_difference:\n        magnitude += 1\n        num /= minimum_magnitude_difference\n\n    # Check if the number rounds to exactly 1000 at the current magnitude\n    if round(abs(num), decimals) == minimum_magnitude_difference:\n        num /= minimum_magnitude_difference\n        magnitude += 1\n\n    # If magnitude exceeds the predefined suffixes, continue with multiples of \"P\"\n    if magnitude &lt; len(_MAGNITUDE_SUFFIXES):\n        suffix = _MAGNITUDE_SUFFIXES[magnitude]\n    else:\n        # Calculate how many times beyond \"P\" we've gone and append that to \"P\"\n        extra_magnitude = magnitude - (len(_MAGNITUDE_SUFFIXES) - 1)\n        suffix = f\"{1000**extra_magnitude}P\"\n\n    # Format the number and remove trailing zeros\n    formatted_num = f\"{prefix}%.{decimals}f\" % num\n    formatted_num = formatted_num.rstrip(\"0\").rstrip(\".\") if \".\" in formatted_num else formatted_num\n\n    return f\"{formatted_num}{suffix}\"\n</code></pre>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.not_none","title":"<code>not_none(value1, value2)</code>","text":"<p>Helper function that returns the first value that is not None.</p> <p>Parameters:</p> Name Type Description Default <code>value1</code> <code>any</code> <p>The first value.</p> required <code>value2</code> <code>any</code> <p>The second value.</p> required <p>Returns:</p> Type Description <code>any</code> <p>The first value that is not None.</p> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>def not_none(value1: any, value2: any) -&gt; any:\n    \"\"\"Helper function that returns the first value that is not None.\n\n    Args:\n        value1: The first value.\n        value2: The second value.\n\n    Returns:\n        The first value that is not None.\n    \"\"\"\n    if value1 is None:\n        return value2\n    return value1\n</code></pre>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.set_axis_percent","title":"<code>set_axis_percent(fmt_axis, xmax=1, decimals=None, symbol='%')</code>","text":"<p>Format an axis to display values as percentages.</p> <p>This function configures a matplotlib axis to display its tick labels as percentages using matplotlib's PercentFormatter.</p> <p>Parameters:</p> Name Type Description Default <code>fmt_axis</code> <code>YAxis | XAxis</code> <p>The axis to format (either ax.yaxis or ax.xaxis).</p> required <code>xmax</code> <code>float</code> <p>The value that represents 100%. Defaults to 1.</p> <code>1</code> <code>decimals</code> <code>int | None</code> <p>Number of decimal places to include. If None, automatically selects based on data range. Defaults to None.</p> <code>None</code> <code>symbol</code> <code>str | None</code> <p>The symbol to use for percentage. If None, no symbol is displayed. Defaults to \"%\".</p> <code>'%'</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function modifies the axis formatter in place.</p> Example <pre><code>import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot([0, 0.25, 0.5, 0.75, 1.0], [0, 0.3, 0.5, 0.7, 1.0])\n# Format y-axis as percentage\nset_axis_percent(ax.yaxis)\n# Format x-axis as percentage with 1 decimal place\nset_axis_percent(ax.xaxis, decimals=1)\n</code></pre> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>def set_axis_percent(\n    fmt_axis: YAxis | XAxis,\n    xmax: float = 1,\n    decimals: int | None = None,\n    symbol: str | None = \"%\",\n) -&gt; None:\n    \"\"\"Format an axis to display values as percentages.\n\n    This function configures a matplotlib axis to display its tick labels as percentages\n    using matplotlib's PercentFormatter.\n\n    Args:\n        fmt_axis (YAxis | XAxis): The axis to format (either ax.yaxis or ax.xaxis).\n        xmax (float, optional): The value that represents 100%. Defaults to 1.\n        decimals (int | None, optional): Number of decimal places to include. If None,\n            automatically selects based on data range. Defaults to None.\n        symbol (str | None, optional): The symbol to use for percentage. If None,\n            no symbol is displayed. Defaults to \"%\".\n\n    Returns:\n        None: The function modifies the axis formatter in place.\n\n    Example:\n        ```python\n        import matplotlib.pyplot as plt\n        fig, ax = plt.subplots()\n        ax.plot([0, 0.25, 0.5, 0.75, 1.0], [0, 0.3, 0.5, 0.7, 1.0])\n        # Format y-axis as percentage\n        set_axis_percent(ax.yaxis)\n        # Format x-axis as percentage with 1 decimal place\n        set_axis_percent(ax.xaxis, decimals=1)\n        ```\n    \"\"\"\n    return fmt_axis.set_major_formatter(mtick.PercentFormatter(xmax=xmax, decimals=decimals, symbol=symbol))\n</code></pre>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.standard_graph_styles","title":"<code>standard_graph_styles(ax, title=None, x_label=None, y_label=None, title_pad=GraphStyles.DEFAULT_TITLE_PAD, x_label_pad=GraphStyles.DEFAULT_AXIS_LABEL_PAD, y_label_pad=GraphStyles.DEFAULT_AXIS_LABEL_PAD, legend_title=None, move_legend_outside=False, show_legend=True)</code>","text":"<p>Apply standard styles to a Matplotlib graph.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The graph to apply the styles to.</p> required <code>title</code> <code>str</code> <p>The title of the graph. Defaults to None.</p> <code>None</code> <code>x_label</code> <code>str</code> <p>The x-axis label. Defaults to None.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label. Defaults to None.</p> <code>None</code> <code>title_pad</code> <code>int</code> <p>The padding above the title. Defaults to GraphStyles.DEFAULT_TITLE_PAD.</p> <code>DEFAULT_TITLE_PAD</code> <code>x_label_pad</code> <code>int</code> <p>The padding below the x-axis label. Defaults to GraphStyles.DEFAULT_AXIS_LABEL_PAD.</p> <code>DEFAULT_AXIS_LABEL_PAD</code> <code>y_label_pad</code> <code>int</code> <p>The padding to the left of the y-axis label. Defaults to GraphStyles.DEFAULT_AXIS_LABEL_PAD.</p> <code>DEFAULT_AXIS_LABEL_PAD</code> <code>legend_title</code> <code>str</code> <p>The title of the legend. If None, no legend title is applied. Defaults to None.</p> <code>None</code> <code>move_legend_outside</code> <code>bool</code> <p>Whether to move the legend outside the plot. Defaults to False.</p> <code>False</code> <code>show_legend</code> <code>bool</code> <p>Whether to display the legend or not.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The graph with the styles applied.</p> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>def standard_graph_styles(\n    ax: Axes,\n    title: str | None = None,\n    x_label: str | None = None,\n    y_label: str | None = None,\n    title_pad: int = GraphStyles.DEFAULT_TITLE_PAD,\n    x_label_pad: int = GraphStyles.DEFAULT_AXIS_LABEL_PAD,\n    y_label_pad: int = GraphStyles.DEFAULT_AXIS_LABEL_PAD,\n    legend_title: str | None = None,\n    move_legend_outside: bool = False,\n    show_legend: bool = True,\n) -&gt; Axes:\n    \"\"\"Apply standard styles to a Matplotlib graph.\n\n    Args:\n        ax (Axes): The graph to apply the styles to.\n        title (str, optional): The title of the graph. Defaults to None.\n        x_label (str, optional): The x-axis label. Defaults to None.\n        y_label (str, optional): The y-axis label. Defaults to None.\n        title_pad (int, optional): The padding above the title. Defaults to GraphStyles.DEFAULT_TITLE_PAD.\n        x_label_pad (int, optional): The padding below the x-axis label. Defaults to GraphStyles.DEFAULT_AXIS_LABEL_PAD.\n        y_label_pad (int, optional): The padding to the left of the y-axis label. Defaults to\n            GraphStyles.DEFAULT_AXIS_LABEL_PAD.\n        legend_title (str, optional): The title of the legend. If None, no legend title is applied. Defaults to None.\n        move_legend_outside (bool, optional): Whether to move the legend outside the plot. Defaults to False.\n        show_legend (bool): Whether to display the legend or not.\n\n    Returns:\n        Axes: The graph with the styles applied.\n    \"\"\"\n    ax.set_facecolor(\"w\")  # set background color to white\n    ax.set_axisbelow(True)  # set grid lines behind the plot\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\n    ax.grid(which=\"major\", axis=\"x\", color=\"#DAD8D7\", alpha=0.5, zorder=1)\n    ax.grid(which=\"major\", axis=\"y\", color=\"#DAD8D7\", alpha=0.5, zorder=1)\n\n    if title is not None:\n        ax.set_title(\n            title,\n            fontproperties=GraphStyles.POPPINS_SEMI_BOLD,\n            fontsize=GraphStyles.DEFAULT_TITLE_FONT_SIZE,\n            pad=title_pad,\n        )\n\n    if x_label is not None:\n        ax.set_xlabel(\n            x_label,\n            fontproperties=GraphStyles.POPPINS_REG,\n            fontsize=GraphStyles.DEFAULT_AXIS_LABEL_FONT_SIZE,\n            labelpad=x_label_pad,\n        )\n\n    if y_label is not None:\n        ax.set_ylabel(\n            y_label,\n            fontproperties=GraphStyles.POPPINS_REG,\n            fontsize=GraphStyles.DEFAULT_AXIS_LABEL_FONT_SIZE,\n            labelpad=y_label_pad,\n        )\n\n    if not show_legend:\n        return ax\n\n    return _add_legend(\n        ax=ax,\n        legend_title=legend_title,\n        move_legend_outside=move_legend_outside,\n    )\n</code></pre>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.standard_tick_styles","title":"<code>standard_tick_styles(ax)</code>","text":"<p>Apply standard tick styles to a Matplotlib graph.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The graph to apply the styles to.</p> required <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>The graph with the styles applied.</p> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>def standard_tick_styles(ax: Axes) -&gt; Axes:\n    \"\"\"Apply standard tick styles to a Matplotlib graph.\n\n    Args:\n        ax (Axes): The graph to apply the styles to.\n\n    Returns:\n        Axes: The graph with the styles applied.\n    \"\"\"\n    for tick in ax.get_xticklabels():\n        tick.set_fontproperties(GraphStyles.POPPINS_REG)\n        tick.set_fontsize(GraphStyles.DEFAULT_TICK_LABEL_FONT_SIZE)\n    for tick in ax.get_yticklabels():\n        tick.set_fontproperties(GraphStyles.POPPINS_REG)\n        tick.set_fontsize(GraphStyles.DEFAULT_TICK_LABEL_FONT_SIZE)\n\n    return ax\n</code></pre>"},{"location":"api/plots/style/graph_utils/#pyretailscience.style.graph_utils.truncate_to_x_digits","title":"<code>truncate_to_x_digits(num_str, digits)</code>","text":"<p>Truncate a human-formatted number to the first <code>num_digits</code> significant digits.</p> <p>Parameters:</p> Name Type Description Default <code>num_str</code> <code>str</code> <p>The formatted number (e.g., '999.999K').</p> required <code>digits</code> <code>int</code> <p>The number of digits to keep.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The truncated formatted number (e.g., '999.9K').</p> Source code in <code>pyretailscience/style/graph_utils.py</code> <pre><code>def truncate_to_x_digits(num_str: str, digits: int) -&gt; str:\n    \"\"\"Truncate a human-formatted number to the first `num_digits` significant digits.\n\n    Args:\n        num_str (str): The formatted number (e.g., '999.999K').\n        digits (int): The number of digits to keep.\n\n    Returns:\n        str: The truncated formatted number (e.g., '999.9K').\n    \"\"\"\n    # Split the number part and the suffix (e.g., \"999.999K\" -&gt; \"999.999\" and \"K\")\n    suffix = \"\"\n    for s in _MAGNITUDE_SUFFIXES:\n        if num_str.endswith(s) and s != \"\":\n            suffix = s\n            num_str = num_str[: -len(s)]  # Remove the suffix for now\n            break\n\n    # Handle negative numbers\n    is_negative = num_str.startswith(\"-\")\n    if is_negative:\n        num_str = num_str[1:]  # Remove the negative sign for now\n\n    # Handle zero case explicitly\n    if float(num_str) == 0:\n        return f\"0{suffix}\"\n\n    # Handle small numbers explicitly to avoid scientific notation\n    scientific_notation_threshold = 1e-4\n    if abs(float(num_str)) &lt; scientific_notation_threshold:\n        return f\"{float(num_str):.{digits}f}\".rstrip(\"0\").rstrip(\".\")\n\n    digits_before_decimal = len(num_str.split(\".\")[0])\n    # Calculate how many digits to keep after the decimal\n    digits_to_keep_after_decimal = digits - digits_before_decimal\n\n    # Ensure truncation without rounding\n    if digits_to_keep_after_decimal &gt; 0:\n        factor = 10**digits_to_keep_after_decimal\n        truncated_num = str(int(float(num_str) * factor) / factor)\n    else:\n        factor = 10**digits\n        truncated_num = str(int(float(num_str) * factor) / factor)\n\n    # Reapply the negative sign if needed\n    if is_negative:\n        truncated_num = f\"-{truncated_num}\"\n\n    # Remove unnecessary trailing zeros and decimal point\n    truncated_num = truncated_num.rstrip(\"0\").rstrip(\".\")\n\n    return f\"{truncated_num}{suffix}\"\n</code></pre>"},{"location":"api/plots/style/tailwind/","title":"Tailwind Colors","text":"<p>Tailwind CSS color Palettes and helper functions.</p> <p>PyRetailScience includes the raw Tailwind CSS color palettes and ListedColormaps and LinearSegmentedColormaps versions for use when charting.</p> <p>Colors from Tailwind CSS https://raw.githubusercontent.com/tailwindlabs/tailwindcss/a1e74f055b13a7ef5775bdd72a77a4d397565016/src/public/colors.js</p>"},{"location":"api/plots/style/tailwind/#pyretailscience.style.tailwind.get_base_cmap","title":"<code>get_base_cmap()</code>","text":"<p>Returns a ListedColormap with all the Tailwind colors.</p> <p>Returns:</p> Name Type Description <code>ListedColormap</code> <code>ListedColormap</code> <p>A ListedColormap with all the Tailwind colors.</p> Source code in <code>pyretailscience/style/tailwind.py</code> <pre><code>def get_base_cmap() -&gt; ListedColormap:\n    \"\"\"Returns a ListedColormap with all the Tailwind colors.\n\n    Returns:\n        ListedColormap: A ListedColormap with all the Tailwind colors.\n    \"\"\"\n    color_order = [\n        \"red\",\n        \"orange\",\n        \"yellow\",\n        \"green\",\n        \"teal\",\n        \"sky\",\n        \"indigo\",\n        \"purple\",\n        \"pink\",\n        \"slate\",\n        \"amber\",\n        \"lime\",\n        \"emerald\",\n        \"cyan\",\n        \"blue\",\n        \"violet\",\n        \"fuchsia\",\n        \"rose\",\n    ]\n    color_numbers = [500, 300, 700]\n    colors = [COLORS[color][color_number] for color_number in color_numbers for color in color_order]\n\n    return ListedColormap(colors)\n</code></pre>"},{"location":"api/plots/style/tailwind/#pyretailscience.style.tailwind.get_color_list","title":"<code>get_color_list(name, starting_color_code=50, ending_color_code=950)</code>","text":"<p>Returns a filtered list of colors from the Tailwind color palette based on the given range.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the color palette (e.g., \"blue\", \"red\").</p> required <code>starting_color_code</code> <code>int</code> <p>The lowest color shade to use (default: 50).</p> <code>50</code> <code>ending_color_code</code> <code>int</code> <p>The highest color shade to use (default: 950).</p> <code>950</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A filtered list of colors from the Tailwind color palette.</p> Source code in <code>pyretailscience/style/tailwind.py</code> <pre><code>def get_color_list(name: str, starting_color_code: int = 50, ending_color_code: int = 950) -&gt; list[str]:\n    \"\"\"Returns a filtered list of colors from the Tailwind color palette based on the given range.\n\n    Args:\n        name (str): The name of the color palette (e.g., \"blue\", \"red\").\n        starting_color_code (int): The lowest color shade to use (default: 50).\n        ending_color_code (int): The highest color shade to use (default: 950).\n\n    Returns:\n        list[str]: A filtered list of colors from the Tailwind color palette.\n    \"\"\"\n    if name not in COLORS:\n        msg = f\"Color pallete {name} not found. Available color palettes are: {', '.join(COLORS.keys())}.\"\n        raise ValueError(msg)\n    return [COLORS[name][key] for key in sorted(COLORS[name].keys()) if starting_color_code &lt;= key &lt;= ending_color_code]\n</code></pre>"},{"location":"api/plots/style/tailwind/#pyretailscience.style.tailwind.get_linear_cmap","title":"<code>get_linear_cmap(name, starting_color_code=50, ending_color_code=950)</code>","text":"<p>Returns a linear segmented colormap using Tailwind colors.</p> <p>This function allows restricting the color range used in the colormap.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the Tailwind color (e.g., \"blue\", \"red\").</p> required <code>starting_color_code</code> <code>int</code> <p>The lowest color shade to use (default: 50).</p> <code>50</code> <code>ending_color_code</code> <code>int</code> <p>The highest color shade to use (default: 950).</p> <code>950</code> <p>Returns:</p> Name Type Description <code>LinearSegmentedColormap</code> <code>LinearSegmentedColormap</code> <p>A colormap object for matplotlib.</p> Source code in <code>pyretailscience/style/tailwind.py</code> <pre><code>def get_linear_cmap(name: str, starting_color_code: int = 50, ending_color_code: int = 950) -&gt; LinearSegmentedColormap:\n    \"\"\"Returns a linear segmented colormap using Tailwind colors.\n\n    This function allows restricting the color range used in the colormap.\n\n    Args:\n        name (str): The name of the Tailwind color (e.g., \"blue\", \"red\").\n        starting_color_code (int): The lowest color shade to use (default: 50).\n        ending_color_code (int): The highest color shade to use (default: 950).\n\n    Returns:\n        LinearSegmentedColormap: A colormap object for matplotlib.\n    \"\"\"\n    return LinearSegmentedColormap.from_list(\n        f\"{name}_linear_colormap\",\n        get_color_list(name, starting_color_code, ending_color_code),\n    )\n</code></pre>"},{"location":"api/plots/style/tailwind/#pyretailscience.style.tailwind.get_listed_cmap","title":"<code>get_listed_cmap(name)</code>","text":"<p>Returns a ListedColormap from the Tailwind color pallete of the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the color pallete.</p> required <p>Returns:</p> Name Type Description <code>ListedColormap</code> <code>ListedColormap</code> <p>The color pallete as a ListedColormap.</p> Source code in <code>pyretailscience/style/tailwind.py</code> <pre><code>def get_listed_cmap(name: str) -&gt; ListedColormap:\n    \"\"\"Returns a ListedColormap from the Tailwind color pallete of the given name.\n\n    Args:\n        name (str): The name of the color pallete.\n\n    Returns:\n        ListedColormap: The color pallete as a ListedColormap.\n    \"\"\"\n    return ListedColormap(get_color_list(name))\n</code></pre>"},{"location":"api/plots/style/tailwind/#pyretailscience.style.tailwind.get_multi_color_cmap","title":"<code>get_multi_color_cmap()</code>","text":"<p>Returns a generator for multiple Tailwind colors and shades.</p> <p>Returns:</p> Name Type Description <code>Generator</code> <code>Generator[str, None, None]</code> <p>A generator yielding the required colors in a looping fashion.</p> Source code in <code>pyretailscience/style/tailwind.py</code> <pre><code>def get_multi_color_cmap() -&gt; Generator[str, None, None]:\n    \"\"\"Returns a generator for multiple Tailwind colors and shades.\n\n    Returns:\n        Generator: A generator yielding the required colors in a looping fashion.\n    \"\"\"\n    color_order = [\"green\", \"blue\", \"red\", \"orange\", \"yellow\", \"violet\", \"pink\"]\n    color_numbers = [500, 300, 700]\n\n    return cycle([COLORS[color][color_number] for color_number in color_numbers for color in color_order])\n</code></pre>"},{"location":"api/plots/style/tailwind/#pyretailscience.style.tailwind.get_single_color_cmap","title":"<code>get_single_color_cmap()</code>","text":"<p>Returns a generator for Tailwind green shades.</p> <p>Returns:</p> Name Type Description <code>Generator</code> <code>Generator[str, None, None]</code> <p>A generator yielding green shades in a looping fashion.</p> Source code in <code>pyretailscience/style/tailwind.py</code> <pre><code>def get_single_color_cmap() -&gt; Generator[str, None, None]:\n    \"\"\"Returns a generator for Tailwind green shades.\n\n    Returns:\n        Generator: A generator yielding green shades in a looping fashion.\n    \"\"\"\n    color_numbers = [500, 300, 700]\n    return cycle([COLORS[\"green\"][shade] for shade in color_numbers])\n</code></pre>"},{"location":"api/segmentation/base/","title":"Base Segmentation","text":"<p>This module provides a base class for segmenting customers based on their spend and transaction statistics.</p>"},{"location":"api/segmentation/base/#pyretailscience.segmentation.base.BaseSegmentation","title":"<code>BaseSegmentation</code>","text":"<p>A base class for customer segmentation.</p> Source code in <code>pyretailscience/segmentation/base.py</code> <pre><code>class BaseSegmentation:\n    \"\"\"A base class for customer segmentation.\"\"\"\n\n    def add_segment(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Adds the segment to the dataframe based on the customer_id column.\n\n        Args:\n            df (pd.DataFrame): The dataframe to add the segment to. The dataframe must have a customer_id column.\n\n        Returns:\n            pd.DataFrame: The dataframe with the segment added.\n\n        Raises:\n            ValueError: If the number of rows before and after the merge do not match.\n        \"\"\"\n        rows_before = len(df)\n        df = df.merge(\n            self.df[\"segment_name\"],\n            how=\"left\",\n            left_on=get_option(\"column.customer_id\"),\n            right_index=True,\n        )\n        rows_after = len(df)\n        if rows_before != rows_after:\n            raise ValueError(\"The number of rows before and after the merge do not match. This should not happen.\")\n\n        return df\n</code></pre>"},{"location":"api/segmentation/base/#pyretailscience.segmentation.base.BaseSegmentation.add_segment","title":"<code>add_segment(df)</code>","text":"<p>Adds the segment to the dataframe based on the customer_id column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to add the segment to. The dataframe must have a customer_id column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The dataframe with the segment added.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of rows before and after the merge do not match.</p> Source code in <code>pyretailscience/segmentation/base.py</code> <pre><code>def add_segment(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Adds the segment to the dataframe based on the customer_id column.\n\n    Args:\n        df (pd.DataFrame): The dataframe to add the segment to. The dataframe must have a customer_id column.\n\n    Returns:\n        pd.DataFrame: The dataframe with the segment added.\n\n    Raises:\n        ValueError: If the number of rows before and after the merge do not match.\n    \"\"\"\n    rows_before = len(df)\n    df = df.merge(\n        self.df[\"segment_name\"],\n        how=\"left\",\n        left_on=get_option(\"column.customer_id\"),\n        right_index=True,\n    )\n    rows_after = len(df)\n    if rows_before != rows_after:\n        raise ValueError(\"The number of rows before and after the merge do not match. This should not happen.\")\n\n    return df\n</code></pre>"},{"location":"api/segmentation/hml/","title":"HML Segmentation","text":"<p>This module provides the <code>HMLSegmentation</code> class for categorizing customers into spend-based segments.</p> <p>HMLSegmentation extends <code>ThresholdSegmentation</code> and classifies customers into Heavy, Medium, Light, and optionally Zero spenders based on the Pareto principle (80/20 rule). It is commonly used in retail to analyze customer spending behavior and optimize marketing strategies.</p>"},{"location":"api/segmentation/hml/#pyretailscience.segmentation.hml.HMLSegmentation","title":"<code>HMLSegmentation</code>","text":"<p>             Bases: <code>ThresholdSegmentation</code></p> <p>Segments customers into Heavy, Medium, Light and Zero spenders based on the total spend.</p> Source code in <code>pyretailscience/segmentation/hml.py</code> <pre><code>class HMLSegmentation(ThresholdSegmentation):\n    \"\"\"Segments customers into Heavy, Medium, Light and Zero spenders based on the total spend.\"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame | ibis.Table,\n        value_col: str | None = None,\n        agg_func: str = \"sum\",\n        zero_value_customers: Literal[\"separate_segment\", \"exclude\", \"include_with_light\"] = \"separate_segment\",\n    ) -&gt; None:\n        \"\"\"Segments customers into Heavy, Medium, Light and Zero spenders based on the total spend.\n\n        HMLSegmentation is a subclass of ThresholdSegmentation and based around an industry standard definition. The\n        thresholds for Heavy (top 20%), Medium (next 30%) and Light (bottom 50%) are chosen based on the pareto\n        distribution, commonly know as the 80/20 rule. It is typically used in retail to segment customers based on\n        their spend, transaction volume or quantities purchased.\n\n        Args:\n            df (pd.DataFrame): A dataframe with the transaction data. The dataframe must contain a customer_id column.\n            value_col (str, optional): The column to use for the segmentation. Defaults to get_option(\"column.unit_spend\").\n            agg_func (str, optional): The aggregation function to use when grouping by customer_id. Defaults to \"sum\".\n            zero_value_customers (Literal[\"separate_segment\", \"exclude\", \"include_with_light\"], optional): How to handle\n                customers with zero spend. Defaults to \"separate_segment\".\n        \"\"\"\n        thresholds = [0.500, 0.800, 1]\n        segments = [\"Light\", \"Medium\", \"Heavy\"]\n        super().__init__(\n            df=df,\n            value_col=value_col,\n            agg_func=agg_func,\n            thresholds=thresholds,\n            segments=segments,\n            zero_value_customers=zero_value_customers,\n        )\n</code></pre>"},{"location":"api/segmentation/hml/#pyretailscience.segmentation.hml.HMLSegmentation.__init__","title":"<code>__init__(df, value_col=None, agg_func='sum', zero_value_customers='separate_segment')</code>","text":"<p>Segments customers into Heavy, Medium, Light and Zero spenders based on the total spend.</p> <p>HMLSegmentation is a subclass of ThresholdSegmentation and based around an industry standard definition. The thresholds for Heavy (top 20%), Medium (next 30%) and Light (bottom 50%) are chosen based on the pareto distribution, commonly know as the 80/20 rule. It is typically used in retail to segment customers based on their spend, transaction volume or quantities purchased.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe with the transaction data. The dataframe must contain a customer_id column.</p> required <code>value_col</code> <code>str</code> <p>The column to use for the segmentation. Defaults to get_option(\"column.unit_spend\").</p> <code>None</code> <code>agg_func</code> <code>str</code> <p>The aggregation function to use when grouping by customer_id. Defaults to \"sum\".</p> <code>'sum'</code> <code>zero_value_customers</code> <code>Literal['separate_segment', 'exclude', 'include_with_light']</code> <p>How to handle customers with zero spend. Defaults to \"separate_segment\".</p> <code>'separate_segment'</code> Source code in <code>pyretailscience/segmentation/hml.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame | ibis.Table,\n    value_col: str | None = None,\n    agg_func: str = \"sum\",\n    zero_value_customers: Literal[\"separate_segment\", \"exclude\", \"include_with_light\"] = \"separate_segment\",\n) -&gt; None:\n    \"\"\"Segments customers into Heavy, Medium, Light and Zero spenders based on the total spend.\n\n    HMLSegmentation is a subclass of ThresholdSegmentation and based around an industry standard definition. The\n    thresholds for Heavy (top 20%), Medium (next 30%) and Light (bottom 50%) are chosen based on the pareto\n    distribution, commonly know as the 80/20 rule. It is typically used in retail to segment customers based on\n    their spend, transaction volume or quantities purchased.\n\n    Args:\n        df (pd.DataFrame): A dataframe with the transaction data. The dataframe must contain a customer_id column.\n        value_col (str, optional): The column to use for the segmentation. Defaults to get_option(\"column.unit_spend\").\n        agg_func (str, optional): The aggregation function to use when grouping by customer_id. Defaults to \"sum\".\n        zero_value_customers (Literal[\"separate_segment\", \"exclude\", \"include_with_light\"], optional): How to handle\n            customers with zero spend. Defaults to \"separate_segment\".\n    \"\"\"\n    thresholds = [0.500, 0.800, 1]\n    segments = [\"Light\", \"Medium\", \"Heavy\"]\n    super().__init__(\n        df=df,\n        value_col=value_col,\n        agg_func=agg_func,\n        thresholds=thresholds,\n        segments=segments,\n        zero_value_customers=zero_value_customers,\n    )\n</code></pre>"},{"location":"api/segmentation/rfm/","title":"RFM Segmentation","text":"<p>Customer Segmentation Using RFM Analysis.</p> <p>This module implements RFM (Recency, Frequency, Monetary) segmentation, a widely used technique in customer analytics to categorize customers based on their purchasing behavior.</p> <p>RFM segmentation assigns scores to customers based on: 1. Recency (R): How recently a customer made a purchase. 2. Frequency (F): How often a customer makes purchases. 3. Monetary (M): The total amount spent by a customer.</p>"},{"location":"api/segmentation/rfm/#pyretailscience.segmentation.rfm--benefits-of-rfm-segmentation","title":"Benefits of RFM Segmentation:","text":"<ul> <li>Customer Value Analysis: Identifies high-value customers who contribute the most revenue.</li> <li>Personalized Marketing: Enables targeted campaigns based on customer purchasing behavior.</li> <li>Customer Retention Strategies: Helps recognize at-risk customers and develop engagement strategies.</li> <li>Sales Forecasting: Provides insights into future revenue trends based on past spending behavior.</li> </ul>"},{"location":"api/segmentation/rfm/#pyretailscience.segmentation.rfm--scoring-methodology","title":"Scoring Methodology:","text":"<ul> <li>Each metric (R, F, M) is divided into 10 bins (0-9) using the NTILE(10) function.</li> <li>A higher score indicates a better customer (e.g., lower recency, higher frequency, and monetary value).</li> <li>The final RFM segment is computed as <code>R*100 + F*10 + M</code>, providing a unique customer classification.</li> </ul> <p>This module leverages <code>pandas</code> and <code>ibis</code> for efficient data processing and integrates with retail analytics workflows to enhance customer insights and business decision-making.</p>"},{"location":"api/segmentation/rfm/#pyretailscience.segmentation.rfm.RFMSegmentation","title":"<code>RFMSegmentation</code>","text":"<p>Segments customers using the RFM (Recency, Frequency, Monetary) methodology.</p> <p>Customers are scored on three dimensions: - Recency (R): Days since the last transaction (lower is better). - Frequency (F): Number of unique transactions (higher is better). - Monetary (M): Total amount spent (higher is better).</p> <p>Each metric is ranked into 10 bins (0-9) using NTILE(10) where, - 9 represents the best score (top 10% of customers). - 0 represents the lowest score (bottom 10% of customers). The RFM segment is a 3-digit number (R100 + F10 + M), representing customer value.</p> Source code in <code>pyretailscience/segmentation/rfm.py</code> <pre><code>class RFMSegmentation:\n    \"\"\"Segments customers using the RFM (Recency, Frequency, Monetary) methodology.\n\n    Customers are scored on three dimensions:\n    - Recency (R): Days since the last transaction (lower is better).\n    - Frequency (F): Number of unique transactions (higher is better).\n    - Monetary (M): Total amount spent (higher is better).\n\n    Each metric is ranked into 10 bins (0-9) using NTILE(10) where,\n    - 9 represents the best score (top 10% of customers).\n    - 0 represents the lowest score (bottom 10% of customers).\n    The RFM segment is a 3-digit number (R*100 + F*10 + M), representing customer value.\n    \"\"\"\n\n    _df: pd.DataFrame | None = None\n\n    def __init__(self, df: pd.DataFrame | ibis.Table, current_date: str | datetime.date | None = None) -&gt; None:\n        \"\"\"Initializes the RFM segmentation process.\n\n        Args:\n            df (pd.DataFrame | ibis.Table): A DataFrame or Ibis table containing transaction data.\n                Must include the following columns:\n                - customer_id\n                - transaction_date\n                - unit_spend\n                - transaction_id\n            current_date (Optional[Union[str, datetime.date]]): The reference date for calculating recency.\n                Can be a string (format: \"YYYY-MM-DD\"), a date object, or None (defaults to the current system date).\n\n        Raises:\n            ValueError: If the dataframe is missing required columns.\n            TypeError: If the input data is not a pandas DataFrame or an Ibis Table.\n        \"\"\"\n        cols = ColumnHelper()\n        required_cols = [\n            cols.customer_id,\n            cols.transaction_date,\n            cols.unit_spend,\n            cols.transaction_id,\n        ]\n        if isinstance(df, pd.DataFrame):\n            df = ibis.memtable(df)\n        elif not isinstance(df, ibis.Table):\n            raise TypeError(\"df must be either a pandas DataFrame or an Ibis Table\")\n\n        missing_cols = set(required_cols) - set(df.columns)\n        if missing_cols:\n            error_message = f\"Missing required columns: {missing_cols}\"\n            raise ValueError(error_message)\n\n        if isinstance(current_date, str):\n            current_date = datetime.date.fromisoformat(current_date)\n        elif current_date is None:\n            current_date = datetime.datetime.now(datetime.UTC).date()\n        elif not isinstance(current_date, datetime.date):\n            raise TypeError(\"current_date must be a string in 'YYYY-MM-DD' format, a datetime.date object, or None\")\n\n        self.table = self._compute_rfm(df, current_date)\n\n    def _compute_rfm(self, df: ibis.Table, current_date: datetime.date) -&gt; ibis.Table:\n        \"\"\"Computes the RFM metrics and segments customers accordingly.\n\n        Args:\n            df (ibis.Table): The transaction data table.\n            current_date (datetime.date): The reference date for calculating recency.\n\n        Returns:\n            ibis.Table: A table with RFM scores and segment values.\n        \"\"\"\n        cols = ColumnHelper()\n        current_date_expr = ibis.literal(current_date)\n\n        customer_metrics = df.group_by(cols.customer_id).aggregate(\n            recency_days=current_date_expr.delta(df[cols.transaction_date].max().cast(\"date\"), unit=\"day\").cast(\n                \"int32\",\n            ),\n            frequency=df[cols.transaction_id].nunique(),\n            monetary=df[cols.unit_spend].sum(),\n        )\n\n        window_recency = ibis.window(\n            order_by=[ibis.asc(customer_metrics.recency_days), ibis.asc(customer_metrics.customer_id)],\n        )\n        window_frequency = ibis.window(\n            order_by=[ibis.asc(customer_metrics.frequency), ibis.asc(customer_metrics.customer_id)],\n        )\n        window_monetary = ibis.window(\n            order_by=[ibis.asc(customer_metrics.monetary), ibis.asc(customer_metrics.customer_id)],\n        )\n\n        rfm_scores = customer_metrics.mutate(\n            r_score=(ibis.ntile(10).over(window_recency)),\n            f_score=(ibis.ntile(10).over(window_frequency)),\n            m_score=(ibis.ntile(10).over(window_monetary)),\n        )\n\n        return rfm_scores.mutate(\n            rfm_segment=(rfm_scores.r_score * 100 + rfm_scores.f_score * 10 + rfm_scores.m_score),\n            fm_segment=(rfm_scores.f_score * 10 + rfm_scores.m_score),\n        )\n\n    @property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"Returns the dataframe with the segment names.\"\"\"\n        if self._df is None:\n            self._df = self.table.execute().set_index(get_option(\"column.customer_id\"))\n        return self._df\n</code></pre>"},{"location":"api/segmentation/rfm/#pyretailscience.segmentation.rfm.RFMSegmentation.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Returns the dataframe with the segment names.</p>"},{"location":"api/segmentation/rfm/#pyretailscience.segmentation.rfm.RFMSegmentation.__init__","title":"<code>__init__(df, current_date=None)</code>","text":"<p>Initializes the RFM segmentation process.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame | Table</code> <p>A DataFrame or Ibis table containing transaction data. Must include the following columns: - customer_id - transaction_date - unit_spend - transaction_id</p> required <code>current_date</code> <code>Optional[Union[str, date]]</code> <p>The reference date for calculating recency. Can be a string (format: \"YYYY-MM-DD\"), a date object, or None (defaults to the current system date).</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataframe is missing required columns.</p> <code>TypeError</code> <p>If the input data is not a pandas DataFrame or an Ibis Table.</p> Source code in <code>pyretailscience/segmentation/rfm.py</code> <pre><code>def __init__(self, df: pd.DataFrame | ibis.Table, current_date: str | datetime.date | None = None) -&gt; None:\n    \"\"\"Initializes the RFM segmentation process.\n\n    Args:\n        df (pd.DataFrame | ibis.Table): A DataFrame or Ibis table containing transaction data.\n            Must include the following columns:\n            - customer_id\n            - transaction_date\n            - unit_spend\n            - transaction_id\n        current_date (Optional[Union[str, datetime.date]]): The reference date for calculating recency.\n            Can be a string (format: \"YYYY-MM-DD\"), a date object, or None (defaults to the current system date).\n\n    Raises:\n        ValueError: If the dataframe is missing required columns.\n        TypeError: If the input data is not a pandas DataFrame or an Ibis Table.\n    \"\"\"\n    cols = ColumnHelper()\n    required_cols = [\n        cols.customer_id,\n        cols.transaction_date,\n        cols.unit_spend,\n        cols.transaction_id,\n    ]\n    if isinstance(df, pd.DataFrame):\n        df = ibis.memtable(df)\n    elif not isinstance(df, ibis.Table):\n        raise TypeError(\"df must be either a pandas DataFrame or an Ibis Table\")\n\n    missing_cols = set(required_cols) - set(df.columns)\n    if missing_cols:\n        error_message = f\"Missing required columns: {missing_cols}\"\n        raise ValueError(error_message)\n\n    if isinstance(current_date, str):\n        current_date = datetime.date.fromisoformat(current_date)\n    elif current_date is None:\n        current_date = datetime.datetime.now(datetime.UTC).date()\n    elif not isinstance(current_date, datetime.date):\n        raise TypeError(\"current_date must be a string in 'YYYY-MM-DD' format, a datetime.date object, or None\")\n\n    self.table = self._compute_rfm(df, current_date)\n</code></pre>"},{"location":"api/segmentation/segstats/","title":"SegTransactionStats Segmentation","text":"<p>Module for calculating and visualizing transaction statistics by segment.</p> <p>This module provides the <code>SegTransactionStats</code> class, which allows for the computation of transaction-based statistics grouped by one or more segment columns. The statistics include aggregations such as total spend, unique customers, transactions per customer, and optional custom aggregations.</p> <p>The module supports both Pandas DataFrames and Ibis Tables as input data formats. It also offers visualization capabilities to generate plots of segment-based statistics.</p>"},{"location":"api/segmentation/segstats/#pyretailscience.segmentation.segstats.SegTransactionStats","title":"<code>SegTransactionStats</code>","text":"<p>Calculates transaction statistics by segment.</p> Source code in <code>pyretailscience/segmentation/segstats.py</code> <pre><code>class SegTransactionStats:\n    \"\"\"Calculates transaction statistics by segment.\"\"\"\n\n    _df: pd.DataFrame | None = None\n\n    def __init__(\n        self,\n        data: pd.DataFrame | ibis.Table,\n        segment_col: str | list[str] = \"segment_name\",\n        calc_total: bool = True,\n        extra_aggs: dict[str, tuple[str, str]] | None = None,\n    ) -&gt; None:\n        \"\"\"Calculates transaction statistics by segment.\n\n        Args:\n            data (pd.DataFrame | ibis.Table): The transaction data. The dataframe must contain the columns\n                customer_id, unit_spend and transaction_id. If the dataframe contains the column unit_quantity, then\n                the columns unit_spend and unit_quantity are used to calculate the price_per_unit and\n                units_per_transaction.\n            segment_col (str | list[str], optional): The column or list of columns to use for the segmentation.\n                Defaults to \"segment_name\".\n            calc_total (bool, optional): Whether to include the total row. Defaults to True.\n            extra_aggs (dict[str, tuple[str, str]], optional): Additional aggregations to perform.\n                The keys in the dictionary will be the column names for the aggregation results.\n                The values are tuples with (column_name, aggregation_function), where:\n                - column_name is the name of the column to aggregate\n                - aggregation_function is a string name of an Ibis aggregation function (e.g., \"nunique\", \"sum\")\n                Example: {\"stores\": (\"store_id\", \"nunique\")} would count unique store_ids.\n        \"\"\"\n        cols = ColumnHelper()\n\n        if isinstance(segment_col, str):\n            segment_col = [segment_col]\n        required_cols = [\n            cols.customer_id,\n            cols.unit_spend,\n            cols.transaction_id,\n            *segment_col,\n        ]\n        if cols.unit_qty in data.columns:\n            required_cols.append(cols.unit_qty)\n\n        missing_cols = set(required_cols) - set(data.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        # Validate extra_aggs if provided\n        if extra_aggs:\n            for col_tuple in extra_aggs.values():\n                col, func = col_tuple\n                if col not in data.columns:\n                    msg = f\"Column '{col}' specified in extra_aggs does not exist in the data\"\n                    raise ValueError(msg)\n                if not hasattr(data[col], func):\n                    msg = f\"Aggregation function '{func}' not available for column '{col}'\"\n                    raise ValueError(msg)\n\n        self.segment_col = segment_col\n        self.extra_aggs = {} if extra_aggs is None else extra_aggs\n\n        self.table = self._calc_seg_stats(data, segment_col, calc_total, self.extra_aggs)\n\n    @staticmethod\n    def _get_col_order(include_quantity: bool) -&gt; list[str]:\n        \"\"\"Returns the default column order.\n\n        Columns should be supplied in the same order regardless of the function being called.\n\n        Args:\n            include_quantity (bool): Whether to include the columns related to quantity.\n\n        Returns:\n            list[str]: The default column order.\n        \"\"\"\n        cols = ColumnHelper()\n        col_order = [\n            cols.agg_unit_spend,\n            cols.agg_transaction_id,\n            cols.agg_customer_id,\n            cols.calc_spend_per_cust,\n            cols.calc_spend_per_trans,\n            cols.calc_trans_per_cust,\n            cols.customers_pct,\n        ]\n        if include_quantity:\n            col_order.insert(3, \"units\")\n            col_order.insert(7, cols.calc_units_per_trans)\n            col_order.insert(7, cols.calc_price_per_unit)\n\n        return col_order\n\n    @staticmethod\n    def _calc_seg_stats(\n        data: pd.DataFrame | ibis.Table,\n        segment_col: list[str],\n        calc_total: bool = True,\n        extra_aggs: dict[str, tuple[str, str]] | None = None,\n    ) -&gt; ibis.Table:\n        \"\"\"Calculates the transaction statistics by segment.\n\n        Args:\n            data (pd.DataFrame | ibis.Table): The transaction data.\n            segment_col (list[str]): The columns to use for the segmentation.\n            extra_aggs (dict[str, tuple[str, str]], optional): Additional aggregations to perform.\n            calc_total (bool, optional): Whether to include the total row. Defaults to True.\n                The keys in the dictionary will be the column names for the aggregation results.\n                The values are tuples with (column_name, aggregation_function).\n\n        Returns:\n            pd.DataFrame: The transaction statistics by segment.\n\n        \"\"\"\n        if isinstance(data, pd.DataFrame):\n            data = ibis.memtable(data)\n\n        elif not isinstance(data, ibis.Table):\n            raise TypeError(\"data must be either a pandas DataFrame or an ibis Table\")\n\n        cols = ColumnHelper()\n\n        # Base aggregations for segments\n        aggs = {\n            cols.agg_unit_spend: data[cols.unit_spend].sum(),\n            cols.agg_transaction_id: data[cols.transaction_id].nunique(),\n            cols.agg_customer_id: data[cols.customer_id].nunique(),\n        }\n        if cols.unit_qty in data.columns:\n            aggs[cols.agg_unit_qty] = data[cols.unit_qty].sum()\n\n        # Add extra aggregations if provided\n        if extra_aggs:\n            for agg_name, col_tuple in extra_aggs.items():\n                col, func = col_tuple\n                aggs[agg_name] = getattr(data[col], func)()\n\n        # Calculate metrics for segments\n        segment_metrics = data.group_by(segment_col).aggregate(**aggs)\n        final_metrics = segment_metrics\n\n        if calc_total:\n            total_metrics = data.aggregate(**aggs).mutate({col: ibis.literal(\"Total\") for col in segment_col})\n            final_metrics = ibis.union(segment_metrics, total_metrics)\n\n        total_customers = data[cols.customer_id].nunique()\n\n        # Cross join with total_customers to make it available for percentage calculation\n        final_metrics = final_metrics.mutate(\n            **{\n                cols.calc_spend_per_cust: ibis._[cols.agg_unit_spend] / ibis._[cols.agg_customer_id],\n                cols.calc_spend_per_trans: ibis._[cols.agg_unit_spend] / ibis._[cols.agg_transaction_id],\n                cols.calc_trans_per_cust: ibis._[cols.agg_transaction_id] / ibis._[cols.agg_customer_id].cast(\"float\"),\n                cols.customers_pct: ibis._[cols.agg_customer_id].cast(\"float\") / total_customers,\n            },\n        )\n\n        if cols.unit_qty in data.columns:\n            final_metrics = final_metrics.mutate(\n                **{\n                    cols.calc_price_per_unit: ibis._[cols.agg_unit_spend] / ibis._[cols.agg_unit_qty].nullif(0),\n                    cols.calc_units_per_trans: ibis._[cols.agg_unit_qty]\n                    / ibis._[cols.agg_transaction_id].cast(\"float\"),\n                },\n            )\n        return final_metrics\n\n    @property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"Returns the dataframe with the transaction statistics by segment.\"\"\"\n        if self._df is None:\n            cols = ColumnHelper()\n            col_order = [\n                *self.segment_col,\n                *SegTransactionStats._get_col_order(include_quantity=cols.agg_unit_qty in self.table.columns),\n            ]\n\n            # Add any extra aggregation columns to the column order\n            if hasattr(self, \"extra_aggs\") and self.extra_aggs:\n                col_order.extend(self.extra_aggs.keys())\n\n            self._df = self.table.execute()[col_order]\n        return self._df\n\n    def plot(\n        self,\n        value_col: str,\n        title: str | None = None,\n        x_label: str | None = None,\n        y_label: str | None = None,\n        ax: Axes | None = None,\n        orientation: Literal[\"vertical\", \"horizontal\"] = \"vertical\",\n        sort_order: Literal[\"ascending\", \"descending\", None] = None,\n        source_text: str | None = None,\n        hide_total: bool = True,\n        **kwargs: dict[str, any],\n    ) -&gt; SubplotBase:\n        \"\"\"Plots the value_col by segment.\n\n        Args:\n            value_col (str): The column to plot.\n            title (str, optional): The title of the plot. Defaults to None.\n            x_label (str, optional): The x-axis label. Defaults to None. When None the x-axis label is blank when the\n                orientation is horizontal. When the orientation is vertical it is set to the `value_col` in title case.\n            y_label (str, optional): The y-axis label. Defaults to None. When None the y-axis label is set to the\n                `value_col` in title case when the orientation is horizontal. Then the orientation is vertical it is\n                set to blank\n            ax (Axes, optional): The matplotlib axes object to plot on. Defaults to None.\n            orientation (Literal[\"vertical\", \"horizontal\"], optional): The orientation of the plot. Defaults to\n                \"vertical\".\n            sort_order (Literal[\"ascending\", \"descending\", None], optional): The sort order of the segments.\n                Defaults to None. If None, the segments are plotted in the order they appear in the dataframe.\n            source_text (str, optional): The source text to add to the plot. Defaults to None.\n            hide_total (bool, optional): Whether to hide the total row. Defaults to True.\n            **kwargs: Additional keyword arguments to pass to the Pandas plot function.\n\n        Returns:\n            SubplotBase: The matplotlib axes object.\n\n        Raises:\n            ValueError: If the sort_order is not \"ascending\", \"descending\" or None.\n            ValueError: If the orientation is not \"vertical\" or \"horizontal\".\n            ValueError: If multiple segment columns are used, as plotting is only supported for a single segment column.\n        \"\"\"\n        if sort_order not in [\"ascending\", \"descending\", None]:\n            raise ValueError(\"sort_order must be either 'ascending' or 'descending' or None\")\n        if orientation not in [\"vertical\", \"horizontal\"]:\n            raise ValueError(\"orientation must be either 'vertical' or 'horizontal'\")\n        if len(self.segment_col) &gt; 1:\n            raise ValueError(\"Plotting is only supported for a single segment column\")\n\n        default_title = f\"{value_col.title()} by Segment\"\n        kind = \"bar\"\n        if orientation == \"horizontal\":\n            kind = \"barh\"\n\n        # Use the first segment column for plotting\n        plot_segment_col = self.segment_col[0]\n        val_s = self.df.set_index(plot_segment_col)[value_col]\n        if hide_total:\n            val_s = val_s[val_s.index != \"Total\"]\n\n        if sort_order is not None:\n            ascending = sort_order == \"ascending\"\n            val_s = val_s.sort_values(ascending=ascending)\n\n        ax = val_s.plot(\n            kind=kind,\n            color=COLORS[\"green\"][500],\n            legend=False,\n            ax=ax,\n            **kwargs,\n        )\n\n        if orientation == \"vertical\":\n            plot_y_label = gu.not_none(y_label, value_col.title())\n            plot_x_label = gu.not_none(x_label, \"\")\n            decimals = gu.get_decimals(ax.get_ylim(), ax.get_yticks())\n            ax.yaxis.set_major_formatter(lambda x, pos: gu.human_format(x, pos, decimals=decimals))\n        else:\n            plot_y_label = gu.not_none(y_label, \"\")\n            plot_x_label = gu.not_none(x_label, value_col.title())\n            decimals = gu.get_decimals(ax.get_xlim(), ax.get_xticks())\n            ax.xaxis.set_major_formatter(lambda x, pos: gu.human_format(x, pos, decimals=decimals))\n\n        ax = gu.standard_graph_styles(\n            ax,\n            title=gu.not_none(title, default_title),\n            x_label=plot_x_label,\n            y_label=plot_y_label,\n        )\n\n        if source_text is not None:\n            gu.add_source_text(ax=ax, source_text=source_text)\n\n        gu.standard_tick_styles(ax)\n\n        return ax\n</code></pre>"},{"location":"api/segmentation/segstats/#pyretailscience.segmentation.segstats.SegTransactionStats.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Returns the dataframe with the transaction statistics by segment.</p>"},{"location":"api/segmentation/segstats/#pyretailscience.segmentation.segstats.SegTransactionStats.__init__","title":"<code>__init__(data, segment_col='segment_name', calc_total=True, extra_aggs=None)</code>","text":"<p>Calculates transaction statistics by segment.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | Table</code> <p>The transaction data. The dataframe must contain the columns customer_id, unit_spend and transaction_id. If the dataframe contains the column unit_quantity, then the columns unit_spend and unit_quantity are used to calculate the price_per_unit and units_per_transaction.</p> required <code>segment_col</code> <code>str | list[str]</code> <p>The column or list of columns to use for the segmentation. Defaults to \"segment_name\".</p> <code>'segment_name'</code> <code>calc_total</code> <code>bool</code> <p>Whether to include the total row. Defaults to True.</p> <code>True</code> <code>extra_aggs</code> <code>dict[str, tuple[str, str]]</code> <p>Additional aggregations to perform. The keys in the dictionary will be the column names for the aggregation results. The values are tuples with (column_name, aggregation_function), where: - column_name is the name of the column to aggregate - aggregation_function is a string name of an Ibis aggregation function (e.g., \"nunique\", \"sum\") Example: {\"stores\": (\"store_id\", \"nunique\")} would count unique store_ids.</p> <code>None</code> Source code in <code>pyretailscience/segmentation/segstats.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame | ibis.Table,\n    segment_col: str | list[str] = \"segment_name\",\n    calc_total: bool = True,\n    extra_aggs: dict[str, tuple[str, str]] | None = None,\n) -&gt; None:\n    \"\"\"Calculates transaction statistics by segment.\n\n    Args:\n        data (pd.DataFrame | ibis.Table): The transaction data. The dataframe must contain the columns\n            customer_id, unit_spend and transaction_id. If the dataframe contains the column unit_quantity, then\n            the columns unit_spend and unit_quantity are used to calculate the price_per_unit and\n            units_per_transaction.\n        segment_col (str | list[str], optional): The column or list of columns to use for the segmentation.\n            Defaults to \"segment_name\".\n        calc_total (bool, optional): Whether to include the total row. Defaults to True.\n        extra_aggs (dict[str, tuple[str, str]], optional): Additional aggregations to perform.\n            The keys in the dictionary will be the column names for the aggregation results.\n            The values are tuples with (column_name, aggregation_function), where:\n            - column_name is the name of the column to aggregate\n            - aggregation_function is a string name of an Ibis aggregation function (e.g., \"nunique\", \"sum\")\n            Example: {\"stores\": (\"store_id\", \"nunique\")} would count unique store_ids.\n    \"\"\"\n    cols = ColumnHelper()\n\n    if isinstance(segment_col, str):\n        segment_col = [segment_col]\n    required_cols = [\n        cols.customer_id,\n        cols.unit_spend,\n        cols.transaction_id,\n        *segment_col,\n    ]\n    if cols.unit_qty in data.columns:\n        required_cols.append(cols.unit_qty)\n\n    missing_cols = set(required_cols) - set(data.columns)\n    if len(missing_cols) &gt; 0:\n        msg = f\"The following columns are required but missing: {missing_cols}\"\n        raise ValueError(msg)\n\n    # Validate extra_aggs if provided\n    if extra_aggs:\n        for col_tuple in extra_aggs.values():\n            col, func = col_tuple\n            if col not in data.columns:\n                msg = f\"Column '{col}' specified in extra_aggs does not exist in the data\"\n                raise ValueError(msg)\n            if not hasattr(data[col], func):\n                msg = f\"Aggregation function '{func}' not available for column '{col}'\"\n                raise ValueError(msg)\n\n    self.segment_col = segment_col\n    self.extra_aggs = {} if extra_aggs is None else extra_aggs\n\n    self.table = self._calc_seg_stats(data, segment_col, calc_total, self.extra_aggs)\n</code></pre>"},{"location":"api/segmentation/segstats/#pyretailscience.segmentation.segstats.SegTransactionStats.plot","title":"<code>plot(value_col, title=None, x_label=None, y_label=None, ax=None, orientation='vertical', sort_order=None, source_text=None, hide_total=True, **kwargs)</code>","text":"<p>Plots the value_col by segment.</p> <p>Parameters:</p> Name Type Description Default <code>value_col</code> <code>str</code> <p>The column to plot.</p> required <code>title</code> <code>str</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>x_label</code> <code>str</code> <p>The x-axis label. Defaults to None. When None the x-axis label is blank when the orientation is horizontal. When the orientation is vertical it is set to the <code>value_col</code> in title case.</p> <code>None</code> <code>y_label</code> <code>str</code> <p>The y-axis label. Defaults to None. When None the y-axis label is set to the <code>value_col</code> in title case when the orientation is horizontal. Then the orientation is vertical it is set to blank</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>The matplotlib axes object to plot on. Defaults to None.</p> <code>None</code> <code>orientation</code> <code>Literal['vertical', 'horizontal']</code> <p>The orientation of the plot. Defaults to \"vertical\".</p> <code>'vertical'</code> <code>sort_order</code> <code>Literal['ascending', 'descending', None]</code> <p>The sort order of the segments. Defaults to None. If None, the segments are plotted in the order they appear in the dataframe.</p> <code>None</code> <code>source_text</code> <code>str</code> <p>The source text to add to the plot. Defaults to None.</p> <code>None</code> <code>hide_total</code> <code>bool</code> <p>Whether to hide the total row. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>dict[str, any]</code> <p>Additional keyword arguments to pass to the Pandas plot function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SubplotBase</code> <code>SubplotBase</code> <p>The matplotlib axes object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sort_order is not \"ascending\", \"descending\" or None.</p> <code>ValueError</code> <p>If the orientation is not \"vertical\" or \"horizontal\".</p> <code>ValueError</code> <p>If multiple segment columns are used, as plotting is only supported for a single segment column.</p> Source code in <code>pyretailscience/segmentation/segstats.py</code> <pre><code>def plot(\n    self,\n    value_col: str,\n    title: str | None = None,\n    x_label: str | None = None,\n    y_label: str | None = None,\n    ax: Axes | None = None,\n    orientation: Literal[\"vertical\", \"horizontal\"] = \"vertical\",\n    sort_order: Literal[\"ascending\", \"descending\", None] = None,\n    source_text: str | None = None,\n    hide_total: bool = True,\n    **kwargs: dict[str, any],\n) -&gt; SubplotBase:\n    \"\"\"Plots the value_col by segment.\n\n    Args:\n        value_col (str): The column to plot.\n        title (str, optional): The title of the plot. Defaults to None.\n        x_label (str, optional): The x-axis label. Defaults to None. When None the x-axis label is blank when the\n            orientation is horizontal. When the orientation is vertical it is set to the `value_col` in title case.\n        y_label (str, optional): The y-axis label. Defaults to None. When None the y-axis label is set to the\n            `value_col` in title case when the orientation is horizontal. Then the orientation is vertical it is\n            set to blank\n        ax (Axes, optional): The matplotlib axes object to plot on. Defaults to None.\n        orientation (Literal[\"vertical\", \"horizontal\"], optional): The orientation of the plot. Defaults to\n            \"vertical\".\n        sort_order (Literal[\"ascending\", \"descending\", None], optional): The sort order of the segments.\n            Defaults to None. If None, the segments are plotted in the order they appear in the dataframe.\n        source_text (str, optional): The source text to add to the plot. Defaults to None.\n        hide_total (bool, optional): Whether to hide the total row. Defaults to True.\n        **kwargs: Additional keyword arguments to pass to the Pandas plot function.\n\n    Returns:\n        SubplotBase: The matplotlib axes object.\n\n    Raises:\n        ValueError: If the sort_order is not \"ascending\", \"descending\" or None.\n        ValueError: If the orientation is not \"vertical\" or \"horizontal\".\n        ValueError: If multiple segment columns are used, as plotting is only supported for a single segment column.\n    \"\"\"\n    if sort_order not in [\"ascending\", \"descending\", None]:\n        raise ValueError(\"sort_order must be either 'ascending' or 'descending' or None\")\n    if orientation not in [\"vertical\", \"horizontal\"]:\n        raise ValueError(\"orientation must be either 'vertical' or 'horizontal'\")\n    if len(self.segment_col) &gt; 1:\n        raise ValueError(\"Plotting is only supported for a single segment column\")\n\n    default_title = f\"{value_col.title()} by Segment\"\n    kind = \"bar\"\n    if orientation == \"horizontal\":\n        kind = \"barh\"\n\n    # Use the first segment column for plotting\n    plot_segment_col = self.segment_col[0]\n    val_s = self.df.set_index(plot_segment_col)[value_col]\n    if hide_total:\n        val_s = val_s[val_s.index != \"Total\"]\n\n    if sort_order is not None:\n        ascending = sort_order == \"ascending\"\n        val_s = val_s.sort_values(ascending=ascending)\n\n    ax = val_s.plot(\n        kind=kind,\n        color=COLORS[\"green\"][500],\n        legend=False,\n        ax=ax,\n        **kwargs,\n    )\n\n    if orientation == \"vertical\":\n        plot_y_label = gu.not_none(y_label, value_col.title())\n        plot_x_label = gu.not_none(x_label, \"\")\n        decimals = gu.get_decimals(ax.get_ylim(), ax.get_yticks())\n        ax.yaxis.set_major_formatter(lambda x, pos: gu.human_format(x, pos, decimals=decimals))\n    else:\n        plot_y_label = gu.not_none(y_label, \"\")\n        plot_x_label = gu.not_none(x_label, value_col.title())\n        decimals = gu.get_decimals(ax.get_xlim(), ax.get_xticks())\n        ax.xaxis.set_major_formatter(lambda x, pos: gu.human_format(x, pos, decimals=decimals))\n\n    ax = gu.standard_graph_styles(\n        ax,\n        title=gu.not_none(title, default_title),\n        x_label=plot_x_label,\n        y_label=plot_y_label,\n    )\n\n    if source_text is not None:\n        gu.add_source_text(ax=ax, source_text=source_text)\n\n    gu.standard_tick_styles(ax)\n\n    return ax\n</code></pre>"},{"location":"api/segmentation/threshold/","title":"Threshold Segmentation","text":"<p>Threshold-Based Customer Segmentation Module.</p> <p>This module provides the <code>ThresholdSegmentation</code> class, which segments customers based on user-defined thresholds and segment mappings.</p> <p>Key Features: - Segments customers based on specified percentile thresholds. - Uses a specified column for segmentation, with an aggregation function applied. - Handles customers with zero spend using configurable options. - Utilizes Ibis for efficient query execution.</p>"},{"location":"api/segmentation/threshold/#pyretailscience.segmentation.threshold.ThresholdSegmentation","title":"<code>ThresholdSegmentation</code>","text":"<p>             Bases: <code>BaseSegmentation</code></p> <p>Segments customers based on user-defined thresholds and segments.</p> Source code in <code>pyretailscience/segmentation/threshold.py</code> <pre><code>class ThresholdSegmentation(BaseSegmentation):\n    \"\"\"Segments customers based on user-defined thresholds and segments.\"\"\"\n\n    _df: pd.DataFrame | None = None\n\n    def __init__(\n        self,\n        df: pd.DataFrame | ibis.Table,\n        thresholds: list[float],\n        segments: list[str],\n        value_col: str | None = None,\n        agg_func: str = \"sum\",\n        zero_segment_name: str = \"Zero\",\n        zero_value_customers: Literal[\"separate_segment\", \"exclude\", \"include_with_light\"] = \"separate_segment\",\n    ) -&gt; None:\n        \"\"\"Segments customers based on user-defined thresholds and segments.\n\n        Args:\n            df (pd.DataFrame | ibis.Table): A dataframe with the transaction data. The dataframe must contain a customer_id column.\n            thresholds (List[float]): The percentile thresholds for segmentation.\n            segments (List[str]): A list of segment names for each threshold.\n            value_col (str, optional): The column to use for the segmentation. Defaults to get_option(\"column.unit_spend\").\n            agg_func (str, optional): The aggregation function to use when grouping by customer_id. Defaults to \"sum\".\n            zero_segment_name (str, optional): The name of the segment for customers with zero spend. Defaults to \"Zero\".\n            zero_value_customers (Literal[\"separate_segment\", \"exclude\", \"include_with_light\"], optional): How to handle\n                customers with zero spend. Defaults to \"separate_segment\".\n\n        Raises:\n            ValueError: If the dataframe is missing the columns option column.customer_id or `value_col`, or these\n                columns contain null values.\n        \"\"\"\n        if len(thresholds) != len(set(thresholds)):\n            raise ValueError(\"The thresholds must be unique.\")\n\n        if len(thresholds) != len(segments):\n            raise ValueError(\"The number of thresholds must match the number of segments.\")\n\n        if isinstance(df, pd.DataFrame):\n            df: ibis.Table = ibis.memtable(df)\n\n        value_col = get_option(\"column.unit_spend\") if value_col is None else value_col\n\n        required_cols = [get_option(\"column.customer_id\"), value_col]\n\n        missing_cols = set(required_cols) - set(df.columns)\n        if len(missing_cols) &gt; 0:\n            msg = f\"The following columns are required but missing: {missing_cols}\"\n            raise ValueError(msg)\n\n        df = df.group_by(get_option(\"column.customer_id\")).aggregate(\n            **{value_col: getattr(df[value_col], agg_func)()},\n        )\n\n        # Separate customers with zero spend\n        zero_df = None\n        if zero_value_customers == \"exclude\":\n            df = df.filter(df[value_col] != 0)\n        elif zero_value_customers == \"separate_segment\":\n            zero_df = df.filter(df[value_col] == 0).mutate(segment_name=ibis.literal(zero_segment_name))\n            df = df.filter(df[value_col] != 0)\n\n        window = ibis.window(order_by=ibis.asc(df[value_col]))\n        df = df.mutate(ptile=ibis.percent_rank().over(window))\n\n        case_args = [(df[\"ptile\"] &lt;= quantile, segment) for quantile, segment in zip(thresholds, segments, strict=True)]\n\n        df = df.mutate(segment_name=ibis.cases(*case_args)).drop([\"ptile\"])\n\n        if zero_value_customers == \"separate_segment\":\n            df = ibis.union(df, zero_df)\n\n        self.table = df\n\n    @property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"Returns the dataframe with the segment names.\"\"\"\n        if self._df is None:\n            self._df = self.table.execute().set_index(get_option(\"column.customer_id\"))\n        return self._df\n</code></pre>"},{"location":"api/segmentation/threshold/#pyretailscience.segmentation.threshold.ThresholdSegmentation.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Returns the dataframe with the segment names.</p>"},{"location":"api/segmentation/threshold/#pyretailscience.segmentation.threshold.ThresholdSegmentation.__init__","title":"<code>__init__(df, thresholds, segments, value_col=None, agg_func='sum', zero_segment_name='Zero', zero_value_customers='separate_segment')</code>","text":"<p>Segments customers based on user-defined thresholds and segments.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame | Table</code> <p>A dataframe with the transaction data. The dataframe must contain a customer_id column.</p> required <code>thresholds</code> <code>List[float]</code> <p>The percentile thresholds for segmentation.</p> required <code>segments</code> <code>List[str]</code> <p>A list of segment names for each threshold.</p> required <code>value_col</code> <code>str</code> <p>The column to use for the segmentation. Defaults to get_option(\"column.unit_spend\").</p> <code>None</code> <code>agg_func</code> <code>str</code> <p>The aggregation function to use when grouping by customer_id. Defaults to \"sum\".</p> <code>'sum'</code> <code>zero_segment_name</code> <code>str</code> <p>The name of the segment for customers with zero spend. Defaults to \"Zero\".</p> <code>'Zero'</code> <code>zero_value_customers</code> <code>Literal['separate_segment', 'exclude', 'include_with_light']</code> <p>How to handle customers with zero spend. Defaults to \"separate_segment\".</p> <code>'separate_segment'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataframe is missing the columns option column.customer_id or <code>value_col</code>, or these columns contain null values.</p> Source code in <code>pyretailscience/segmentation/threshold.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame | ibis.Table,\n    thresholds: list[float],\n    segments: list[str],\n    value_col: str | None = None,\n    agg_func: str = \"sum\",\n    zero_segment_name: str = \"Zero\",\n    zero_value_customers: Literal[\"separate_segment\", \"exclude\", \"include_with_light\"] = \"separate_segment\",\n) -&gt; None:\n    \"\"\"Segments customers based on user-defined thresholds and segments.\n\n    Args:\n        df (pd.DataFrame | ibis.Table): A dataframe with the transaction data. The dataframe must contain a customer_id column.\n        thresholds (List[float]): The percentile thresholds for segmentation.\n        segments (List[str]): A list of segment names for each threshold.\n        value_col (str, optional): The column to use for the segmentation. Defaults to get_option(\"column.unit_spend\").\n        agg_func (str, optional): The aggregation function to use when grouping by customer_id. Defaults to \"sum\".\n        zero_segment_name (str, optional): The name of the segment for customers with zero spend. Defaults to \"Zero\".\n        zero_value_customers (Literal[\"separate_segment\", \"exclude\", \"include_with_light\"], optional): How to handle\n            customers with zero spend. Defaults to \"separate_segment\".\n\n    Raises:\n        ValueError: If the dataframe is missing the columns option column.customer_id or `value_col`, or these\n            columns contain null values.\n    \"\"\"\n    if len(thresholds) != len(set(thresholds)):\n        raise ValueError(\"The thresholds must be unique.\")\n\n    if len(thresholds) != len(segments):\n        raise ValueError(\"The number of thresholds must match the number of segments.\")\n\n    if isinstance(df, pd.DataFrame):\n        df: ibis.Table = ibis.memtable(df)\n\n    value_col = get_option(\"column.unit_spend\") if value_col is None else value_col\n\n    required_cols = [get_option(\"column.customer_id\"), value_col]\n\n    missing_cols = set(required_cols) - set(df.columns)\n    if len(missing_cols) &gt; 0:\n        msg = f\"The following columns are required but missing: {missing_cols}\"\n        raise ValueError(msg)\n\n    df = df.group_by(get_option(\"column.customer_id\")).aggregate(\n        **{value_col: getattr(df[value_col], agg_func)()},\n    )\n\n    # Separate customers with zero spend\n    zero_df = None\n    if zero_value_customers == \"exclude\":\n        df = df.filter(df[value_col] != 0)\n    elif zero_value_customers == \"separate_segment\":\n        zero_df = df.filter(df[value_col] == 0).mutate(segment_name=ibis.literal(zero_segment_name))\n        df = df.filter(df[value_col] != 0)\n\n    window = ibis.window(order_by=ibis.asc(df[value_col]))\n    df = df.mutate(ptile=ibis.percent_rank().over(window))\n\n    case_args = [(df[\"ptile\"] &lt;= quantile, segment) for quantile, segment in zip(thresholds, segments, strict=True)]\n\n    df = df.mutate(segment_name=ibis.cases(*case_args)).drop([\"ptile\"])\n\n    if zero_value_customers == \"separate_segment\":\n        df = ibis.union(df, zero_df)\n\n    self.table = df\n</code></pre>"},{"location":"api/utils/date/","title":"Date Utils","text":"<p>Utility functions for time-related operations in retail analysis.</p>"},{"location":"api/utils/date/#pyretailscience.utils.date.filter_and_label_by_periods","title":"<code>filter_and_label_by_periods(transactions, period_ranges)</code>","text":"<p>Filters transactions to specified time periods and adds period labels.</p> <p>This function filters transactions based on specified time periods and adds a new column indicating the period name. It is useful for analyzing transactions within specific date ranges and comparing KPIs between them.</p> Example <p>transactions = ibis.table(\"transactions\") period_ranges = {     \"Q1\": (\"2023-01-01\", \"2023-03-31\"),     \"Q2\": (\"2023-04-01\", \"2023-06-30\"), } filtered_transactions = filter_and_label_by_periods(transactions, period_ranges)</p> <p>Parameters:</p> Name Type Description Default <code>transactions</code> <code>Table</code> <p>An ibis table with a transaction_date column.</p> required <code>period_ranges</code> <code>dict[str, tuple[datetime, datetime] | tuple[str, str]]</code> <p>Dict where keys are period names and values are(start_date, end_date) tuples.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>An ibis table with filtered transactions and added period_name column.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any value in period_ranges is not a tuple of length 2.</p> Source code in <code>pyretailscience/utils/date.py</code> <pre><code>def filter_and_label_by_periods(\n    transactions: ibis.Table,\n    period_ranges: dict[str, tuple[datetime, datetime] | tuple[str, str]],\n) -&gt; ibis.Table:\n    \"\"\"Filters transactions to specified time periods and adds period labels.\n\n    This function filters transactions based on specified time periods and adds a new column indicating the period name.\n    It is useful for analyzing transactions within specific date ranges and comparing KPIs between them.\n\n    Example:\n        transactions = ibis.table(\"transactions\")\n        period_ranges = {\n            \"Q1\": (\"2023-01-01\", \"2023-03-31\"),\n            \"Q2\": (\"2023-04-01\", \"2023-06-30\"),\n        }\n        filtered_transactions = filter_and_label_by_periods(transactions, period_ranges)\n        # filtered_transactions will only contain transactions from the date ranges specified in Q1 and Q2 and a new\n        # column 'period_name' will be in the table defining the period for each transaction.\n\n    Args:\n        transactions (ibis.Table): An ibis table with a transaction_date column.\n        period_ranges (dict[str, tuple[datetime, datetime] | tuple[str, str]]): Dict where keys are period names and\n            values are(start_date, end_date) tuples.\n\n    Returns:\n        An ibis table with filtered transactions and added period_name column.\n\n    Raises:\n        ValueError: If any value in period_ranges is not a tuple of length 2.\n    \"\"\"\n    branches = []\n\n    for period_name, date_range in period_ranges.items():\n        if not (isinstance(date_range, tuple) and len(date_range) == 2):  # noqa: PLR2004 - Explained in the error below\n            msg = f\"Period '{period_name}' must have a (start_date, end_date) tuple\"\n            raise ValueError(msg)\n\n        period_condition = transactions[get_option(\"column.transaction_date\")].between(date_range[0], date_range[1])\n        branches.append((period_condition, ibis.literal(period_name)))\n\n    conditions = ibis.or_(*[condition[0] for condition in branches])\n    return transactions.filter(conditions).mutate(period_name=ibis.cases(*branches))\n</code></pre>"},{"location":"api/utils/date/#pyretailscience.utils.date.filter_and_label_by_periods--filtered_transactions-will-only-contain-transactions-from-the-date-ranges-specified-in-q1-and-q2-and-a-new","title":"filtered_transactions will only contain transactions from the date ranges specified in Q1 and Q2 and a new","text":""},{"location":"api/utils/date/#pyretailscience.utils.date.filter_and_label_by_periods--column-period_name-will-be-in-the-table-defining-the-period-for-each-transaction","title":"column 'period_name' will be in the table defining the period for each transaction.","text":""},{"location":"api/utils/date/#pyretailscience.utils.date.find_overlapping_periods","title":"<code>find_overlapping_periods(start_date, end_date, return_str=True)</code>","text":"<p>Find overlapping time periods within the given date range, split by year.</p> <p>This function generates overlapping periods between a given start date and end date. The first period will start from the given start date, and each subsequent period will start on the same month and day for the following years, ending each period on the same month and day of the end date but in the subsequent year, except for the last period, which ends at the provided end date.</p> Note <p>This function does not adjust for leap years. If the start or end date is February 29, it may cause an issue in non-leap years.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Union[datetime, str]</code> <p>The starting date of the range, either as a datetime object or 'YYYY-MM-DD' string.</p> required <code>end_date</code> <code>Union[datetime, str]</code> <p>The ending date of the range, either as a datetime object or 'YYYY-MM-DD' string.</p> required <code>return_str</code> <code>bool</code> <p>If True, returns dates as ISO-formatted strings ('YYYY-MM-DD').                          If False, returns datetime objects. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[tuple[str | datetime, str | datetime]]</code> <p>List[Tuple[Union[str, datetime], Union[str, datetime]]]:</p> <code>list[tuple[str | datetime, str | datetime]]</code> <p>A list of tuples where each tuple contains the start and end dates of an overlapping period,</p> <code>list[tuple[str | datetime, str | datetime]]</code> <p>either as strings (ISO format) or datetime objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the start date is after the end date.</p> Source code in <code>pyretailscience/utils/date.py</code> <pre><code>def find_overlapping_periods(\n    start_date: datetime | str,\n    end_date: datetime | str,\n    return_str: bool = True,\n) -&gt; list[tuple[str | datetime, str | datetime]]:\n    \"\"\"Find overlapping time periods within the given date range, split by year.\n\n    This function generates overlapping periods between a given start date and end date.\n    The first period will start from the given start date, and each subsequent period will start on\n    the same month and day for the following years, ending each period on the same month and day\n    of the end date but in the subsequent year,\n    except for the last period, which ends at the provided end date.\n\n    Note:\n        This function does not adjust for leap years. If the start or end date is February 29,\n        it may cause an issue in non-leap years.\n\n    Args:\n        start_date (Union[datetime, str]): The starting date of the range, either as a datetime object or 'YYYY-MM-DD' string.\n        end_date (Union[datetime, str]): The ending date of the range, either as a datetime object or 'YYYY-MM-DD' string.\n        return_str (bool, optional): If True, returns dates as ISO-formatted strings ('YYYY-MM-DD').\n                                     If False, returns datetime objects. Defaults to True.\n\n    Returns:\n        List[Tuple[Union[str, datetime], Union[str, datetime]]]:\n        A list of tuples where each tuple contains the start and end dates of an overlapping period,\n        either as strings (ISO format) or datetime objects.\n\n    Raises:\n        ValueError: If the start date is after the end date.\n    \"\"\"\n    if isinstance(start_date, str):\n        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")  # noqa: DTZ007\n    if isinstance(end_date, str):\n        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")  # noqa: DTZ007\n\n    if start_date &gt; end_date:\n        raise ValueError(\"Start date must be before end date\")\n\n    start_year, start_month, start_day = start_date.year, start_date.month, start_date.day\n    end_year, end_month, end_day = end_date.year, end_date.month, end_date.day\n    if start_year == end_year:\n        return []\n\n    years = np.arange(start_year, end_year)\n\n    period_starts = [start_date if year == start_year else datetime(year, start_month, start_day) for year in years]  # noqa: DTZ001\n    period_ends = [datetime(year + 1, end_month, end_day) for year in years]  # noqa: DTZ001\n\n    df = pd.DataFrame({\"start\": period_starts, \"end\": period_ends})\n\n    if return_str:\n        return [\n            (start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\"))\n            for start, end in zip(df[\"start\"], df[\"end\"], strict=False)\n        ]\n    return list(zip(df[\"start\"], df[\"end\"], strict=False))\n</code></pre>"},{"location":"api/utils/filter_and_label/","title":"Filter &amp; Label Utils","text":"<p>filter_and_label Utilities - Filtering and Labeling by Condition.</p> <p>This module provides utilities to filter an Ibis table based on arbitrary logical conditions and attach descriptive labels to matched rows. It is useful for segmenting and analyzing data according to business-defined rules, such as category classification, customer segmentation, or pricing tiers.</p> <p>Example use cases: - Tagging product records based on category. - Classifying transactions into business segments. - Preparing labeled datasets for analysis or machine learning.</p>"},{"location":"api/utils/filter_and_label/#pyretailscience.utils.filter_and_label.filter_and_label_by_condition","title":"<code>filter_and_label_by_condition(table, conditions)</code>","text":"<p>Filters a table based on specified conditions and adds labels.</p> <p>This function filters rows in a table based on specified conditions and adds a new column indicating the label associated with each condition. It's useful for categorizing data based on different criteria.</p> Example <p>data = ibis.memtable(df) labeled_data = filter_and_label_by_condition(     data,     conditions={         \"toys\": data[\"category\"] == \"toys\",         \"shoes\": data[\"category\"] == \"shoes\"     } )</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>An ibis table to filter.</p> required <code>conditions</code> <code>dict[str, BooleanColumn]</code> <p>Dict where keys are labels and values are ibis boolean expressions representing filter conditions.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>ibis.Table: An ibis table with filtered rows and an added label column.</p> Source code in <code>pyretailscience/utils/filter_and_label.py</code> <pre><code>def filter_and_label_by_condition(\n    table: ibis.Table,\n    conditions: dict[str, ibis.expr.types.BooleanColumn],\n) -&gt; ibis.Table:\n    \"\"\"Filters a table based on specified conditions and adds labels.\n\n    This function filters rows in a table based on specified conditions and adds a new column\n    indicating the label associated with each condition. It's useful for categorizing data\n    based on different criteria.\n\n    Example:\n        data = ibis.memtable(df)\n        labeled_data = filter_and_label_by_condition(\n            data,\n            conditions={\n                \"toys\": data[\"category\"] == \"toys\",\n                \"shoes\": data[\"category\"] == \"shoes\"\n            }\n        )\n        # labeled_data will only contain rows where category is either \"toys\" or \"shoes\",\n        # and a new column 'label' will be added indicating which category it belongs to.\n\n    Args:\n        table (ibis.Table): An ibis table to filter.\n        conditions (dict[str, ibis.expr.types.BooleanColumn]): Dict where keys are labels and\n            values are ibis boolean expressions representing filter conditions.\n\n    Returns:\n        ibis.Table: An ibis table with filtered rows and an added label column.\n    \"\"\"\n    branches = [(condition, ibis.literal(label)) for label, condition in conditions.items()]\n    combined_condition = ibis.or_(*[condition for condition, _ in branches])\n\n    return table.filter(combined_condition).mutate(label=ibis.cases(*branches))\n</code></pre>"},{"location":"api/utils/filter_and_label/#pyretailscience.utils.filter_and_label.filter_and_label_by_condition--labeled_data-will-only-contain-rows-where-category-is-either-toys-or-shoes","title":"labeled_data will only contain rows where category is either \"toys\" or \"shoes\",","text":""},{"location":"api/utils/filter_and_label/#pyretailscience.utils.filter_and_label.filter_and_label_by_condition--and-a-new-column-label-will-be-added-indicating-which-category-it-belongs-to","title":"and a new column 'label' will be added indicating which category it belongs to.","text":""},{"location":"examples/cross_shop/","title":"Cross Shop Analysis","text":"<p>The cross shop module calculates the overlap of customers buying or transactions containing 2 to 3 different products or categories.</p> <p>A typical use case is to analyze promotion cannibalization, which we will explore below.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_parquet(\"../../data/transactions.parquet\")\ndf.head()\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd  df = pd.read_parquet(\"../../data/transactions.parquet\") df.head() Out[\u00a0]: transaction_id transaction_date transaction_time customer_id product_id product_name category_0_name category_0_id category_1_name category_1_id brand_name brand_id unit_quantity unit_cost unit_spend store_id 0 16050 2023-01-12 17:44:29 1 15 Spawn Figure Toys 1 Action Figures 1 McFarlane Toys 3 2 36.10 55.98 6 1 16050 2023-01-12 17:44:29 1 1317 Gone Girl Books 8 Mystery &amp; Thrillers 53 Alfred A. Knopf 264 1 6.98 10.49 6 2 20090 2023-02-05 09:31:42 1 509 Ryzen 3 3300X Electronics 3 Computer Components 21 AMD 102 3 200.61 360.00 4 3 20090 2023-02-05 09:31:42 1 735 Linden Wood Paneled Mirror Home 5 Home Decor 30 Pottery Barn 147 1 379.83 599.00 4 4 20090 2023-02-05 09:31:42 1 1107 Pro-V Daily Moisture Renewal Conditioner Beauty 7 Hair Care 45 Pantene 222 1 3.32 4.99 4 In\u00a0[\u00a0]: Copied! <pre>print(f\"Number of unique customers: {df['customer_id'].nunique()}\")\nprint(f\"Number of unique transactions: {df['transaction_id'].nunique()}\")\n</pre> print(f\"Number of unique customers: {df['customer_id'].nunique()}\") print(f\"Number of unique transactions: {df['transaction_id'].nunique()}\") <pre>Number of unique customers: 4250\nNumber of unique transactions: 25490\n</pre> <p>We'll artifically alter our data set to show that there are more shoe buyers. This is purely for the example.</p> In\u00a0[\u00a0]: Copied! <pre>shoes_idx = df[\"category_1_name\"] == \"Shoes\"\ndf.loc[shoes_idx, \"category_1_name\"] = np.random.RandomState(42).choice(\n    [\"Shoes\", \"Jeans\"],\n    size=shoes_idx.sum(),\n    p=[0.5, 0.5],\n)\n</pre> shoes_idx = df[\"category_1_name\"] == \"Shoes\" df.loc[shoes_idx, \"category_1_name\"] = np.random.RandomState(42).choice(     [\"Shoes\", \"Jeans\"],     size=shoes_idx.sum(),     p=[0.5, 0.5], ) <p>We'll start by looking at the overlap of customer spend. From the graph below you can see that 21% of customer spend both is on jeans and dresses. This is far higher than the 3.6% of spend shoes and dresses, and the highest of all the combinations. This suggests this is a significant trend.</p> In\u00a0[\u00a0]: Copied! <pre>from pyretailscience.analysis import cross_shop\n\ncs = cross_shop.CrossShop(\n    df,\n    group_1_col=\"category_1_name\",\n    group_1_val=\"Jeans\",\n    group_2_col=\"category_1_name\",\n    group_2_val=\"Shoes\",\n    group_3_col=\"category_1_name\",\n    group_3_val=\"Dresses\",\n    labels=[\"Jeans\", \"Shoes\", \"Dresses\"],\n)\ncs.plot(\n    title=\"Jeans are a popular cross-shopping category with dresses\",\n    source_text=\"Source: Transactions 2023-01-01 to 2023-12-31\",\n    figsize=(6, 6),\n    subset_label_formatter=lambda x: f\"{x:.1%}\",\n)\nplt.show()\n# Let's see which customers were in which groups\ndisplay(cs.cross_shop_df.head())\n# And the totals for all groups\ndisplay(cs.cross_shop_table_df)\n</pre> from pyretailscience.analysis import cross_shop  cs = cross_shop.CrossShop(     df,     group_1_col=\"category_1_name\",     group_1_val=\"Jeans\",     group_2_col=\"category_1_name\",     group_2_val=\"Shoes\",     group_3_col=\"category_1_name\",     group_3_val=\"Dresses\",     labels=[\"Jeans\", \"Shoes\", \"Dresses\"], ) cs.plot(     title=\"Jeans are a popular cross-shopping category with dresses\",     source_text=\"Source: Transactions 2023-01-01 to 2023-12-31\",     figsize=(6, 6),     subset_label_formatter=lambda x: f\"{x:.1%}\", ) plt.show() # Let's see which customers were in which groups display(cs.cross_shop_df.head()) # And the totals for all groups display(cs.cross_shop_table_df) group_1 group_2 group_3 groups unit_spend customer_id 1 0 1 0 (0, 1, 0) 15444.07 2 0 0 0 (0, 0, 0) 52596.03 3 0 0 0 (0, 0, 0) 8106.66 4 1 0 1 (1, 0, 1) 43042.60 5 0 0 0 (0, 0, 0) 15103.83 groups unit_spend percent 0 (0, 0, 0) 17955616.81 0.201549 1 (0, 0, 1) 12340333.28 0.138518 2 (0, 1, 0) 4403812.22 0.049432 3 (0, 1, 1) 3241751.96 0.036388 4 (1, 0, 0) 17722455.78 0.198932 5 (1, 0, 1) 18695128.69 0.209850 6 (1, 1, 0) 7162512.42 0.080398 7 (1, 1, 1) 7566570.23 0.084933 <p>To make this point more dramatic, it's also possible to vary the size of the circles based on the values by setting <code>vary_sizes</code> to True when plotting. When we do this you can see that the size of the Shoes circle is much smaller.</p> In\u00a0[\u00a0]: Copied! <pre>cs.plot(\n    title=\"Jeans are a popular cross-shopping category with dresses\",\n    source_text=\"Source: Transactions 2023-01-01 to 2023-12-31\",\n    figsize=(6, 6),\n    vary_size=True,\n    subset_label_formatter=lambda x: f\"{x:.1%}\",\n)\nplt.show()\n</pre> cs.plot(     title=\"Jeans are a popular cross-shopping category with dresses\",     source_text=\"Source: Transactions 2023-01-01 to 2023-12-31\",     figsize=(6, 6),     vary_size=True,     subset_label_formatter=lambda x: f\"{x:.1%}\", ) plt.show() <p>We can back up our first observation by looking at the overlap in customer groups. Here we see that several times as many customers buy both jeans and dresses and shoes and dresses, and that jeans is a much more popular category than shoes. From this simple analysis it seems that jeans would make a much better category for a cross promotion than shoes.</p> In\u00a0[\u00a0]: Copied! <pre>cs = cross_shop.CrossShop(\n    df,\n    group_1_col=\"category_1_name\",\n    group_1_val=\"Jeans\",\n    group_2_col=\"category_1_name\",\n    group_2_val=\"Shoes\",\n    group_3_col=\"category_1_name\",\n    group_3_val=\"Dresses\",\n    labels=[\"Jeans\", \"Shoes\", \"Dresses\"],\n    value_col=\"customer_id\",\n    agg_func=\"nunique\",\n)\ncs.plot(\n    title=\"Jeans are a popular with customers as well\",\n    source_text=\"Source: Transactions 2023-01-01 to 2023-12-31\",\n    figsize=(6, 6),\n    subset_label_formatter=lambda x: f\"{x:.1%}\",\n)\nplt.show()\n</pre> cs = cross_shop.CrossShop(     df,     group_1_col=\"category_1_name\",     group_1_val=\"Jeans\",     group_2_col=\"category_1_name\",     group_2_val=\"Shoes\",     group_3_col=\"category_1_name\",     group_3_val=\"Dresses\",     labels=[\"Jeans\", \"Shoes\", \"Dresses\"],     value_col=\"customer_id\",     agg_func=\"nunique\", ) cs.plot(     title=\"Jeans are a popular with customers as well\",     source_text=\"Source: Transactions 2023-01-01 to 2023-12-31\",     figsize=(6, 6),     subset_label_formatter=lambda x: f\"{x:.1%}\", ) plt.show()"},{"location":"examples/cross_shop/#setup","title":"Setup\u00b6","text":"<p>We'll start by loading some simulated data</p>"},{"location":"examples/cross_shop/#scenario-jeans-or-shoes-cross-promotion","title":"Scenario: Jeans or Shoes cross promotion\u00b6","text":"<p>You are in charge of the Dresses product category for a large department store. You would like to run a targeted promotion to attract more dress shoppers and are trying to decide whether to target customers who have bought jeans or shoes in the past, but haven't bought Jeans. Your key question is whether a jeans or a Shoes buyer is more likely to also purchase dresses?</p>"},{"location":"examples/gain_loss/","title":"Gain Loss Analysis","text":"<p>The gain/loss module shows the increase or decrease of a product or category's sales or customer account between two time periods. It is anaylzes the shift in a KPI based on,</p> <ul> <li>New vs lost buyers</li> <li>Increased vs decreased spending</li> <li>Switching between the focus group and a comparison group</li> </ul> <p>A typical use case is to analyze promotion cannibalization, which we will explore below.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_parquet(\"../../data/transactions.parquet\")\ndf.head()\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd  df = pd.read_parquet(\"../../data/transactions.parquet\") df.head() Out[\u00a0]: transaction_id transaction_date transaction_time customer_id product_id product_name category_0_name category_0_id category_1_name category_1_id brand_name brand_id unit_quantity unit_cost unit_spend store_id 0 16050 2023-01-12 17:44:29 1 15 Spawn Figure Toys 1 Action Figures 1 McFarlane Toys 3 2 36.10 55.98 6 1 16050 2023-01-12 17:44:29 1 1317 Gone Girl Books 8 Mystery &amp; Thrillers 53 Alfred A. Knopf 264 1 6.98 10.49 6 2 20090 2023-02-05 09:31:42 1 509 Ryzen 3 3300X Electronics 3 Computer Components 21 AMD 102 3 200.61 360.00 4 3 20090 2023-02-05 09:31:42 1 735 Linden Wood Paneled Mirror Home 5 Home Decor 30 Pottery Barn 147 1 379.83 599.00 4 4 20090 2023-02-05 09:31:42 1 1107 Pro-V Daily Moisture Renewal Conditioner Beauty 7 Hair Care 45 Pantene 222 1 3.32 4.99 4 In\u00a0[\u00a0]: Copied! <pre>print(f\"Number of unique customers: {df['customer_id'].nunique()}\")\nprint(f\"Number of unique transactions: {df['transaction_id'].nunique()}\")\n</pre> print(f\"Number of unique customers: {df['customer_id'].nunique()}\") print(f\"Number of unique transactions: {df['transaction_id'].nunique()}\") <pre>Number of unique customers: 4250\nNumber of unique transactions: 25490\n</pre> In\u00a0[\u00a0]: Copied! <pre>df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"])\n\n# Define our periods\ntime_period_1 = df[\"transaction_date\"] &lt;= \"2023-06-30\"\ntime_period_2 = df[\"transaction_date\"] &gt; \"2023-06-30\"\n</pre> df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"])  # Define our periods time_period_1 = df[\"transaction_date\"] &lt;= \"2023-06-30\" time_period_2 = df[\"transaction_date\"] &gt; \"2023-06-30\" <p>We'll artifically alter our data set to show that CK actually canalbaized half of Diesel transactions. Also we'll simulate that the promotion caused a 50% increase in the quantity of items purchase.</p> In\u00a0[\u00a0]: Copied! <pre># Reasign half the rows to Calvin Klein and leave the other half as Diesel\np2_diesel_idx = time_period_2 &amp; (df[\"brand_name\"] == \"Diesel\")\ndf.loc[p2_diesel_idx, \"brand_name\"] = np.random.RandomState(42).choice(\n    [\"Calvin Klein\", \"Diesel\"],\n    size=p2_diesel_idx.sum(),\n    p=[0.75, 0.25],\n)\n\n# Apply a 20% discount to Calvin Klein products and increase the quantity by 50%\np2_ck_idx = time_period_2 &amp; (df[\"brand_name\"] == \"Calvin Klein\")\ndf.loc[p2_ck_idx, \"unit_spend\"] = df.loc[p2_ck_idx, \"unit_spend\"] * 0.8\ndf.loc[p2_ck_idx, \"unit_quantity\"] = (df.loc[p2_ck_idx, \"unit_quantity\"] * 1.5).astype(int)\ndf.loc[p2_ck_idx, \"unit_spend\"] = df.loc[p2_ck_idx, \"unit_spend\"] * df.loc[p2_ck_idx, \"unit_quantity\"]\n</pre> # Reasign half the rows to Calvin Klein and leave the other half as Diesel p2_diesel_idx = time_period_2 &amp; (df[\"brand_name\"] == \"Diesel\") df.loc[p2_diesel_idx, \"brand_name\"] = np.random.RandomState(42).choice(     [\"Calvin Klein\", \"Diesel\"],     size=p2_diesel_idx.sum(),     p=[0.75, 0.25], )  # Apply a 20% discount to Calvin Klein products and increase the quantity by 50% p2_ck_idx = time_period_2 &amp; (df[\"brand_name\"] == \"Calvin Klein\") df.loc[p2_ck_idx, \"unit_spend\"] = df.loc[p2_ck_idx, \"unit_spend\"] * 0.8 df.loc[p2_ck_idx, \"unit_quantity\"] = (df.loc[p2_ck_idx, \"unit_quantity\"] * 1.5).astype(int) df.loc[p2_ck_idx, \"unit_spend\"] = df.loc[p2_ck_idx, \"unit_spend\"] * df.loc[p2_ck_idx, \"unit_quantity\"] <p>Now we start the analysis. We will produce a chart showing where revenue changes in Calvin Klein products came from between the pre and promotion periods.</p> In\u00a0[\u00a0]: Copied! <pre>from pyretailscience.analysis.gain_loss import GainLoss\n\ngl = GainLoss(\n    df,\n    # Flag the rows of period 1\n    p1_index=time_period_1,\n    # Flag the rows of period 2\n    p2_index=time_period_2,\n    # Flag which rows are part of the focus group. Namely, which rows are Calvin Klein sales\n    focus_group_index=df[\"brand_name\"] == \"Calvin Klein\",\n    focus_group_name=\"Calvin Klein\",\n    # Flag which rows are part of the comparison group. Namely, which rows are Diesel sales\n    comparison_group_index=df[\"brand_name\"] == \"Diesel\",\n    comparison_group_name=\"Diesel\",\n    # Finally we specifiy that we want to calculate the gain/loss in total revenue\n    value_col=\"unit_spend\",\n)\ngl.plot(\n    x_label=\"Revenue Change\",\n    source_text=\"Transactions 2023-01-01 to 2023-12-31\",\n    move_legend_outside=True,\n)\nplt.show()\n</pre> from pyretailscience.analysis.gain_loss import GainLoss  gl = GainLoss(     df,     # Flag the rows of period 1     p1_index=time_period_1,     # Flag the rows of period 2     p2_index=time_period_2,     # Flag which rows are part of the focus group. Namely, which rows are Calvin Klein sales     focus_group_index=df[\"brand_name\"] == \"Calvin Klein\",     focus_group_name=\"Calvin Klein\",     # Flag which rows are part of the comparison group. Namely, which rows are Diesel sales     comparison_group_index=df[\"brand_name\"] == \"Diesel\",     comparison_group_name=\"Diesel\",     # Finally we specifiy that we want to calculate the gain/loss in total revenue     value_col=\"unit_spend\", ) gl.plot(     x_label=\"Revenue Change\",     source_text=\"Transactions 2023-01-01 to 2023-12-31\",     move_legend_outside=True, ) plt.show() <p>While we can see from the above chart that the majority of new revenue came from new customers, it appears that the revenue increase for the period came from canabalizing Diesel sales.</p> <p>If we want the raw figures on a customer or total level we can access the dataframe <code>gain_loss_df</code> and <code>gain_loss_tabel_df</code> respectively.</p> In\u00a0[\u00a0]: Copied! <pre>display(gl.gain_loss_df)\ndisplay(gl.gain_loss_table_df)\n</pre> display(gl.gain_loss_df) display(gl.gain_loss_table_df) focus_p1 comparison_p1 total_p1 focus_p2 comparison_p2 total_p2 focus_diff comparison_diff total_diff new lost increased_focus decreased_focus switch_from_comparison switch_to_comparison customer_id 1 0.0 0.0 0.0 36.8 0.0 36.8 36.8 0.0 36.8 36.8 0.0 0.0 0.0 0.0 0.0 4 24.0 0.0 24.0 0.0 0.0 0.0 -24.0 0.0 -24.0 0.0 -24.0 0.0 0.0 0.0 0.0 7 0.0 0.0 0.0 672.0 0.0 672.0 672.0 0.0 672.0 672.0 0.0 0.0 0.0 0.0 0.0 9 0.0 840.0 840.0 0.0 0.0 0.0 0.0 -840.0 -840.0 0.0 -0.0 0.0 0.0 0.0 0.0 10 0.0 0.0 0.0 0.0 585.0 585.0 0.0 585.0 585.0 0.0 0.0 0.0 0.0 0.0 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4143 0.0 0.0 0.0 441.6 0.0 441.6 441.6 0.0 441.6 441.6 0.0 0.0 0.0 0.0 0.0 4187 0.0 0.0 0.0 22.4 0.0 22.4 22.4 0.0 22.4 22.4 0.0 0.0 0.0 0.0 0.0 4200 0.0 0.0 0.0 14976.0 0.0 14976.0 14976.0 0.0 14976.0 14976.0 0.0 0.0 0.0 0.0 0.0 4236 0.0 0.0 0.0 936.0 0.0 936.0 936.0 0.0 936.0 936.0 0.0 0.0 0.0 0.0 0.0 4238 0.0 0.0 0.0 112.0 0.0 112.0 112.0 0.0 112.0 112.0 0.0 0.0 0.0 0.0 0.0 <p>1131 rows \u00d7 15 columns</p> focus_p1 comparison_p1 total_p1 focus_p2 comparison_p2 total_p2 focus_diff comparison_diff total_diff new lost increased_focus decreased_focus switch_from_comparison switch_to_comparison 50027.91 59885.0 109912.91 216468.824 12820.0 229288.824 166440.914 -47065.0 119375.914 192840.944 -43846.51 16630.224 -1820.436 3010.692 -374.0 <p>We can see a breakdown of the sales by another column using the <code>group_col</code> parameter. In this case, let's say that we're interested in seeing how the promotion did by store. In this case, store 1 seemed to see the largest canabalization effect.</p> In\u00a0[\u00a0]: Copied! <pre>gl = GainLoss(\n    df,\n    p1_index=time_period_1,\n    p2_index=time_period_2,\n    focus_group_index=df[\"brand_name\"] == \"Calvin Klein\",\n    focus_group_name=\"Calvin Klein\",\n    comparison_group_index=df[\"brand_name\"] == \"Diesel\",\n    comparison_group_name=\"Diesel\",\n    group_col=\"store_id\",\n    value_col=\"customer_id\",\n    agg_func=\"nunique\",\n)\n\ngl.plot(\n    x_label=\"Revenue Change\",\n    y_label=\"Store ID\",\n    source_text=\"Transactions 2023-01-01 to 2023-12-31\",\n    move_legend_outside=True,\n)\nplt.show()\n</pre> gl = GainLoss(     df,     p1_index=time_period_1,     p2_index=time_period_2,     focus_group_index=df[\"brand_name\"] == \"Calvin Klein\",     focus_group_name=\"Calvin Klein\",     comparison_group_index=df[\"brand_name\"] == \"Diesel\",     comparison_group_name=\"Diesel\",     group_col=\"store_id\",     value_col=\"customer_id\",     agg_func=\"nunique\", )  gl.plot(     x_label=\"Revenue Change\",     y_label=\"Store ID\",     source_text=\"Transactions 2023-01-01 to 2023-12-31\",     move_legend_outside=True, ) plt.show() <p>We can also see the effect of the promotion by on the number of customers by changing the <code>value_col</code> parameter to customer_id and <code>agg_func</code> to nunqiue. This tells the code to count the number of unique customer IDs for each group.</p> <p>From this view it's clear that the promotion wasn't able to grow the number of customers to the brand in the second half of the year.</p> In\u00a0[\u00a0]: Copied! <pre>gl = GainLoss(\n    df,\n    p1_index=time_period_1,\n    p2_index=time_period_2,\n    focus_group_index=df[\"brand_name\"] == \"Calvin Klein\",\n    focus_group_name=\"Calvin Klein\",\n    comparison_group_index=df[\"brand_name\"] == \"Diesel\",\n    comparison_group_name=\"Diesel\",\n    value_col=\"customer_id\",\n    agg_func=\"nunique\",\n)\ngl.plot(\n    x_label=\"Revenue Change\",\n    y_label=\"Store ID\",\n    source_text=\"Transactions 2023-01-01 to 2023-12-31\",\n    move_legend_outside=True,\n)\nplt.show()\n</pre> gl = GainLoss(     df,     p1_index=time_period_1,     p2_index=time_period_2,     focus_group_index=df[\"brand_name\"] == \"Calvin Klein\",     focus_group_name=\"Calvin Klein\",     comparison_group_index=df[\"brand_name\"] == \"Diesel\",     comparison_group_name=\"Diesel\",     value_col=\"customer_id\",     agg_func=\"nunique\", ) gl.plot(     x_label=\"Revenue Change\",     y_label=\"Store ID\",     source_text=\"Transactions 2023-01-01 to 2023-12-31\",     move_legend_outside=True, ) plt.show()"},{"location":"examples/gain_loss/#setup","title":"Setup\u00b6","text":"<p>We'll start by loading some simulated data</p>"},{"location":"examples/gain_loss/#scenario-calvin-klein-promotion-cannibalization","title":"Scenario: Calvin Klein Promotion Cannibalization\u00b6","text":"<p>You work for a large department store and last year you decided to put all Calvin Klein products on a 20% discount from July 1st to the end of the year. From another analysis you know what a lot of Calvin Klein buyers also buy the brand Diesel. You want to know whether the promotion brought in new buyers to the brand, or canalbalized Diesel buyers.</p> <p>For a pre-period you will use the start of the year, transactions before July 1st. The promotion period is betwen July 1st and the end of the year.</p>"},{"location":"examples/gain_loss/#follow-on-on-analysis","title":"Follow on on analysis\u00b6","text":"<p>Gain/loss analysis only forms part of a review. As follow on analysis you may want to ask questions such as,</p> <ul> <li>Did the lost customers make any other purchases in the store? Or did they churn entirely?</li> <li>Did buyers of the promotion spend more in the store overall?</li> <li>Did Diesel brand purchases decrease in H2, negating or offsetting the canbalization?</li> </ul>"},{"location":"examples/product_association/","title":"Product Association","text":"<p>The product association module implements functionality for generating product association rules, a powerful technique in retail analytics and market basket analysis.</p> <p>Product association rules are used to uncover relationships between different products that customers tend to purchase together. These rules provide valuable insights into consumer behavior and purchasing patterns, which can be leveraged by retail businesses in various ways:</p> <ol> <li><p>Cross-selling and upselling: By identifying products frequently bought together, retailers can make targeted product recommendations to increase sales and average order value.</p> </li> <li><p>Store layout optimization: Understanding product associations helps in strategic product placement within stores, potentially increasing impulse purchases and overall sales.</p> </li> <li><p>Inventory management: Knowing which products are often bought together aids in maintaining appropriate stock levels and predicting demand.</p> </li> <li><p>Marketing and promotions: Association rules can guide the creation ofeffective bundle offers and promotional campaigns.</p> </li> <li><p>Customer segmentation: Patterns in product associations can reveal distinct customer segments with specific preferences.</p> </li> <li><p>New product development: Insights from association rules can inform decisions about new product lines or features.</p> </li> </ol> <p>The module uses metrics such as support, confidence, and uplift to quantifythe strength and significance of product associations:</p> <ul> <li>Support: The frequency of items appearing together in transactions.</li> <li>Confidence: The likelihood of buying one product given the purchase of another.</li> <li>Uplift: The increase in purchase probability of one product when another is bought.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\ndf = pd.read_parquet(\"../../data/transactions.parquet\")\ndf.head()\n</pre> import pandas as pd  df = pd.read_parquet(\"../../data/transactions.parquet\") df.head() Out[\u00a0]: transaction_id transaction_date transaction_time customer_id product_id product_name category_0_name category_0_id category_1_name category_1_id brand_name brand_id unit_quantity unit_cost unit_spend store_id 0 16050 2023-01-12 17:44:29 1 15 Spawn Figure Toys 1 Action Figures 1 McFarlane Toys 3 2 36.10 55.98 6 1 16050 2023-01-12 17:44:29 1 1317 Gone Girl Books 8 Mystery &amp; Thrillers 53 Alfred A. Knopf 264 1 6.98 10.49 6 2 20090 2023-02-05 09:31:42 1 509 Ryzen 3 3300X Electronics 3 Computer Components 21 AMD 102 3 200.61 360.00 4 3 20090 2023-02-05 09:31:42 1 735 Linden Wood Paneled Mirror Home 5 Home Decor 30 Pottery Barn 147 1 379.83 599.00 4 4 20090 2023-02-05 09:31:42 1 1107 Pro-V Daily Moisture Renewal Conditioner Beauty 7 Hair Care 45 Pantene 222 1 3.32 4.99 4 In\u00a0[\u00a0]: Copied! <pre>print(f\"Number of unique customers: {df['customer_id'].nunique()}\")\nprint(f\"Number of unique transactions: {df['transaction_id'].nunique()}\")\n</pre> print(f\"Number of unique customers: {df['customer_id'].nunique()}\") print(f\"Number of unique transactions: {df['transaction_id'].nunique()}\") <pre>Number of unique customers: 4250\nNumber of unique transactions: 25490\n</pre> <p>Here we'll see simple example to generate the production association rules.</p> In\u00a0[\u00a0]: Copied! <pre>from pyretailscience.analysis.product_association import ProductAssociation\n\npa = ProductAssociation(\n    df,\n    value_col=\"product_name\",\n    group_col=\"transaction_id\",\n)\npa.df.head()\n</pre> from pyretailscience.analysis.product_association import ProductAssociation  pa = ProductAssociation(     df,     value_col=\"product_name\",     group_col=\"transaction_id\", ) pa.df.head() Out[\u00a0]: product_name_1 product_name_2 occurrences_1 occurrences_2 cooccurrences support confidence uplift 0 100 Animals Book 100% Organic Cold-Pressed Rose Hip Seed Oil 78 78 1 0.000039 0.012821 4.189678 1 100 Animals Book 20K Sousaphone 78 81 3 0.000118 0.038462 12.103514 2 100 Animals Book 360 Sport 2.0 Boxer Briefs 78 79 1 0.000039 0.012821 4.136644 3 100 Animals Book 4-Series 4K UHD 78 82 1 0.000039 0.012821 3.985303 4 100 Animals Book 700S Eterna Trumpet 78 71 1 0.000039 0.012821 4.602745 <p>You can also limit the returned items to those that include a specific item.</p> In\u00a0[\u00a0]: Copied! <pre>pa_specific_item = ProductAssociation(\n    df,\n    value_col=\"product_name\",\n    group_col=\"transaction_id\",\n    target_item=\"4-Series 4K UHD\",\n)\npa_specific_item.df.head()\n</pre> pa_specific_item = ProductAssociation(     df,     value_col=\"product_name\",     group_col=\"transaction_id\",     target_item=\"4-Series 4K UHD\", ) pa_specific_item.df.head() Out[\u00a0]: product_name_1 product_name_2 occurrences_1 occurrences_2 cooccurrences support confidence uplift 0 4-Series 4K UHD 100 Animals Book 82 78 1 0.000039 0.012195 3.985303 1 4-Series 4K UHD 122HD45 Gas Hedge Trimmer 82 92 1 0.000039 0.012195 3.378844 2 4-Series 4K UHD 2-in-1 Touch &amp; Learn Tablet 82 81 2 0.000078 0.024390 7.675399 3 4-Series 4K UHD 20K Sousaphone 82 81 1 0.000039 0.012195 3.837699 4 4-Series 4K UHD 3 Minute Miracle Deep Conditioner 82 70 1 0.000039 0.012195 4.440767 <p>You can filter the returned results by,</p> <ul> <li>Mininum occurrences of an item</li> <li>Mininum cooccurrences of pair of items</li> <li>Mininum support of a pair of items</li> <li>Mininum confidence of a pair of items</li> <li>Mininum uplift of a pair of items</li> </ul> In\u00a0[\u00a0]: Copied! <pre>pa_min_uplift = ProductAssociation(\n    df,\n    value_col=\"product_name\",\n    group_col=\"transaction_id\",\n    min_uplift=5,\n)\npa_min_uplift.df.head()\n</pre> pa_min_uplift = ProductAssociation(     df,     value_col=\"product_name\",     group_col=\"transaction_id\",     min_uplift=5, ) pa_min_uplift.df.head() Out[\u00a0]: product_name_1 product_name_2 occurrences_1 occurrences_2 cooccurrences support confidence uplift 0 100 Animals Book 20K Sousaphone 78 81 3 0.000118 0.038462 12.103514 1 100 Animals Book Activia Probiotic Yogurt 78 57 2 0.000078 0.025641 11.466487 2 100 Animals Book Aether AG 70 Pack 78 72 2 0.000078 0.025641 9.077635 3 100 Animals Book All Natural Plain Yogurt 78 62 1 0.000039 0.012821 5.270885 4 100 Animals Book American Ultra Jazz Bass 78 59 2 0.000078 0.025641 11.077792"},{"location":"examples/product_association/#setup","title":"Setup\u00b6","text":"<p>We'll start by loading some simulated data</p>"},{"location":"examples/retention/","title":"Customer Retention","text":"<p>Customer churn is a critical metric for retailers. Understanding churn is not only about recognizing lost sales. It's a test for understanding customer satisfaction and loyalty.</p> <p>In this example, we'll look at how to measure your churn and its impact on your business.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom pyretailscience.analysis import customer\n\ndf = pd.read_parquet(\"../../data/transactions.parquet\")\ndf.head()\n</pre> import matplotlib.pyplot as plt import pandas as pd  from pyretailscience.analysis import customer  df = pd.read_parquet(\"../../data/transactions.parquet\") df.head() Out[\u00a0]: transaction_id transaction_date transaction_time customer_id product_id product_name category_0_name category_0_id category_1_name category_1_id brand_name brand_id unit_quantity unit_cost unit_spend store_id 0 16050 2023-01-12 17:44:29 1 15 Spawn Figure Toys 1 Action Figures 1 McFarlane Toys 3 2 36.10 55.98 6 1 16050 2023-01-12 17:44:29 1 1317 Gone Girl Books 8 Mystery &amp; Thrillers 53 Alfred A. Knopf 264 1 6.98 10.49 6 2 20090 2023-02-05 09:31:42 1 509 Ryzen 3 3300X Electronics 3 Computer Components 21 AMD 102 3 200.61 360.00 4 3 20090 2023-02-05 09:31:42 1 735 Linden Wood Paneled Mirror Home 5 Home Decor 30 Pottery Barn 147 1 379.83 599.00 4 4 20090 2023-02-05 09:31:42 1 1107 Pro-V Daily Moisture Renewal Conditioner Beauty 7 Hair Care 45 Pantene 222 1 3.32 4.99 4 <p>Some details about the data</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Number of unique customers: {df['customer_id'].nunique()}\")\nprint(f\"Number of unique transactions: {df['transaction_id'].nunique()}\")\n</pre> print(f\"Number of unique customers: {df['customer_id'].nunique()}\") print(f\"Number of unique transactions: {df['transaction_id'].nunique()}\") <pre>Number of unique customers: 4250\nNumber of unique transactions: 25490\n</pre> In\u00a0[\u00a0]: Copied! <pre>df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"])\ndbp = customer.DaysBetweenPurchases(df)\ndbp.plot(\n    figsize=(10, 5),\n    bins=20,\n    source_text=\"Source: Purchases between 2023-01-01 and 2023-12-31\",\n)\nplt.show()\n</pre> df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"]) dbp = customer.DaysBetweenPurchases(df) dbp.plot(     figsize=(10, 5),     bins=20,     source_text=\"Source: Purchases between 2023-01-01 and 2023-12-31\", ) plt.show() <p>Here, we can see that customers seem to be on a roughly monthly shopping cycle. We can look at this data another way to help us set a churn window, by asking the question in a probabilistic way. Specifically,</p> <p>How many days will need to have passed such that 80% of customers would have purchased?</p> In\u00a0[\u00a0]: Copied! <pre>ax = dbp.plot(\n    figsize=(10, 5),\n    bins=20,\n    cumulative=True,\n    percentile_line=0.8,\n    source_text=\"Source: Transactions in 2023\",\n    title=\"When Do Customers Make Their Next Purchase?\",\n)\n\n# Let's dress up the chart a bit of text and get rid of the legend\nchurn_period = dbp.purchases_percentile(0.8)\nax.annotate(\n    f\"80% of customers made\\nanother purchase within\\n{round(churn_period)} days\",\n    xy=(churn_period, 0.81),\n    xytext=(dbp.purchase_dist_s.min(), 0.8),\n    fontsize=15,\n    ha=\"left\",\n    va=\"center\",\n    arrowprops={\"facecolor\": \"black\", \"arrowstyle\": \"-|&gt;\", \"connectionstyle\": \"arc3,rad=-0.25\", \"mutation_scale\": 25},\n)\nax.legend().set_visible(False)\nplt.show()\n</pre> ax = dbp.plot(     figsize=(10, 5),     bins=20,     cumulative=True,     percentile_line=0.8,     source_text=\"Source: Transactions in 2023\",     title=\"When Do Customers Make Their Next Purchase?\", )  # Let's dress up the chart a bit of text and get rid of the legend churn_period = dbp.purchases_percentile(0.8) ax.annotate(     f\"80% of customers made\\nanother purchase within\\n{round(churn_period)} days\",     xy=(churn_period, 0.81),     xytext=(dbp.purchase_dist_s.min(), 0.8),     fontsize=15,     ha=\"left\",     va=\"center\",     arrowprops={\"facecolor\": \"black\", \"arrowstyle\": \"-|&gt;\", \"connectionstyle\": \"arc3,rad=-0.25\", \"mutation_scale\": 25}, ) ax.legend().set_visible(False) plt.show() <p>Ok so now you have your churn period. 80% of customers made a purchase within 33 days of their last purchase. As a result we'll assume that if a customer hasn't purchased within the last 33 days they have churned.</p> In\u00a0[\u00a0]: Copied! <pre>tc = customer.TransactionChurn(df, churn_period=churn_period)\ntc.plot(\n    figsize=(10, 5),\n    source_text=\"Source: Transactions in 2023\",\n)\nplt.show()\n</pre> tc = customer.TransactionChurn(df, churn_period=churn_period) tc.plot(     figsize=(10, 5),     source_text=\"Source: Transactions in 2023\", ) plt.show() <p>Here we can see a high churn rate. It's about 10% after each transaction. The simulated data has a churn rate of 10% configured after each transaction, so this is as expected.</p> <p>Now let's look at this from a slightly different perspective. What percent of customers will have churned after each transaction.</p> In\u00a0[\u00a0]: Copied! <pre>tc.plot(\n    cumulative=True,\n    figsize=(10, 5),\n    source_text=\"Source: Transactions in 2023\",\n)\nplt.show()\n</pre> tc.plot(     cumulative=True,     figsize=(10, 5),     source_text=\"Source: Transactions in 2023\", ) plt.show()"},{"location":"examples/retention/#setup","title":"Setup\u00b6","text":""},{"location":"examples/retention/#assess-the-current-situation","title":"Assess the current situation\u00b6","text":""},{"location":"examples/retention/#finding-your-churn-window","title":"Finding your churn window\u00b6","text":"<p>All churn analysis starts with picking the churn period. This is the period after which you will consider the customer to have churned. Typically, businesses measure this as some days after the customer's last transaction.</p> <p>Note:</p> <p>Another way is to estimate when each customer will churn. This is instead of having a single fixed churn window. It is often called \"time-to-event prediction\" or \"survival analysis.\" It is an advanced technique, but it can be hard to do. Challenges include collecting and managing data. The data must cover a long time to enable the analysis. The result is often to \"censor\" the data or, in other words, limit the data used in the model. Most examples we've seen involve \"right censoring\" the data. This means setting a cut-off window. For example, say any customer who \"survives\" the past 12 months is still active. This results in a situation where some have a long churn window, others a short one. This makes a more detailed but more complex churn window. You'll need to consider whether the added benefit is offset by your business case.</p> <p>Let's start by identifying the typical number of days between customer purchases.</p>"},{"location":"examples/retention/#transaction-vs-period-churn","title":"Transaction vs Period Churn\u00b6","text":"<p>Depending on your business, you may want to focus on period churn or transaction churn.</p> <p>Period churn Period, or time frame, churn is the most common way to speak about churn. You'll often hear phrases like monthly or quarterly churn.</p> <p>The period is often selected based on customers' buying patterns. Eg, if you are a clothing company, you may expect customers to come each quarter with each new season. In this case, you would choose a 12-week churn period. Even so, we still recommend calculating your churn window to validate your assumptions.</p> <p>Transaction churn Transaction churn is the other way of looking at churn. In this case, you focus on how quickly customers churn after each transaction. You may have heard of phrases like one-and-done churn. This refers to the rate of churn after one transaction.</p> <p>When to consider one versus the other</p> <p>Transaction churn is most interesting when you are focusing on moving new customers from an acquired state in the customer lifecycle into stable (or regular) buying pattern state.</p> <p>A period churn rate measurement is more applicable if you are mostly concerned with keeping customers in a stable buying pattern.</p> <p>Shouldn't I use both?</p> <p>From a practical perspective, this is an advanced retention strategy. It should only be combined with a lifecycle segmentation and when you are having trouble moving customers into a stable purchasing state. You can observe this if you see a steady decline in churn as the number of transactions increases.</p> <p>Let's start by looking at transaction churn</p>"},{"location":"examples/retention/#transaction-churn","title":"Transaction churn\u00b6","text":"<p>Ideally, we would have all of a company's transaction data. In that case, we can confidently know a customer's first transaction and count from that time.</p> <p>If you don't have all the transaction data, try and get the date of each customer's first transaction. Exclude those customers whose first transaction is outside your time window.</p> <p>You have all the transaction data</p>"},{"location":"examples/retention/#coming-soon","title":"Coming soon\u00b6","text":"<ul> <li>Transaction churn with incomplete data</li> <li>Period churn</li> <li>Measuring the cost of customer churn to the business</li> <li>Cohort analysis of customer churn</li> <li>... and more</li> </ul>"},{"location":"examples/revenue_tree/","title":"Revenue Tree","text":"<p>Overview</p> <p>The Revenue Tree is a hierarchical breakdown of factors contributing to overall revenue, allowing for detailed analysis of sales performance and identification of areas for improvement. It is also know as a KPI tree or sometimes as a driver tree.</p> <p>In this notebook, we'll look at a quick overview of how to generate a revenue tree.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\ndf = pd.read_parquet(\"../../data/transactions.parquet\")\ndf.head()\n</pre> import pandas as pd  df = pd.read_parquet(\"../../data/transactions.parquet\") df.head() Out[\u00a0]: transaction_id transaction_date transaction_time customer_id product_id product_name category_0_name category_0_id category_1_name category_1_id brand_name brand_id unit_quantity unit_cost unit_spend store_id 0 16050 2023-01-12 17:44:29 1 15 Spawn Figure Toys 1 Action Figures 1 McFarlane Toys 3 2 36.10 55.98 6 1 16050 2023-01-12 17:44:29 1 1317 Gone Girl Books 8 Mystery &amp; Thrillers 53 Alfred A. Knopf 264 1 6.98 10.49 6 2 20090 2023-02-05 09:31:42 1 509 Ryzen 3 3300X Electronics 3 Computer Components 21 AMD 102 3 200.61 360.00 4 3 20090 2023-02-05 09:31:42 1 735 Linden Wood Paneled Mirror Home 5 Home Decor 30 Pottery Barn 147 1 379.83 599.00 4 4 20090 2023-02-05 09:31:42 1 1107 Pro-V Daily Moisture Renewal Conditioner Beauty 7 Hair Care 45 Pantene 222 1 3.32 4.99 4 <p>Some details about the data</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Number of unique customers: {df['customer_id'].nunique()}\")\nprint(f\"Number of unique transactions: {df['transaction_id'].nunique()}\")\n</pre> print(f\"Number of unique customers: {df['customer_id'].nunique()}\") print(f\"Number of unique transactions: {df['transaction_id'].nunique()}\") <pre>Number of unique customers: 4250\nNumber of unique transactions: 25490\n</pre> <p>In this simple example we want to compare revenue from the first half of the year with that from the second half to see how revenue changed and which factors drove it.</p> In\u00a0[\u00a0]: Copied! <pre>from pyretailscience.analysis import revenue_tree\n\ndf[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"])\n\ndf[\"period\"] = df[\"transaction_date\"].apply(lambda x: \"p2\" if x &gt;= pd.to_datetime(\"2023-06-01\") else \"p1\")\n\nrev_tree = revenue_tree.RevenueTree(\n    df=df,\n    period_col=\"period\",\n    p1_value=\"p1\",\n    p2_value=\"p2\",\n)\n</pre> from pyretailscience.analysis import revenue_tree  df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"])  df[\"period\"] = df[\"transaction_date\"].apply(lambda x: \"p2\" if x &gt;= pd.to_datetime(\"2023-06-01\") else \"p1\")  rev_tree = revenue_tree.RevenueTree(     df=df,     period_col=\"period\",     p1_value=\"p1\",     p2_value=\"p2\", ) <p>The default view of the KPIs can be difficult to view. Use the <code>draw_tree</code> method to visualize the data as a tree.</p> In\u00a0[\u00a0]: Copied! <pre>rev_tree.draw_tree()\n</pre> rev_tree.draw_tree() Out[\u00a0]:"},{"location":"examples/revenue_tree/#setup","title":"Setup\u00b6","text":"<p>Here we'll create some simulated data to demonstrate the code. You do not need to know this to perform segmentations.</p>"},{"location":"examples/segmentation/","title":"Segmentation","text":"<p>Overview</p> <p>Customer segmentation involves categorizing your customers into smaller groups based on shared characteristics such as buying habits or demographics. This helps retailers gain deeper insights into their customer base, enabling them to tailor their marketing strategies and products to meet each group's specific needs.</p> <p>By employing customer segmentation, companies can enhance customer satisfaction, improve targeting accuracy, and optimize resource allocation, ultimately leading to increased profitability and competitiveness in the market.</p> <p>In this notebook, we'll explore a few basic customer segmentations.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_parquet(\"../../data/transactions.parquet\")\ndf.head()\n</pre> import matplotlib.pyplot as plt import pandas as pd  df = pd.read_parquet(\"../../data/transactions.parquet\") df.head() Out[\u00a0]: transaction_id transaction_date transaction_time customer_id product_id product_name category_0_name category_0_id category_1_name category_1_id brand_name brand_id unit_quantity unit_cost unit_spend store_id 0 16050 2023-01-12 17:44:29 1 15 Spawn Figure Toys 1 Action Figures 1 McFarlane Toys 3 2 36.10 55.98 6 1 16050 2023-01-12 17:44:29 1 1317 Gone Girl Books 8 Mystery &amp; Thrillers 53 Alfred A. Knopf 264 1 6.98 10.49 6 2 20090 2023-02-05 09:31:42 1 509 Ryzen 3 3300X Electronics 3 Computer Components 21 AMD 102 3 200.61 360.00 4 3 20090 2023-02-05 09:31:42 1 735 Linden Wood Paneled Mirror Home 5 Home Decor 30 Pottery Barn 147 1 379.83 599.00 4 4 20090 2023-02-05 09:31:42 1 1107 Pro-V Daily Moisture Renewal Conditioner Beauty 7 Hair Care 45 Pantene 222 1 3.32 4.99 4 <p>Some details about the data</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Number of unique customers: {df['customer_id'].nunique()}\")\nprint(f\"Number of unique transactions: {df['transaction_id'].nunique()}\")\n</pre> print(f\"Number of unique customers: {df['customer_id'].nunique()}\") print(f\"Number of unique transactions: {df['transaction_id'].nunique()}\") <pre>Number of unique customers: 4250\nNumber of unique transactions: 25490\n</pre> In\u00a0[\u00a0]: Copied! <pre>from pyretailscience.segmentation.hml import HMLSegmentation\n\nseg = HMLSegmentation(df, zero_value_customers=\"include_with_light\")\nseg.df.head()\n</pre> from pyretailscience.segmentation.hml import HMLSegmentation  seg = HMLSegmentation(df, zero_value_customers=\"include_with_light\") seg.df.head() <pre>/home/sys15/Documents/pyretailscience/pyretailscience/segmentation/threshold.py:86: FutureWarning: `case` is deprecated as of v10.0.0, removed in v11.0; use ibis.cases()\n  case = ibis.case()\n</pre> Out[\u00a0]: unit_spend segment_name customer_id 4221 2.99 Light 1406 3.49 Light 4094 6.98 Light 3499 8.49 Light 3720 8.50 Light <p>We can then attach those segments back to the original dataframe. You can see these segments to the very right of the dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>df_with_segments = seg.add_segment(df)\ndf_with_segments.head()\n</pre> df_with_segments = seg.add_segment(df) df_with_segments.head() Out[\u00a0]: transaction_id transaction_date transaction_time customer_id product_id product_name category_0_name category_0_id category_1_name category_1_id brand_name brand_id unit_quantity unit_cost unit_spend store_id segment_name 0 16050 2023-01-12 17:44:29 1 15 Spawn Figure Toys 1 Action Figures 1 McFarlane Toys 3 2 36.10 55.98 6 Light 1 16050 2023-01-12 17:44:29 1 1317 Gone Girl Books 8 Mystery &amp; Thrillers 53 Alfred A. Knopf 264 1 6.98 10.49 6 Light 2 20090 2023-02-05 09:31:42 1 509 Ryzen 3 3300X Electronics 3 Computer Components 21 AMD 102 3 200.61 360.00 4 Light 3 20090 2023-02-05 09:31:42 1 735 Linden Wood Paneled Mirror Home 5 Home Decor 30 Pottery Barn 147 1 379.83 599.00 4 Light 4 20090 2023-02-05 09:31:42 1 1107 Pro-V Daily Moisture Renewal Conditioner Beauty 7 Hair Care 45 Pantene 222 1 3.32 4.99 4 Light <p>Now it's only one line of code to get standard KPIs on each customer segment. This makes segment comparison easy.</p> In\u00a0[\u00a0]: Copied! <pre>from pyretailscience.segmentation.segstats import SegTransactionStats\n\nseg_stats = SegTransactionStats(df_with_segments, segment_col=\"segment_name\")\nseg_stats.df\n</pre> from pyretailscience.segmentation.segstats import SegTransactionStats  seg_stats = SegTransactionStats(df_with_segments, segment_col=\"segment_name\") seg_stats.df Out[\u00a0]: segment_name spend transactions customers units spend_per_customer spend_per_transaction transactions_per_customer price_per_unit units_per_transaction customers_pct 0 Light 1.344912e+07 7054 2125 56523 6328.995732 1906.594263 3.319529 237.940589 8.012900 0.5 1 Heavy 4.518138e+07 8572 850 80558 53154.567635 5270.809903 10.084706 560.855315 9.397807 0.2 2 Medium 3.045768e+07 9864 1275 86869 23888.378800 3087.761858 7.736471 350.616249 8.806671 0.3 3 Total 8.908818e+07 25490 4250 223950 20961.925033 3495.024770 5.997647 397.803891 8.785798 1.0 <p>We can quickly get a bar plot to compare any of the values using the <code>plot</code> function. With a few extra lines of code, we can dress up the plot with a callout. Now the chart highlights how much more valuable Heavy customers are compared to the Light ones.</p> In\u00a0[\u00a0]: Copied! <pre>from pyretailscience.style.graph_utils import GraphStyles\n\nax = seg_stats.plot(\n    figsize=(10, 5),\n    value_col=\"spend\",\n    source_text=\"Source: Transaction data financial year 2023\",\n    sort_order=\"descending\",\n    title=\"What's the value of a Heavy customer?\",\n    rot=0,\n)\n\n# Dress up the plot with an arrow calling out the important point\nheavy_rev = seg_stats.df[seg_stats.df[\"segment_name\"] == \"Heavy\"][\"spend\"].iat[0]\nlight_rev = seg_stats.df[seg_stats.df[\"segment_name\"] == \"Light\"][\"spend\"].iat[0]\nh_vs_l_rev = heavy_rev / light_rev\n\nax.annotate(\n    f\"The top 20% of customers\\ngenerate {h_vs_l_rev:.1f}x MORE REVENUE\\nthan the bottom 50%!\",\n    xy=(0.24, heavy_rev * 0.8),\n    xytext=(0.65, heavy_rev * 0.8),\n    fontsize=18,\n    fontproperties=GraphStyles.POPPINS_REG,\n    ha=\"left\",\n    va=\"center\",\n    arrowprops={\n        \"facecolor\": \"white\",\n        \"arrowstyle\": \"-&gt;\",\n        \"connectionstyle\": \"arc3,rad=0.1\",\n    },\n    color=\"black\",\n    bbox={\n        \"facecolor\": \"white\",\n        \"edgecolor\": \"white\",\n        \"boxstyle\": \"round,rounding_size=0.75\",\n        \"pad\": 0.75,\n    },\n    linespacing=1.5,\n)\n\nplt.savefig(\"monthly_active_customers.svg\")\nplt.show()\n</pre> from pyretailscience.style.graph_utils import GraphStyles  ax = seg_stats.plot(     figsize=(10, 5),     value_col=\"spend\",     source_text=\"Source: Transaction data financial year 2023\",     sort_order=\"descending\",     title=\"What's the value of a Heavy customer?\",     rot=0, )  # Dress up the plot with an arrow calling out the important point heavy_rev = seg_stats.df[seg_stats.df[\"segment_name\"] == \"Heavy\"][\"spend\"].iat[0] light_rev = seg_stats.df[seg_stats.df[\"segment_name\"] == \"Light\"][\"spend\"].iat[0] h_vs_l_rev = heavy_rev / light_rev  ax.annotate(     f\"The top 20% of customers\\ngenerate {h_vs_l_rev:.1f}x MORE REVENUE\\nthan the bottom 50%!\",     xy=(0.24, heavy_rev * 0.8),     xytext=(0.65, heavy_rev * 0.8),     fontsize=18,     fontproperties=GraphStyles.POPPINS_REG,     ha=\"left\",     va=\"center\",     arrowprops={         \"facecolor\": \"white\",         \"arrowstyle\": \"-&gt;\",         \"connectionstyle\": \"arc3,rad=0.1\",     },     color=\"black\",     bbox={         \"facecolor\": \"white\",         \"edgecolor\": \"white\",         \"boxstyle\": \"round,rounding_size=0.75\",         \"pad\": 0.75,     },     linespacing=1.5, )  plt.savefig(\"monthly_active_customers.svg\") plt.show() <p>Getting a plot of the segment's activity over time is also now a snap.</p> In\u00a0[\u00a0]: Copied! <pre>from pyretailscience.plots.time import plot\n\ndf[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"])\n\ndf_with_groups = seg.add_segment(df)\nplot(\n    df_with_groups,\n    figsize=(10, 5),\n    period=\"M\",\n    group_col=\"segment_name\",\n    value_col=\"customer_id\",\n    agg_func=\"nunique\",\n    title=\"Monthly Active Customers\",\n    y_label=\"Number of Customers\",\n    source_text=\"Source: PyRetailScience\",\n)\nplt.show()\n</pre> from pyretailscience.plots.time import plot  df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"])  df_with_groups = seg.add_segment(df) plot(     df_with_groups,     figsize=(10, 5),     period=\"M\",     group_col=\"segment_name\",     value_col=\"customer_id\",     agg_func=\"nunique\",     title=\"Monthly Active Customers\",     y_label=\"Number of Customers\",     source_text=\"Source: PyRetailScience\", ) plt.show() In\u00a0[\u00a0]: Copied! <pre>filename = \"heavy_customers.csv\"\nseg.df[seg.df[\"segment_name\"] == \"Heavy\"].index.to_series().to_csv(filename, index=False)\n\n# Display the CSV\nwith open(filename) as f:\n    print(\"\".join(f.readlines()[:5]))\n</pre> filename = \"heavy_customers.csv\" seg.df[seg.df[\"segment_name\"] == \"Heavy\"].index.to_series().to_csv(filename, index=False)  # Display the CSV with open(filename) as f:     print(\"\".join(f.readlines()[:5])) <pre>customer_id\n1566\n925\n2095\n1623\n\n</pre>"},{"location":"examples/segmentation/#setup","title":"Setup\u00b6","text":"<p>Here we'll load some simulated data to demonstrate the code. You do not need to know this to perform segmentations.</p>"},{"location":"examples/segmentation/#heavy-medium-light","title":"Heavy / Medium / Light\u00b6","text":"<p>Heavy, Medium, Light (HML) is a segmentation that places customers into groups based on the their percentile of spend or the number of products they bought. Heavy customers are the top 20% of customers, medium are the next 30%, and light are the bottom 50% of customers. These values are chosen based on the proportions of the Pareto distribution. Often, purchase behaviour follows this distribution. This is typified by the expression 20% of your customers generate 80% of your sales.</p> <p>You would use a HML segmentation to answer question such as these,</p> <ul> <li>How much more are your best customers worth?</li> <li>How much more could I spend acquiring my best customers?</li> <li>What is the concentration of sales with my top (heavy) customers?</li> </ul> <p>Using PyRetailScience it takes only one line of code to get HML segments.</p>"},{"location":"examples/segmentation/#activating","title":"Activating\u00b6","text":"<p>Once you have your segments, you will often want to activate them. It is easy to export them to a CSV for activation in your CRM or with platforms.</p>"},{"location":"examples/segmentation/#coming-soon","title":"Coming soon\u00b6","text":"<ul> <li>Quickly segmenting new vs returning customers</li> <li>More charting options</li> <li>Activating segments to Facebook and other social platforms</li> <li>... and more</li> </ul>"},{"location":"getting_started/installation/","title":"Getting started","text":"<p>PyRetailScience is a powerful framework for rapid bespoke and deep dive retail analytic. It equips you with a wide array of retail analytical capabilities, from segmentations to gain-loss analysis. Leave the mundane to us and elevate your role from data janitor to insights virtuoso.</p>"},{"location":"getting_started/installation/#installation","title":"Installation","text":"<p>PyRetailScience is published as a Python package and can be installed with <code>pip</code>, ideally by using a virtual environment. Open a terminal and install with:</p> <pre><code>pip install pyretailscience\n</code></pre>"}]}